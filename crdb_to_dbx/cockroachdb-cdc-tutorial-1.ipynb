{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Stream CockroachDB CDC to Databricks (Azure)\n",
    "\n",
    "This notebook demonstrates how to stream CockroachDB changefeeds to Databricks using Azure Blob Storage.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- CockroachDB cluster (Cloud or self-hosted)\n",
    "- Azure Storage Account with hierarchical namespace enabled\n",
    "- Databricks workspace with Unity Catalog\n",
    "- Unity Catalog External Location configured for your storage account\n",
    "\n",
    "**Note:** This notebook uses the **YCSB (Yahoo! Cloud Serving Benchmark)** schema as the default table structure, with `ycsb_key` as the primary key and `field0-9` columns. The default schema name is `public`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c81271",
   "metadata": {},
   "source": [
    "## CDC Mode Selection\n",
    "\n",
    "This notebook supports **4 CDC ingestion modes** by combining two independent settings:\n",
    "\n",
    "### 1. CDC Processing Mode (`cdc_mode`)\n",
    "How CDC events are processed in the target table:\n",
    "\n",
    "- **`append_only`**: Store all CDC events as rows (audit log)\n",
    "  - **Behavior**: All events (INSERT/UPDATE/DELETE) are appended as new rows\n",
    "  - **Use case**: History tracking, time-series analysis, audit logs\n",
    "  - **Storage**: Higher (keeps all historical events)\n",
    "\n",
    "- **`update_delete`**: Apply MERGE logic (current state replication)\n",
    "  - **Behavior**: DELETE removes rows, UPDATE modifies rows in-place\n",
    "  - **Use case**: Current state synchronization, production replication\n",
    "  - **Storage**: Lower (only latest state per key)\n",
    "\n",
    "### 2. Column Family Mode (`column_family_mode`)\n",
    "Table structure and changefeed configuration:\n",
    "\n",
    "- **`single_cf`**: Standard table (1 column family, default)\n",
    "  - **Changefeed**: `split_column_families=false`\n",
    "  - **Files**: 1 Parquet file per CDC event\n",
    "  - **Use case**: Most tables, simpler configuration, better performance\n",
    "\n",
    "- **`multi_cf`**: Multiple column families (for wide tables)\n",
    "  - **Changefeed**: `split_column_families=true`\n",
    "  - **Files**: Multiple Parquet files per CDC event (fragments need merging)\n",
    "  - **Use case**: Wide tables (50+ columns), selective column access patterns\n",
    "\n",
    "### Function Selection Matrix\n",
    "\n",
    "The notebook automatically selects the appropriate ingestion function based on your configuration:\n",
    "\n",
    "| CDC Mode | Column Family Mode | Function Called |\n",
    "|----------|-------------------|-----------------|\n",
    "| `append_only` | `single_cf` | `ingest_cdc_append_only_single_family()` |\n",
    "| `append_only` | `multi_cf` | `ingest_cdc_append_only_multi_family()` |\n",
    "| `update_delete` | `single_cf` | `ingest_cdc_with_merge_single_family()` |\n",
    "| `update_delete` | `multi_cf` | `ingest_cdc_with_merge_multi_family()` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Configuration file path (adjust as needed)\n",
    "config_file = \"/Users/robert.lee/github/lakeflow-community-connectors/sources/cockroachdb/.env/cockroachdb_cdc_tutorial_config.json\"\n",
    "\n",
    "# Try to load from file, fallback to embedded config\n",
    "try:\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(f\"‚úÖ Configuration loaded from: {config_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Using embedded configuration (config file error: {e})\")\n",
    "    config = None\n",
    "\n",
    "# Embedded configuration (fallback)\n",
    "if config is None:\n",
    "    config = {\n",
    "      \"cockroachdb\": {\n",
    "        \"host\": \"replace_me\",\n",
    "        \"port\": 26257,\n",
    "        \"user\": \"replace_me\",\n",
    "        \"password\": \"replace_me\",\n",
    "        \"database\": \"defaultdb\"\n",
    "      },\n",
    "      \"cockroachdb_source\": {\n",
    "        \"catalog\": \"defaultdb\",\n",
    "        \"schema\": \"public\",\n",
    "        \"table_name\": \"usertable\",\n",
    "        \"_schema_note\": \"Default schema is 'public'. Table uses YCSB structure (ycsb_key, field0-9)\",\n",
    "      },\n",
    "      \"azure_storage\": {\n",
    "        \"account_name\": \"replace_me\",\n",
    "        \"account_key\": \"replace_me\",\n",
    "        \"container_name\": \"changefeed-events\"\n",
    "      },\n",
    "      \"databricks_target\": {\n",
    "        \"catalog\": \"main\",\n",
    "        \"schema\": \"replace_me\",\n",
    "        \"table_name\": \"usertable\",\n",
    "      },\n",
    "      \"cdc_config\": {\n",
    "        \"mode\": \"append_only\",\n",
    "        \"column_family_mode\": \"multi_cf\",\n",
    "        \"primary_key_columns\": [\"ycsb_key\"],\n",
    "        \"auto_suffix_mode_family\": True,\n",
    "      },\n",
    "      \"workload_config\": {\n",
    "        \"snapshot_count\": 10,\n",
    "        \"insert_count\": 10,\n",
    "        \"update_count\": 9,\n",
    "        \"delete_count\": 8,\n",
    "      }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "\n",
    "# Extract configuration values\n",
    "cockroachdb_host = config[\"cockroachdb\"][\"host\"]\n",
    "cockroachdb_port = config[\"cockroachdb\"][\"port\"]\n",
    "cockroachdb_user = config[\"cockroachdb\"][\"user\"]\n",
    "cockroachdb_password = config[\"cockroachdb\"][\"password\"]\n",
    "cockroachdb_database = config[\"cockroachdb\"][\"database\"]\n",
    "\n",
    "source_catalog = config[\"cockroachdb_source\"][\"catalog\"]\n",
    "source_schema = config[\"cockroachdb_source\"][\"schema\"]\n",
    "source_table = config[\"cockroachdb_source\"][\"table_name\"]\n",
    "\n",
    "storage_account_name = config[\"azure_storage\"][\"account_name\"]\n",
    "storage_account_key = config[\"azure_storage\"][\"account_key\"]\n",
    "storage_account_key_encoded = quote(storage_account_key, safe='')\n",
    "container_name = config[\"azure_storage\"][\"container_name\"]\n",
    "\n",
    "target_catalog = config[\"databricks_target\"][\"catalog\"]\n",
    "target_schema = config[\"databricks_target\"][\"schema\"]\n",
    "target_table = config[\"databricks_target\"][\"table_name\"]\n",
    "\n",
    "cdc_mode = config[\"cdc_config\"][\"mode\"]\n",
    "column_family_mode = config[\"cdc_config\"][\"column_family_mode\"]\n",
    "primary_key_columns = config[\"cdc_config\"][\"primary_key_columns\"]\n",
    "\n",
    "snapshot_count = config[\"workload_config\"][\"snapshot_count\"]\n",
    "insert_count = config[\"workload_config\"][\"insert_count\"]\n",
    "update_count = config[\"workload_config\"][\"update_count\"]\n",
    "delete_count = config[\"workload_config\"][\"delete_count\"]\n",
    "\n",
    "# Auto-suffix table names with mode and column family if enabled\n",
    "auto_suffix = config[\"cdc_config\"].get(\"auto_suffix_mode_family\", False)\n",
    "if auto_suffix:\n",
    "    suffix = f\"_{cdc_mode}_{column_family_mode}\"\n",
    "    \n",
    "    # Add suffix to source_table if not already present\n",
    "    if not source_table.endswith(suffix):\n",
    "        source_table = f\"{source_table}{suffix}\"\n",
    "    \n",
    "    # Add suffix to target_table if not already present\n",
    "    if not target_table.endswith(suffix):\n",
    "        target_table = f\"{target_table}{suffix}\"\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   CDC Processing Mode: {cdc_mode}\")\n",
    "print(f\"   Column Family Mode: {column_family_mode}\")\n",
    "print(f\"   Primary Keys: {primary_key_columns}\")\n",
    "print(f\"   Target Table: {target_table}\")\n",
    "print(f\"   CDC Workload: {snapshot_count} snapshot ‚Üí +{insert_count} INSERTs, ~{update_count} UPDATEs, -{delete_count} DELETEs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "%pip install pg8000 azure-storage-blob --quiet\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: CONNECT TO COCKROACHDB\n",
    "# ============================================================================\n",
    "import pg8000\n",
    "import ssl\n",
    "\n",
    "def get_cockroachdb_connection():\n",
    "    \"\"\"Create connection to CockroachDB using pg8000\"\"\"\n",
    "    # Create SSL context (required for CockroachDB Cloud)\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "    \n",
    "    # Parse host (in case port is accidentally included in host string)\n",
    "    host = cockroachdb_host.split(':')[0] if ':' in cockroachdb_host else cockroachdb_host\n",
    "    \n",
    "    conn = pg8000.connect(\n",
    "        user=cockroachdb_user,\n",
    "        password=cockroachdb_password,\n",
    "        host=host,\n",
    "        port=cockroachdb_port,\n",
    "        database=cockroachdb_database,\n",
    "        ssl_context=ssl_context\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_cockroachdb_connection()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT version()\")\n",
    "        version = cur.fetchone()[0]\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"‚úÖ Connected to CockroachDB\")\n",
    "    print(f\"   Version: {version[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c40c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: HELPER FUNCTIONS (CockroachDB & Azure)\n",
    "# ============================================================================\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_table_stats(conn, table_name):\n",
    "    \"\"\"\n",
    "    Get min key, max key, and count for a table.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        table_name: Name of the table\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'min_key', 'max_key', 'count', 'is_empty'\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f\"SELECT MIN(ycsb_key), MAX(ycsb_key), COUNT(*) FROM {table_name}\")\n",
    "        result = cur.fetchone()\n",
    "        min_key, max_key, count = result\n",
    "        \n",
    "        return {\n",
    "            'min_key': min_key,\n",
    "            'max_key': max_key,\n",
    "            'count': count,\n",
    "            'is_empty': min_key is None and max_key is None\n",
    "        }\n",
    "\n",
    "\n",
    "def check_azure_files(storage_account_name, storage_account_key, container_name, \n",
    "                      source_catalog, source_schema, source_table, target_table, \n",
    "                      verbose=True):\n",
    "    \"\"\"\n",
    "    Check for changefeed files in Azure Blob Storage.\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        storage_account_key: Azure storage account key\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_table: Target table name\n",
    "        verbose: Print detailed output\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'data_files' and 'resolved_files' lists\n",
    "    \"\"\"\n",
    "    # Connect to Azure\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "    blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service.get_container_client(container_name)\n",
    "    \n",
    "    # Build path - list ALL files recursively under this changefeed path\n",
    "    prefix = f\"parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}/\"\n",
    "    \n",
    "    # List all blobs recursively (no date filtering)\n",
    "    blobs = list(container_client.list_blobs(name_starts_with=prefix))\n",
    "    \n",
    "    # Categorize files (using same filtering logic as cockroachdb.py)\n",
    "    # Data files: .parquet files, excluding:\n",
    "    #   - .RESOLVED files (CDC watermarks)\n",
    "    #   - _metadata/ directory (schema files)\n",
    "    #   - Files starting with _ (_SUCCESS, _committed_*, etc.)\n",
    "    data_files = [\n",
    "        b for b in blobs \n",
    "        if b.name.endswith('.parquet') \n",
    "        and '.RESOLVED' not in b.name\n",
    "        and '/_metadata/' not in b.name\n",
    "        and not b.name.split('/')[-1].startswith('_')\n",
    "    ]\n",
    "    resolved_files = [b for b in blobs if '.RESOLVED' in b.name]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìÅ Files in Azure changefeed path:\")\n",
    "        print(f\"   Path: {prefix}\")\n",
    "        print(f\"   üìÑ Data files: {len(data_files)}\")\n",
    "        print(f\"   üïê Resolved files: {len(resolved_files)}\")\n",
    "        print(f\"   üìä Total: {len(blobs)}\")\n",
    "        \n",
    "        if data_files:\n",
    "            print(f\"\\n   Example data file:\")\n",
    "            print(f\"   {data_files[0].name}\")\n",
    "    \n",
    "    return {\n",
    "        'data_files': data_files,\n",
    "        'resolved_files': resolved_files,\n",
    "        'total': len(blobs)\n",
    "    }\n",
    "\n",
    "\n",
    "def wait_for_changefeed_files(storage_account_name, storage_account_key, container_name,\n",
    "                               source_catalog, source_schema, source_table, target_table,\n",
    "                               max_wait=120, check_interval=5, stabilization_wait=5):\n",
    "    \"\"\"\n",
    "    Wait for changefeed files to appear in Azure with timeout and stabilization period.\n",
    "    \n",
    "    This function:\n",
    "    1. Polls Azure until first file(s) appear\n",
    "    2. Once files are detected, waits for additional files (important for column families)\n",
    "    3. Exits when no new files appear for 'stabilization_wait' seconds\n",
    "    \n",
    "    Args:\n",
    "        max_wait: Maximum seconds to wait for initial files (default: 120)\n",
    "        check_interval: Seconds between checks (default: 5)\n",
    "        stabilization_wait: Seconds to wait for file count to stabilize (default: 5)\n",
    "                           Important for column family mode where multiple files are written\n",
    "    \n",
    "    Returns:\n",
    "        True if files found, False if timeout\n",
    "    \"\"\"\n",
    "    print(f\"‚è≥ Waiting for initial snapshot files to appear in Azure...\")\n",
    "    \n",
    "    elapsed = 0\n",
    "    files_found = False\n",
    "    last_file_count = 0\n",
    "    stable_elapsed = 0\n",
    "    \n",
    "    while elapsed < max_wait:\n",
    "        result = check_azure_files(\n",
    "            storage_account_name, storage_account_key, container_name,\n",
    "            source_catalog, source_schema, source_table, target_table,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        current_file_count = len(result['data_files'])\n",
    "        \n",
    "        if not files_found and current_file_count > 0:\n",
    "            # First files detected - switch to stabilization mode\n",
    "            files_found = True\n",
    "            last_file_count = current_file_count\n",
    "            stable_elapsed = 0\n",
    "            print(f\"\\n‚úÖ First files appeared after {elapsed} seconds!\")\n",
    "            print(f\"   Found {current_file_count} file(s) so far...\")\n",
    "            print(f\"   Waiting {stabilization_wait}s for more files (column family fragments)...\")\n",
    "        \n",
    "        elif files_found:\n",
    "            # In stabilization mode - check if file count is stable\n",
    "            if current_file_count > last_file_count:\n",
    "                # More files arrived - reset stabilization timer\n",
    "                print(f\"   üìÑ File count increased: {last_file_count} ‚Üí {current_file_count}\")\n",
    "                last_file_count = current_file_count\n",
    "                stable_elapsed = 0\n",
    "            else:\n",
    "                # File count unchanged - increment stabilization timer\n",
    "                stable_elapsed += check_interval\n",
    "                \n",
    "                if stable_elapsed >= stabilization_wait:\n",
    "                    # Stabilization period complete - all files have landed\n",
    "                    print(f\"\\n‚úÖ File count stable at {current_file_count} for {stabilization_wait}s\")\n",
    "                    print(f\"   Total wait time: {elapsed + stable_elapsed}s\")\n",
    "                    print(f\"   Example: {result['data_files'][0].name}\")\n",
    "                    return True\n",
    "        \n",
    "        if not files_found:\n",
    "            print(f\"   Checking... ({elapsed}s elapsed)\", end='\\r')\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "        elapsed += check_interval\n",
    "    \n",
    "    if files_found:\n",
    "        # Files were found but stabilization didn't complete within max_wait\n",
    "        print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s (found {last_file_count} files but more may still be generating)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - no files appeared\")\n",
    "    \n",
    "    print(f\"   Run Cell 11 to check manually\")\n",
    "    return files_found  # Return True if we found at least some files\n",
    "\n",
    "\n",
    "\n",
    "def get_column_sum(conn, table_name, column_name):\n",
    "    \"\"\"\n",
    "    Get the sum of a numeric column in a table.\n",
    "    Text columns have non-numeric characters stripped before casting.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        table_name: Name of the table\n",
    "        column_name: Name of the column to sum\n",
    "    \n",
    "    Returns:\n",
    "        Sum of the column (handles mixed text/numeric values)\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        # Strip non-numeric chars, handle empty strings, cast to BIGINT\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT SUM(\n",
    "                CASE \n",
    "                    WHEN regexp_replace({column_name}::TEXT, '[^0-9]', '', 'g') = '' THEN 0\n",
    "                    ELSE regexp_replace({column_name}::TEXT, '[^0-9]', '', 'g')::BIGINT\n",
    "                END\n",
    "            ) \n",
    "            FROM {table_name}\n",
    "        \"\"\")\n",
    "        result = cur.fetchone()\n",
    "        return result[0]\n",
    "\n",
    "\n",
    "\n",
    "def get_column_sum_spark(df, column_name):\n",
    "    \"\"\"\n",
    "    Get the sum of a numeric column in a Spark DataFrame.\n",
    "    Text columns have non-numeric characters stripped before casting.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        column_name: Name of the column to sum\n",
    "    \n",
    "    Returns:\n",
    "        Sum of the column (handles mixed text/numeric values)\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    result = df.select(\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.regexp_replace(F.col(column_name).cast('string'), '[^0-9]', '') == '',\n",
    "                0\n",
    "            ).otherwise(\n",
    "                F.regexp_replace(F.col(column_name).cast('string'), '[^0-9]', '').cast('bigint')\n",
    "            )\n",
    "        ).alias('sum')\n",
    "    ).collect()[0]['sum']\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded (CockroachDB & Azure)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6719c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: DATABRICKS STREAMING MODES (CDC Ingestion Functions)\n",
    "# ============================================================================\n",
    "# This cell contains different CDC ingestion functions for various scenarios.\n",
    "# Select the appropriate function based on your use case:\n",
    "#\n",
    "# 1. ingest_cdc_append_only_single_family() - Simple append_only mode\n",
    "# 2. ingest_cdc_with_merge_single_family() - Apply updates/deletes (single CF)\n",
    "# 3. ingest_cdc_append_only_multi_family() - Append_only with column families\n",
    "# 4. ingest_cdc_with_merge_multi_family() - Full CDC with column families\n",
    "# ============================================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def ingest_cdc_append_only_single_family(\n",
    "    storage_account_name, container_name, \n",
    "    source_catalog, source_schema, source_table, \n",
    "    target_catalog, target_schema, target_table,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingest CDC events in APPEND-ONLY mode for single column family tables.\n",
    "    \n",
    "    This function:\n",
    "    - Reads Parquet CDC files from Azure using Auto Loader\n",
    "    - Filters out .RESOLVED files and metadata\n",
    "    - Transforms CockroachDB CDC columns (__crdb__*) to standard format\n",
    "    - Writes all events (INSERT/UPDATE/DELETE) as rows to Delta table\n",
    "    - Does NOT apply deletes or deduplicate updates (append_only)\n",
    "    \n",
    "    Use this for:\n",
    "    - Audit logs and full history tracking\n",
    "    - Tables WITHOUT column families (split_column_families=false)\n",
    "    - Simple CDC pipelines without MERGE logic\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_catalog: Databricks catalog\n",
    "        target_schema: Databricks schema\n",
    "        target_table: Target table name\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        StreamingQuery object\n",
    "    \"\"\"\n",
    "    # Build paths\n",
    "    source_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "    checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}\"\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    \n",
    "    print(\"üìñ Ingesting CDC events (Append-Only Mode)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Mode: APPEND-ONLY (Single Column Family)\")\n",
    "    print(f\"Source: {source_catalog}.{source_schema}.{source_table} (CockroachDB)\")\n",
    "    print(f\"Target: {target_table_fqn} (Databricks Delta)\")\n",
    "    print(f\"Source path: {source_path}/ (all dates, recursively)\")\n",
    "    print(f\"File filter: *{source_table}*.parquet\")\n",
    "    print(f\"   ‚úÖ Includes: Data files\")\n",
    "    print(f\"   ‚ùå Excludes: .RESOLVED, _metadata/, _SUCCESS, etc.\")\n",
    "    print()\n",
    "    \n",
    "    # Read with Auto Loader (production-grade filtering)\n",
    "    raw_df = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/schema\")\n",
    "        .option(\"pathGlobFilter\", f\"*{source_table}*.parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Schema inferred from data files\")\n",
    "    print(\"   (Filtering matches cockroachdb.py production code)\")\n",
    "    print()\n",
    "    \n",
    "    # Transform: CockroachDB CDC ‚Üí Standard CDC format\n",
    "    df = raw_df.select(\n",
    "        \"*\",\n",
    "        # Convert __crdb__updated (nanoseconds) to timestamp\n",
    "        F.from_unixtime(\n",
    "            F.col(\"__crdb__updated\").cast(\"double\").cast(\"bigint\") / 1000000000\n",
    "        ).cast(\"timestamp\").alias(\"_cdc_timestamp\"),\n",
    "        # Map event type\n",
    "        F.when(F.col(\"__crdb__event_type\") == \"d\", \"DELETE\")\n",
    "         .otherwise(\"UPSERT\")\n",
    "         .alias(\"_cdc_operation\")\n",
    "    ).drop(\"__crdb__updated\", \"__crdb__event_type\")\n",
    "    \n",
    "    # Write to Delta table (append_only)\n",
    "    query = (df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}/data\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(target_table_fqn)\n",
    "    )\n",
    "    \n",
    "    print(\"‚è≥ Processing CDC events...\")\n",
    "    return query\n",
    "\n",
    "\n",
    "def ingest_cdc_with_merge_single_family(\n",
    "    storage_account_name, container_name,\n",
    "    source_catalog, source_schema, source_table,\n",
    "    target_catalog, target_schema, target_table,\n",
    "    primary_key_columns,  # NEW: Required for MERGE join condition\n",
    "    spark\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingest CDC events with MERGE logic for single column family tables.\n",
    "    \n",
    "    This function:\n",
    "    - Reads Parquet CDC files from Azure using Auto Loader\n",
    "    - Filters out .RESOLVED files and metadata\n",
    "    - Transforms CockroachDB CDC columns (__crdb__*) to standard format\n",
    "    - Deduplicates events within each microbatch (handles column family fragments)\n",
    "    - Applies MERGE logic to target Delta table:\n",
    "      * UPDATE: When key exists and timestamp is newer\n",
    "      * DELETE: When key exists and operation is DELETE\n",
    "      * INSERT: When key doesn't exist and operation is UPSERT\n",
    "    - Preserves _cdc_operation column for monitoring and observability\n",
    "    \n",
    "    Use this for:\n",
    "    - Applications needing current state (not history)\n",
    "    - Tables WITHOUT column families (split_column_families=false)\n",
    "    - Production CDC pipelines with UPDATE/DELETE support\n",
    "    - Lower storage requirements (only latest state)\n",
    "    \n",
    "    Target table will contain:\n",
    "    - All data columns from source\n",
    "    - _cdc_operation: \"UPSERT\" (shows last operation on each row)\n",
    "    - _cdc_timestamp: Timestamp of last CDC event\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_catalog: Databricks catalog\n",
    "        target_schema: Databricks schema\n",
    "        target_table: Target table name\n",
    "        primary_key_columns: List of primary key column names (e.g., ['ycsb_key'])\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        Dict with query, staging_table, target_table, raw_count, deduped_count, merged\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F, Window\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    # Build paths\n",
    "    source_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "    checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}_merge\"\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    \n",
    "    print(\"üìñ Ingesting CDC events (MERGE Mode)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Mode: MERGE (Apply UPDATE/DELETE)\")\n",
    "    print(f\"Source: {source_catalog}.{source_schema}.{source_table} (CockroachDB)\")\n",
    "    print(f\"Target: {target_table_fqn} (Databricks Delta)\")\n",
    "    print(f\"Primary keys: {primary_key_columns}\")\n",
    "    print(f\"Source path: {source_path}/ (all dates, recursively)\")\n",
    "    print(f\"File filter: *{source_table}*.parquet\")\n",
    "    print(f\"   ‚úÖ Includes: Data files\")\n",
    "    print(f\"   ‚ùå Excludes: .RESOLVED, _metadata/, _SUCCESS, etc.\")\n",
    "    print()\n",
    "    \n",
    "    # Read with Auto Loader (production-grade filtering)\n",
    "    raw_df = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/schema\")\n",
    "        .option(\"pathGlobFilter\", f\"*{source_table}*.parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Schema inferred from data files\")\n",
    "    print(\"   (Filtering matches cockroachdb.py production code)\")\n",
    "    print()\n",
    "    \n",
    "    # Transform: CockroachDB CDC ‚Üí Standard CDC format\n",
    "    transformed_df = raw_df.select(\n",
    "        \"*\",\n",
    "        # Convert __crdb__updated (nanoseconds) to timestamp\n",
    "        F.from_unixtime(\n",
    "            F.col(\"__crdb__updated\").cast(\"double\").cast(\"bigint\") / 1000000000\n",
    "        ).cast(\"timestamp\").alias(\"_cdc_timestamp\"),\n",
    "        # Map event type\n",
    "        F.when(F.col(\"__crdb__event_type\") == \"d\", \"DELETE\")\n",
    "         .otherwise(\"UPSERT\")\n",
    "         .alias(\"_cdc_operation\")\n",
    "    ).drop(\"__crdb__event_type\", \"__crdb__updated\")\n",
    "    \n",
    "    print(\"‚úÖ CDC transformations applied (streaming compatible)\")\n",
    "    print(\"   ‚ÑπÔ∏è  Deduplication will happen in Stage 2 (batch mode)\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 1: Stream to Staging Table (Serverless Compatible - No Python UDFs)\n",
    "    # ========================================================================\n",
    "    staging_table_fqn = f\"{target_table_fqn}_staging\"\n",
    "    \n",
    "    print(\"üî∑ STAGE 1: Streaming to staging table (no Python UDFs)\")\n",
    "    print(f\"   Staging: {staging_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Write to staging table (pure Spark, no foreachBatch, no window functions)\n",
    "    query = (transformed_df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}/data\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(staging_table_fqn)  # ‚Üê No Python UDFs, Serverless compatible!\n",
    "    )\n",
    "    \n",
    "    print(\"‚è≥ Streaming CDC events to staging table...\")\n",
    "    query.awaitTermination()\n",
    "    print(\"‚úÖ Stream completed\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 2: Batch MERGE from Staging to Target (Runs on Driver)\n",
    "    # ========================================================================\n",
    "    print(\"üî∑ STAGE 2: Applying MERGE logic (batch operation)\")\n",
    "    print(f\"   Source: {staging_table_fqn}\")\n",
    "    print(f\"   Target: {target_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Read staging table (batch mode - window functions allowed!)\n",
    "    staging_df_raw = spark.read.table(staging_table_fqn)\n",
    "    staging_count_raw = staging_df_raw.count()\n",
    "    print(f\"   üìä Raw staging events: {staging_count_raw}\")\n",
    "    \n",
    "    if staging_count_raw == 0:\n",
    "        print(\"   ‚ÑπÔ∏è  No new events to process\")\n",
    "        return {\"query\": query, \"staging_table\": staging_table_fqn, \"merged\": 0}\n",
    "    \n",
    "    # Deduplicate by primary key (batch mode, matches cockroachdb.py logic)\n",
    "    # Keep only LATEST event per primary key based on timestamp\n",
    "    from pyspark.sql import Window\n",
    "    \n",
    "    print(f\"   üîÑ Deduplicating by primary keys: {primary_key_columns}...\")\n",
    "    window_spec = Window.partitionBy(*primary_key_columns).orderBy(F.col(\"_cdc_timestamp\").desc())\n",
    "    staging_df = (staging_df_raw\n",
    "        .withColumn(\"_row_num\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"_row_num\") == 1)\n",
    "        .drop(\"_row_num\")\n",
    "    )\n",
    "    \n",
    "    staging_count = staging_df.count()\n",
    "    duplicates_removed = staging_count_raw - staging_count\n",
    "    print(f\"   ‚úÖ Deduplicated: {staging_count} unique events ({duplicates_removed} duplicates removed)\")\n",
    "    \n",
    "    if staging_count == 0:\n",
    "        print(\"   ‚ÑπÔ∏è  All events were duplicates\")\n",
    "        return {\"query\": query, \"staging_table\": staging_table_fqn, \"merged\": 0}\n",
    "    \n",
    "    # Create target table if it doesn't exist\n",
    "    if not spark.catalog.tableExists(target_table_fqn):\n",
    "        print(f\"   üìù Creating new target table: {target_table_fqn}\")\n",
    "        \n",
    "        # CRITICAL: Proper DELETE handling for initial table creation\n",
    "        # This matches cockroachdb.py reference implementation\n",
    "        \n",
    "        # 1. Get keys that have DELETE events\n",
    "        delete_keys = staging_df.filter(F.col(\"_cdc_operation\") == \"DELETE\") \\\n",
    "            .select(*primary_key_columns) \\\n",
    "            .distinct()\n",
    "        delete_count = delete_keys.count()\n",
    "        \n",
    "        # 2. Get all non-DELETE rows\n",
    "        active_rows = staging_df.filter(F.col(\"_cdc_operation\") != \"DELETE\")\n",
    "        active_count = active_rows.count()\n",
    "        \n",
    "        # 3. Exclude rows with keys that are deleted (left anti join)\n",
    "        # This handles case where key has UPSERT at T1, DELETE at T2\n",
    "        rows_after_delete = active_rows.join(\n",
    "            delete_keys,\n",
    "            on=primary_key_columns,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        after_delete_count = rows_after_delete.count()\n",
    "        \n",
    "        # Note: staging_df is already deduplicated, so final_rows = rows_after_delete\n",
    "        final_rows = rows_after_delete\n",
    "        final_count = after_delete_count\n",
    "        \n",
    "        if delete_count > 0:\n",
    "            print(f\"   ‚ÑπÔ∏è  Found {delete_count} DELETE events\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Active rows before DELETE: {active_count}\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Active rows after DELETE: {after_delete_count}\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Rows removed by DELETE: {active_count - after_delete_count}\")\n",
    "        \n",
    "        # Keep ALL columns including _cdc_operation for monitoring\n",
    "        final_rows.write.format(\"delta\").saveAsTable(target_table_fqn)\n",
    "        merged_count = final_count\n",
    "        print(f\"   ‚úÖ Created table with {merged_count} initial rows\")\n",
    "        print(f\"      Schema includes _cdc_operation for observability\\n\")\n",
    "    else:\n",
    "        # Get Delta table and apply MERGE\n",
    "        delta_table = DeltaTable.forName(spark, target_table_fqn)\n",
    "        \n",
    "        # Check if _cdc_operation exists in target (might be missing from old tables)\n",
    "        target_columns = set(spark.read.table(target_table_fqn).columns)\n",
    "        if \"_cdc_operation\" not in target_columns:\n",
    "            print(f\"   ‚ö†Ô∏è  Target table missing _cdc_operation column (old schema)\")\n",
    "            print(f\"   üîß Adding _cdc_operation column for observability...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                ALTER TABLE {target_table_fqn} \n",
    "                ADD COLUMN _cdc_operation STRING\n",
    "            \"\"\")\n",
    "            print(f\"   ‚úÖ Column added\\n\")\n",
    "        \n",
    "        # Build join condition dynamically\n",
    "        join_condition = \" AND \".join([f\"target.{col} = source.{col}\" for col in primary_key_columns])\n",
    "        \n",
    "        # Get all data columns (KEEP _cdc_operation for observability!)\n",
    "        data_columns = [col for col in staging_df.columns]\n",
    "        \n",
    "        # Build UPDATE/INSERT clauses dynamically\n",
    "        update_set = {col: f\"source.{col}\" for col in data_columns}\n",
    "        insert_values = {col: f\"source.{col}\" for col in data_columns}\n",
    "        \n",
    "        print(f\"   üîÑ Executing MERGE...\")\n",
    "        print(f\"      Join: {join_condition}\")\n",
    "        print(f\"      ‚ÑπÔ∏è  _cdc_operation will be preserved for monitoring\")\n",
    "        \n",
    "        # Apply MERGE (runs on driver, not workers)\n",
    "        (delta_table.alias(\"target\").merge(\n",
    "            staging_df.alias(\"source\"),\n",
    "            join_condition\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"source._cdc_operation = 'UPSERT' AND source._cdc_timestamp > target._cdc_timestamp\",\n",
    "            set=update_set\n",
    "        )\n",
    "        .whenMatchedDelete(\n",
    "            condition=\"source._cdc_operation = 'DELETE'\"\n",
    "        )\n",
    "        .whenNotMatchedInsert(\n",
    "            condition=\"source._cdc_operation = 'UPSERT'\",\n",
    "            values=insert_values\n",
    "        )\n",
    "        .execute())\n",
    "        \n",
    "        merged_count = staging_count\n",
    "        print(f\"   ‚úÖ MERGE complete: processed {merged_count} events\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ CDC INGESTION COMPLETE (TWO-STAGE MERGE)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä Raw events: {staging_count_raw}\")\n",
    "    print(f\"üìä After deduplication: {staging_count} unique events\")\n",
    "    print(f\"üìä Staging table: {staging_table_fqn}\")\n",
    "    print(f\"üìä Target table:  {target_table_fqn}\")\n",
    "    print()\n",
    "    print(\"üìã Target table includes:\")\n",
    "    print(\"   - All data columns from source\")\n",
    "    print(\"   - _cdc_operation: UPSERT (for monitoring)\")\n",
    "    print(\"   - _cdc_timestamp: Last CDC event timestamp\")\n",
    "    print()\n",
    "    print(\"üí° TIP: Staging table can be dropped after successful MERGE:\")\n",
    "    print(f\"   spark.sql('DROP TABLE IF EXISTS {staging_table_fqn}')\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"staging_table\": staging_table_fqn,\n",
    "        \"target_table\": target_table_fqn,\n",
    "        \"raw_count\": staging_count_raw,\n",
    "        \"deduped_count\": staging_count,\n",
    "        \"merged\": merged_count\n",
    "    }\n",
    "\n",
    "\n",
    "def merge_column_family_fragments(df, primary_key_columns, spark):\n",
    "    \"\"\"\n",
    "    Merge column family fragments into complete rows (streaming-compatible).\n",
    "    \n",
    "    When split_column_families=true, CockroachDB creates multiple CDC events per row update,\n",
    "    one for each column family. This function merges these fragments by:\n",
    "    1. Grouping by (primary_key + _cdc_timestamp + _cdc_operation)\n",
    "    2. Using first(col, ignorenulls=True) to coalesce NULL values from different fragments\n",
    "    \n",
    "    Each fragment has:\n",
    "    - Primary key columns (always present)\n",
    "    - Data for ONE column family (other columns are NULL)\n",
    "    - Same _cdc_timestamp and _cdc_operation\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame with potential column family fragments\n",
    "        primary_key_columns: List of primary key column names (e.g., ['ycsb_key'])\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        Merged DataFrame with complete rows\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    # Get all columns\n",
    "    all_columns = df.columns\n",
    "    \n",
    "    # Metadata columns to preserve (not aggregate as data)\n",
    "    metadata_columns = {'_cdc_operation', '_cdc_timestamp', '_rescued_data'}\n",
    "    \n",
    "    # Data columns = all columns except PK and metadata\n",
    "    data_columns = [\n",
    "        col for col in all_columns\n",
    "        if col not in primary_key_columns \n",
    "        and col not in metadata_columns\n",
    "    ]\n",
    "    \n",
    "    # Group by: PK + timestamp + operation (preserves all distinct CDC events)\n",
    "    group_by_cols = primary_key_columns + ['_cdc_timestamp', '_cdc_operation']\n",
    "    \n",
    "    # Build aggregation expressions\n",
    "    # Use first(col, ignorenulls=True) to merge NULL values from fragments\n",
    "    agg_exprs = [\n",
    "        F.first(col, ignorenulls=True).alias(col) \n",
    "        for col in data_columns\n",
    "    ]\n",
    "    \n",
    "    # Add metadata columns that aren't in the grouping key\n",
    "    for col in metadata_columns:\n",
    "        if col in all_columns and col not in group_by_cols:\n",
    "            agg_exprs.append(F.first(col, ignorenulls=True).alias(col))\n",
    "    \n",
    "    # Apply merge\n",
    "    df_merged = df.groupBy(*group_by_cols).agg(*agg_exprs)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def ingest_cdc_append_only_multi_family(\n",
    "    storage_account_name, container_name,\n",
    "    source_catalog, source_schema, source_table,\n",
    "    target_catalog, target_schema, target_table,\n",
    "    primary_key_columns,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingest CDC events in APPEND-ONLY mode with COLUMN FAMILY support.\n",
    "    \n",
    "    **Two-Stage Approach (Serverless Compatible)**:\n",
    "    - Stage 1: Stream raw CDC events to staging table (no aggregations)\n",
    "    - Stage 2: Batch merge column family fragments to target table\n",
    "    \n",
    "    This function:\n",
    "    - Reads Parquet CDC files from Azure using Auto Loader\n",
    "    - Filters out .RESOLVED files and metadata\n",
    "    - Transforms CockroachDB CDC columns (__crdb__*) to standard format\n",
    "    - MERGES column family fragments (split_column_families=true) in batch mode\n",
    "    - Writes all events (INSERT/UPDATE/DELETE) as rows to Delta table\n",
    "    - Does NOT apply deletes or deduplicate updates (append_only)\n",
    "    \n",
    "    Use this for:\n",
    "    - Audit logs with column family tables\n",
    "    - Tables WITH column families (split_column_families=true)\n",
    "    - Full history tracking with wide tables\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_catalog: Databricks catalog\n",
    "        target_schema: Databricks schema\n",
    "        target_table: Target table name\n",
    "        primary_key_columns: List of primary key column names (required for fragment merging)\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        StreamingQuery object\n",
    "    \"\"\"\n",
    "    # Build paths\n",
    "    source_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "    checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}\"\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    \n",
    "    print(\"üìñ Ingesting CDC events (Append-Only + Column Families)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Mode: APPEND-ONLY (Multi Column Family)\")\n",
    "    print(f\"Source: {source_catalog}.{source_schema}.{source_table} (CockroachDB)\")\n",
    "    print(f\"Target: {target_table_fqn} (Databricks Delta)\")\n",
    "    print(f\"Primary keys: {primary_key_columns}\")\n",
    "    print(f\"Source path: {source_path}/ (all dates, recursively)\")\n",
    "    print(f\"File filter: *{source_table}*.parquet\")\n",
    "    print()\n",
    "    \n",
    "    # Read with Auto Loader\n",
    "    raw_df = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/schema\")\n",
    "        .option(\"pathGlobFilter\", f\"*{source_table}*.parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Schema inferred from data files\")\n",
    "    print()\n",
    "    \n",
    "    # Transform: CockroachDB CDC ‚Üí Standard CDC format\n",
    "    transformed_df = raw_df.select(\n",
    "        \"*\",\n",
    "        F.from_unixtime(\n",
    "            F.col(\"__crdb__updated\").cast(\"double\").cast(\"bigint\") / 1000000000\n",
    "        ).cast(\"timestamp\").alias(\"_cdc_timestamp\"),\n",
    "        F.when(F.col(\"__crdb__event_type\") == \"d\", \"DELETE\")\n",
    "         .otherwise(\"UPSERT\")\n",
    "         .alias(\"_cdc_operation\")\n",
    "    ).drop(\"__crdb__event_type\", \"__crdb__updated\")\n",
    "    \n",
    "    print(\"‚úÖ CDC transformations applied (streaming compatible)\")\n",
    "    print(\"   ‚ÑπÔ∏è  Column family merge will happen in Stage 2 (batch mode)\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 1: Stream to Staging Table (Serverless Compatible - No Aggregations)\n",
    "    # ========================================================================\n",
    "    staging_table_fqn = f\"{target_table_fqn}_staging_cf\"\n",
    "    \n",
    "    print(\"üî∑ STAGE 1: Streaming to staging table (no aggregations)\")\n",
    "    print(f\"   Staging: {staging_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Write to staging table (pure Spark, no aggregations)\n",
    "    query = (transformed_df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}/data\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(staging_table_fqn)  # ‚Üê No aggregations, Serverless compatible!\n",
    "    )\n",
    "    \n",
    "    print(\"‚è≥ Streaming CDC events to staging table...\")\n",
    "    query.awaitTermination()\n",
    "    print(\"‚úÖ Stream completed\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 2: Merge Column Families in Batch Mode\n",
    "    # ========================================================================\n",
    "    print(\"üî∑ STAGE 2: Merging column family fragments (batch mode)\")\n",
    "    print(f\"   Reading from staging: {staging_table_fqn}\")\n",
    "    print(f\"   Writing to target: {target_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Read staging table in batch mode\n",
    "    staging_df = spark.table(staging_table_fqn)\n",
    "    \n",
    "    # Merge column family fragments (batch mode - no streaming limitations!)\n",
    "    print(\"üîß Merging column family fragments...\")\n",
    "    print(f\"   Grouping by: {primary_key_columns} + _cdc_timestamp + _cdc_operation\")\n",
    "    print(f\"   Using first(col, ignorenulls=True) to coalesce fragments\")\n",
    "    merged_df = merge_column_family_fragments(staging_df, primary_key_columns, spark)\n",
    "    print(\"‚úÖ Column family fragments merged\")\n",
    "    print()\n",
    "    \n",
    "    # Write to final target table (batch mode, append_only)\n",
    "    print(f\"üíæ Writing merged events to {target_table_fqn}...\")\n",
    "    merged_df.write.format(\"delta\").mode(\"append\").saveAsTable(target_table_fqn)\n",
    "    print(\"‚úÖ Append-only write complete\")\n",
    "    print()\n",
    "    \n",
    "    # Clean up staging table\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {staging_table_fqn}\")\n",
    "    print(f\"üßπ Staging table dropped: {staging_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    return query\n",
    "\n",
    "\n",
    "def ingest_cdc_with_merge_multi_family(\n",
    "    storage_account_name, container_name,\n",
    "    source_catalog, source_schema, source_table,\n",
    "    target_catalog, target_schema, target_table,\n",
    "    primary_key_columns,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingest CDC events with MERGE logic and COLUMN FAMILY support.\n",
    "    \n",
    "    **Two-Stage Approach (Serverless Compatible)**:\n",
    "    - Stage 1: Stream raw CDC events to staging table (no aggregations)\n",
    "    - Stage 2: Batch merge column families + deduplicate + MERGE to target\n",
    "    \n",
    "    This function:\n",
    "    - Reads Parquet CDC files from Azure using Auto Loader\n",
    "    - Filters out .RESOLVED files and metadata\n",
    "    - Transforms CockroachDB CDC columns (__crdb__*) to standard format\n",
    "    - MERGES column family fragments (split_column_families=true) in batch mode\n",
    "    - Streams to staging table (Serverless-compatible)\n",
    "    - Deduplicates by primary key in batch mode\n",
    "    - Applies MERGE logic to target Delta table\n",
    "    \n",
    "    Use this for:\n",
    "    - Current state replication with column families\n",
    "    - Tables WITH column families (split_column_families=true)\n",
    "    - Production CDC with UPDATE/DELETE support\n",
    "    \n",
    "    Target table will contain:\n",
    "    - All data columns from source\n",
    "    - _cdc_operation: \"UPSERT\" (shows last operation)\n",
    "    - _cdc_timestamp: Timestamp of last CDC event\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_catalog: Databricks catalog\n",
    "        target_schema: Databricks schema\n",
    "        target_table: Target table name\n",
    "        primary_key_columns: List of primary key column names (required for fragments + MERGE)\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        Dict with query, staging_table, target_table, raw_count, deduped_count, merged\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F, Window\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    # Build paths\n",
    "    source_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "    checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}_merge_cf\"\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    \n",
    "    print(\"üìñ Ingesting CDC events (MERGE + Column Families)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Mode: MERGE with Column Families\")\n",
    "    print(f\"Source: {source_catalog}.{source_schema}.{source_table} (CockroachDB)\")\n",
    "    print(f\"Target: {target_table_fqn} (Databricks Delta)\")\n",
    "    print(f\"Primary keys: {primary_key_columns}\")\n",
    "    print(f\"Source path: {source_path}/ (all dates, recursively)\")\n",
    "    print()\n",
    "    \n",
    "    # Read with Auto Loader\n",
    "    raw_df = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/schema\")\n",
    "        .option(\"pathGlobFilter\", f\"*{source_table}*.parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Schema inferred from data files\")\n",
    "    print()\n",
    "    \n",
    "    # Transform: CockroachDB CDC ‚Üí Standard CDC format\n",
    "    transformed_df = raw_df.select(\n",
    "        \"*\",\n",
    "        F.from_unixtime(\n",
    "            F.col(\"__crdb__updated\").cast(\"double\").cast(\"bigint\") / 1000000000\n",
    "        ).cast(\"timestamp\").alias(\"_cdc_timestamp\"),\n",
    "        F.when(F.col(\"__crdb__event_type\") == \"d\", \"DELETE\")\n",
    "         .otherwise(\"UPSERT\")\n",
    "         .alias(\"_cdc_operation\")\n",
    "    ).drop(\"__crdb__event_type\", \"__crdb__updated\")\n",
    "    \n",
    "    print(\"‚úÖ CDC transformations applied (streaming compatible)\")\n",
    "    print(\"   ‚ÑπÔ∏è  Column family merge will happen in Stage 2 (batch mode)\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 1: Stream to Staging Table (Serverless Compatible - No Aggregations)\n",
    "    # ========================================================================\n",
    "    staging_table_fqn = f\"{target_table_fqn}_staging_cf\"\n",
    "    \n",
    "    print(\"üî∑ STAGE 1: Streaming to staging table (no aggregations)\")\n",
    "    print(f\"   Staging: {staging_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    query = (transformed_df.writeStream  # ‚Üê Stream raw events (no column family merge yet)\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}/data\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(staging_table_fqn)  # ‚Üê No aggregations, Serverless compatible!\n",
    "    )\n",
    "    \n",
    "    print(\"‚è≥ Streaming CDC events to staging table...\")\n",
    "    query.awaitTermination()\n",
    "    print(\"‚úÖ Stream completed\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 2: Batch MERGE from Staging to Target\n",
    "    # ========================================================================\n",
    "    print(\"üî∑ STAGE 2: Applying MERGE logic (batch operation)\")\n",
    "    print(f\"   Source: {staging_table_fqn}\")\n",
    "    print(f\"   Target: {target_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Read staging table (batch mode)\n",
    "    staging_df_raw = spark.read.table(staging_table_fqn)\n",
    "    staging_count_raw = staging_df_raw.count()\n",
    "    print(f\"   üìä Raw staging events: {staging_count_raw}\")\n",
    "    \n",
    "    if staging_count_raw == 0:\n",
    "        print(\"   ‚ÑπÔ∏è  No new events to process\")\n",
    "        return {\"query\": query, \"staging_table\": staging_table_fqn, \"merged\": 0}\n",
    "    \n",
    "    # Merge column family fragments (batch mode - no streaming limitations!)\n",
    "    print(f\"   üîß Merging column family fragments...\")\n",
    "    print(f\"      Grouping by: {primary_key_columns} + _cdc_timestamp + _cdc_operation\")\n",
    "    staging_df_merged = merge_column_family_fragments(staging_df_raw, primary_key_columns, spark)\n",
    "    print(f\"   ‚úÖ Column family fragments merged\")\n",
    "    \n",
    "    # Deduplicate by primary key (keep latest event)\n",
    "    print(f\"   üîÑ Deduplicating by primary keys: {primary_key_columns}...\")\n",
    "    window_spec = Window.partitionBy(*primary_key_columns).orderBy(F.col(\"_cdc_timestamp\").desc())\n",
    "    staging_df = (staging_df_merged  # ‚Üê Use merged DataFrame\n",
    "        .withColumn(\"_row_num\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"_row_num\") == 1)\n",
    "        .drop(\"_row_num\")\n",
    "    )\n",
    "    \n",
    "    staging_count = staging_df.count()\n",
    "    fragments_removed = staging_count_raw - staging_df_merged.count()\n",
    "    duplicates_removed = staging_df_merged.count() - staging_count\n",
    "    print(f\"   ‚úÖ Column family fragments coalesced: {fragments_removed} fragments merged\")\n",
    "    print(f\"   ‚úÖ Deduplicated: {staging_count} unique events ({duplicates_removed} duplicates removed)\")\n",
    "    \n",
    "    if staging_count == 0:\n",
    "        print(\"   ‚ÑπÔ∏è  All events were duplicates\")\n",
    "        return {\"query\": query, \"staging_table\": staging_table_fqn, \"merged\": 0}\n",
    "    \n",
    "    # Create target table if it doesn't exist\n",
    "    if not spark.catalog.tableExists(target_table_fqn):\n",
    "        print(f\"   üìù Creating new target table: {target_table_fqn}\")\n",
    "        \n",
    "        # CRITICAL: Proper DELETE handling for initial table creation\n",
    "        # This matches cockroachdb.py reference implementation\n",
    "        \n",
    "        # 1. Get keys that have DELETE events\n",
    "        delete_keys = staging_df.filter(F.col(\"_cdc_operation\") == \"DELETE\") \\\n",
    "            .select(*primary_key_columns) \\\n",
    "            .distinct()\n",
    "        delete_count = delete_keys.count()\n",
    "        \n",
    "        # 2. Get all non-DELETE rows\n",
    "        active_rows = staging_df.filter(F.col(\"_cdc_operation\") != \"DELETE\")\n",
    "        active_count = active_rows.count()\n",
    "        \n",
    "        # 3. Exclude rows with keys that are deleted (left anti join)\n",
    "        # This handles case where key has UPSERT at T1, DELETE at T2\n",
    "        rows_after_delete = active_rows.join(\n",
    "            delete_keys,\n",
    "            on=primary_key_columns,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        after_delete_count = rows_after_delete.count()\n",
    "        \n",
    "        # Note: staging_df is already deduplicated, so final_rows = rows_after_delete\n",
    "        final_rows = rows_after_delete\n",
    "        final_count = after_delete_count\n",
    "        \n",
    "        if delete_count > 0:\n",
    "            print(f\"   ‚ÑπÔ∏è  Found {delete_count} DELETE events\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Active rows before DELETE: {active_count}\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Active rows after DELETE: {after_delete_count}\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Rows removed by DELETE: {active_count - after_delete_count}\")\n",
    "        \n",
    "        final_rows.write.format(\"delta\").saveAsTable(target_table_fqn)\n",
    "        merged_count = final_count\n",
    "        print(f\"   ‚úÖ Created table with {merged_count} initial rows\")\n",
    "        print(f\"      Schema includes _cdc_operation for observability\\n\")\n",
    "    else:\n",
    "        # Get Delta table and apply MERGE\n",
    "        delta_table = DeltaTable.forName(spark, target_table_fqn)\n",
    "        \n",
    "        # Check if _cdc_operation exists in target\n",
    "        target_columns = set(spark.read.table(target_table_fqn).columns)\n",
    "        if \"_cdc_operation\" not in target_columns:\n",
    "            print(f\"   ‚ö†Ô∏è  Target table missing _cdc_operation column\")\n",
    "            print(f\"   üîß Adding _cdc_operation column...\")\n",
    "            spark.sql(f\"ALTER TABLE {target_table_fqn} ADD COLUMN _cdc_operation STRING\")\n",
    "            print(f\"   ‚úÖ Column added\\n\")\n",
    "        \n",
    "        # Build join condition\n",
    "        join_condition = \" AND \".join([f\"target.{col} = source.{col}\" for col in primary_key_columns])\n",
    "        \n",
    "        # Get all columns\n",
    "        data_columns = [col for col in staging_df.columns]\n",
    "        update_set = {col: f\"source.{col}\" for col in data_columns}\n",
    "        insert_values = {col: f\"source.{col}\" for col in data_columns}\n",
    "        \n",
    "        print(f\"   üîÑ Executing MERGE...\")\n",
    "        print(f\"      Join: {join_condition}\")\n",
    "        print(f\"      ‚ÑπÔ∏è  _cdc_operation preserved for monitoring\")\n",
    "        \n",
    "        # Apply MERGE\n",
    "        (delta_table.alias(\"target\").merge(\n",
    "            staging_df.alias(\"source\"),\n",
    "            join_condition\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"source._cdc_operation = 'UPSERT' AND source._cdc_timestamp > target._cdc_timestamp\",\n",
    "            set=update_set\n",
    "        )\n",
    "        .whenMatchedDelete(\n",
    "            condition=\"source._cdc_operation = 'DELETE'\"\n",
    "        )\n",
    "        .whenNotMatchedInsert(\n",
    "            condition=\"source._cdc_operation = 'UPSERT'\",\n",
    "            values=insert_values\n",
    "        )\n",
    "        .execute())\n",
    "        \n",
    "        merged_count = staging_count\n",
    "        print(f\"   ‚úÖ MERGE complete: processed {merged_count} events\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ CDC INGESTION COMPLETE (MERGE + COLUMN FAMILIES)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä Raw events: {staging_count_raw}\")\n",
    "    print(f\"üìä After deduplication: {staging_count} unique events\")\n",
    "    print(f\"üìä Staging table: {staging_table_fqn}\")\n",
    "    print(f\"üìä Target table:  {target_table_fqn}\")\n",
    "    print()\n",
    "    print(\"üí° TIP: Staging table can be dropped after successful MERGE:\")\n",
    "    print(f\"   spark.sql('DROP TABLE IF EXISTS {staging_table_fqn}')\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"staging_table\": staging_table_fqn,\n",
    "        \"target_table\": target_table_fqn,\n",
    "        \"raw_count\": staging_count_raw,\n",
    "        \"deduped_count\": staging_count,\n",
    "        \"merged\": merged_count\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Databricks streaming modes loaded (4 functions available)\")\n",
    "print(\"   1. ingest_cdc_append_only_single_family (append_only, no column families)\")\n",
    "print(\"   2. ingest_cdc_with_merge_single_family (update_delete, no column families)\")\n",
    "print(\"   3. ingest_cdc_append_only_multi_family (append_only, WITH column families)\")\n",
    "print(\"   4. ingest_cdc_with_merge_multi_family (update_delete, WITH column families)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: CREATE TEST TABLE (Mode-Aware: Single or Multiple Column Families)\n",
    "# ============================================================================\n",
    "# Create table structure based on column_family_mode:\n",
    "# - single_cf: 1 column family (default, better performance)\n",
    "# - multi_cf: 3 column families (for testing split_column_families=true)\n",
    "\n",
    "if column_family_mode == \"multi_cf\":\n",
    "    # Create table with MULTIPLE column families for testing split_column_families=true\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {source_table} (\n",
    "        ycsb_key INT PRIMARY KEY,\n",
    "        -- Family 1: Frequently accessed fields\n",
    "        field0 TEXT,\n",
    "        field1 TEXT,\n",
    "        field2 TEXT,\n",
    "        FAMILY frequently_read (ycsb_key, field0, field1, field2),\n",
    "        \n",
    "        -- Family 2: Medium-frequency fields\n",
    "        field3 TEXT,\n",
    "        field4 TEXT,\n",
    "        field5 TEXT,\n",
    "        FAMILY medium_read (field3, field4, field5),\n",
    "        \n",
    "        -- Family 3: Rarely accessed fields\n",
    "        field6 TEXT,\n",
    "        field7 TEXT,\n",
    "        field8 TEXT,\n",
    "        field9 TEXT,\n",
    "        FAMILY rarely_read (field6, field7, field8, field9)\n",
    "    )\n",
    "    \"\"\"\n",
    "    family_info = \"3 column families (frequently_read, medium_read, rarely_read)\"\n",
    "else:\n",
    "    # Create table with SINGLE column family (default)\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {source_table} (\n",
    "        ycsb_key INT PRIMARY KEY,\n",
    "        field0 TEXT,\n",
    "        field1 TEXT,\n",
    "        field2 TEXT,\n",
    "        field3 TEXT,\n",
    "        field4 TEXT,\n",
    "        field5 TEXT,\n",
    "        field6 TEXT,\n",
    "        field7 TEXT,\n",
    "        field8 TEXT,\n",
    "        field9 TEXT\n",
    "    )\n",
    "    \"\"\"\n",
    "    family_info = \"1 column family (default primary)\"\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(create_table_sql)\n",
    "        conn.commit()\n",
    "    print(f\"‚úÖ Table '{source_table}' created (or already exists)\")\n",
    "    print(f\"   Column Family Mode: {column_family_mode}\")\n",
    "    print(f\"   Column families: {family_info}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: INSERT SAMPLE DATA (Snapshot Phase)\n",
    "# ============================================================================\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    # Check if table is empty using helper function\n",
    "    stats = get_table_stats(conn, source_table)\n",
    "    \n",
    "    if stats['is_empty']:\n",
    "        # Table is empty - insert snapshot data\n",
    "        print(f\"üìä Table is empty. Inserting {snapshot_count} initial rows (snapshot phase)...\")\n",
    "        \n",
    "        with conn.cursor() as cur:\n",
    "            # Use generate_series for efficient bulk insert\n",
    "            insert_sql = f\"\"\"\n",
    "            INSERT INTO {source_table} \n",
    "            (ycsb_key, field0, field1, field2, field3, field4, field5, field6, field7, field8, field9)\n",
    "            SELECT \n",
    "                i AS ycsb_key,\n",
    "                'snapshot_value_' || i || '_0' AS field0,\n",
    "                'snapshot_value_' || i || '_1' AS field1,\n",
    "                'snapshot_value_' || i || '_2' AS field2,\n",
    "                'snapshot_value_' || i || '_3' AS field3,\n",
    "                'snapshot_value_' || i || '_4' AS field4,\n",
    "                'snapshot_value_' || i || '_5' AS field5,\n",
    "                'snapshot_value_' || i || '_6' AS field6,\n",
    "                'snapshot_value_' || i || '_7' AS field7,\n",
    "                'snapshot_value_' || i || '_8' AS field8,\n",
    "                'snapshot_value_' || i || '_9' AS field9\n",
    "            FROM generate_series(0, %s - 1) AS i\n",
    "            \"\"\"\n",
    "            \n",
    "            cur.execute(insert_sql, (snapshot_count,))\n",
    "            conn.commit()\n",
    "        \n",
    "        print(f\"‚úÖ Sample data inserted using generate_series\")\n",
    "        print(f\"   Rows inserted: {snapshot_count} (keys 0 to {snapshot_count - 1})\")\n",
    "    else:\n",
    "        # Table already has data - skip insert\n",
    "        print(f\"‚ÑπÔ∏è  Table already contains data - skipping snapshot insert\")\n",
    "        print(f\"   Current key range: {stats['min_key']} to {stats['max_key']}\")\n",
    "        print(f\"   Tip: If you want to re-run the snapshot, drop the table first (see Cleanup cells)\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_changefeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: CREATE CHANGEFEED\n",
    "# ============================================================================\n",
    "# Build Azure Blob Storage URI with table-specific path\n",
    "# Note: For Azure, path goes in URI (not as path_prefix query parameter like S3)\n",
    "path = f\"parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "changefeed_path = f\"azure://{container_name}/{path}?AZURE_ACCOUNT_NAME={storage_account_name}&AZURE_ACCOUNT_KEY={storage_account_key_encoded}\"\n",
    "\n",
    "# Build changefeed options based on column_family_mode\n",
    "if column_family_mode == \"multi_cf\":\n",
    "    # Include split_column_families for multi-family mode\n",
    "    changefeed_options = \"\"\"\n",
    "    format='parquet',\n",
    "    updated,\n",
    "    resolved='10s',\n",
    "    split_column_families\n",
    "\"\"\"\n",
    "else:\n",
    "    # Standard options for single-family mode\n",
    "    changefeed_options = \"\"\"\n",
    "    format='parquet',\n",
    "    updated,\n",
    "    resolved='10s'\n",
    "\"\"\"\n",
    "\n",
    "# Create changefeed SQL\n",
    "create_changefeed_sql = f\"\"\"\n",
    "CREATE CHANGEFEED FOR TABLE {source_table}\n",
    "INTO '{changefeed_path}'\n",
    "WITH {changefeed_options}\n",
    "\"\"\"\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        # Check for existing changefeed with THIS specific destination path\n",
    "        # (checks for source table AND full path to ensure uniqueness)\n",
    "        path_pattern = f\"%{source_table}%{source_catalog}/{source_schema}/{source_table}/{target_table}%\"\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "            SELECT job_id, status, description\n",
    "            FROM [SHOW CHANGEFEED JOBS] \n",
    "            WHERE description LIKE %s\n",
    "            AND status IN ('running', 'paused')\n",
    "            LIMIT 1\n",
    "        \"\"\", (path_pattern,))\n",
    "        \n",
    "        existing = cur.fetchone()\n",
    "        \n",
    "        if existing:\n",
    "            job_id, status, description = existing\n",
    "            print(f\"‚úÖ Changefeed already exists for this source ‚Üí target mapping\")\n",
    "            print(f\"   Job ID: {job_id}\")\n",
    "            print(f\"   Status: {status}\")\n",
    "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
    "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
    "            if column_family_mode == \"multi_cf\":\n",
    "                print(f\"   Expected: Column family fragments\")\n",
    "            print(f\"\")\n",
    "            print(f\"üí° Tip: Run Cell 10 to generate UPDATE/DELETE events\")\n",
    "            print(f\"   Then check Cell 11 to verify new files appear\")\n",
    "        else:\n",
    "            # Create new changefeed\n",
    "            cur.execute(create_changefeed_sql)\n",
    "            result = cur.fetchone()\n",
    "            job_id = result[0]\n",
    "            \n",
    "            print(f\"‚úÖ Changefeed created\")\n",
    "            print(f\"   Job ID: {job_id}\")\n",
    "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
    "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
    "            print(f\"   Format: Parquet\")\n",
    "            if column_family_mode == \"multi_cf\":\n",
    "                print(f\"   Split column families: TRUE (fragments will be generated)\")\n",
    "            else:\n",
    "                print(f\"   Split column families: FALSE (single file per event)\")\n",
    "            print(f\"   Destination: Azure Blob Storage\")\n",
    "            print(f\"\")\n",
    "            \n",
    "            # Wait for files to appear using helper function\n",
    "            wait_for_changefeed_files(\n",
    "                storage_account_name, storage_account_key, container_name,\n",
    "                source_catalog, source_schema, source_table, target_table,\n",
    "                max_wait=300, check_interval=5\n",
    "            )\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_workload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: RUN CDC WORKLOAD (UPDATE & DELETE)\n",
    "# ============================================================================\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Capture baseline file count BEFORE generating CDC events\n",
    "print(\"üìä Capturing baseline file count...\")\n",
    "result_before = check_azure_files(\n",
    "    storage_account_name, storage_account_key, container_name,\n",
    "    source_catalog, source_schema, source_table, target_table,\n",
    "    verbose=False\n",
    ")\n",
    "files_before = len(result_before['data_files'])\n",
    "print(f\"   Current files: {files_before}\")\n",
    "print()\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    # Get current table state using helper function\n",
    "    stats_before = get_table_stats(conn, source_table)\n",
    "    min_key = stats_before['min_key']\n",
    "    max_key = stats_before['max_key']\n",
    "    count_before = stats_before['count']\n",
    "    \n",
    "    print(f\"üìä Current table state:\")\n",
    "    print(f\"   Min key: {min_key}, Max key: {max_key}, Total rows: {count_before}\")\n",
    "    print()\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        \n",
    "        # 1. INSERT: Add new rows starting from max_key + 1 (using generate_series)\n",
    "        print(f\"‚ûï Running {insert_count} INSERTs (keys {max_key + 1} to {max_key + insert_count})...\")\n",
    "        insert_sql = f\"\"\"\n",
    "        INSERT INTO {source_table} \n",
    "        (ycsb_key, field0, field1, field2, field3, field4, field5, field6, field7, field8, field9)\n",
    "        SELECT \n",
    "            i AS ycsb_key,\n",
    "            'inserted_value_' || i || '_0' AS field0,\n",
    "            'inserted_value_' || i || '_1' AS field1,\n",
    "            'inserted_value_' || i || '_2' AS field2,\n",
    "            'inserted_value_' || i || '_3' AS field3,\n",
    "            'inserted_value_' || i || '_4' AS field4,\n",
    "            'inserted_value_' || i || '_5' AS field5,\n",
    "            'inserted_value_' || i || '_6' AS field6,\n",
    "            'inserted_value_' || i || '_7' AS field7,\n",
    "            'inserted_value_' || i || '_8' AS field8,\n",
    "            'inserted_value_' || i || '_9' AS field9\n",
    "        FROM generate_series(%s, %s) AS i\n",
    "        \"\"\"\n",
    "        cur.execute(insert_sql, (max_key + 1, max_key + insert_count))\n",
    "        \n",
    "        # 2. UPDATE: Update existing rows starting from min_key (single UPDATE statement)\n",
    "        print(f\"üìù Running {update_count} UPDATEs (keys {min_key} to {min_key + update_count - 1})...\")\n",
    "        timestamp = int(time.time())\n",
    "        cur.execute(f\"\"\"\n",
    "            UPDATE {source_table}\n",
    "            SET field0 = %s\n",
    "            WHERE ycsb_key >= %s AND ycsb_key < %s\n",
    "        \"\"\", (f\"updated_at_{timestamp}\", min_key, min_key + update_count))\n",
    "        \n",
    "        # 3. DELETE: Delete oldest rows starting from min_key (single DELETE)\n",
    "        delete_max = min_key + delete_count - 1\n",
    "        print(f\"üóëÔ∏è  Running {delete_count} DELETEs (keys {min_key} to {delete_max})...\")\n",
    "        cur.execute(f\"\"\"\n",
    "            DELETE FROM {source_table}\n",
    "            WHERE ycsb_key >= %s AND ycsb_key <= %s\n",
    "        \"\"\", (min_key, delete_max))\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    # Get final table state using helper function\n",
    "    stats_after = get_table_stats(conn, source_table)\n",
    "    min_key_after = stats_after['min_key']\n",
    "    max_key_after = stats_after['max_key']\n",
    "    count_after = stats_after['count']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Workload complete\")\n",
    "    print(f\"   Inserts: {insert_count}\")\n",
    "    print(f\"   Updates: {update_count}\")\n",
    "    print(f\"   Deletes: {delete_count}\")\n",
    "    print(f\"   Before: {count_before} rows (keys {min_key}-{max_key})\")\n",
    "    print(f\"   After:  {count_after} rows (keys {min_key_after}-{max_key_after})\")\n",
    "    print(f\"   Net change: {count_after - count_before:+d} rows\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    # Wait for new CDC files to appear in Azure (positive confirmation)\n",
    "    print(f\"‚è≥ Waiting for new CDC files to appear in Azure...\")\n",
    "    print(f\"   Baseline: {files_before} files\")\n",
    "    print()\n",
    "    \n",
    "    # Poll for new files (max 90 seconds)\n",
    "    max_wait = 90\n",
    "    check_interval = 10\n",
    "    elapsed = 0\n",
    "    \n",
    "    while elapsed < max_wait:\n",
    "        result = check_azure_files(\n",
    "            storage_account_name, storage_account_key, container_name,\n",
    "            source_catalog, source_schema, source_table, target_table,\n",
    "            verbose=False\n",
    "        )\n",
    "        files_now = len(result['data_files'])\n",
    "        \n",
    "        if files_now > files_before:\n",
    "            print(f\"‚úÖ New CDC files appeared after {elapsed} seconds!\")\n",
    "            print(f\"   Baseline (before workload): {files_before} files\")\n",
    "            print(f\"   Current (after workload): {files_now} files\")\n",
    "            print(f\"   New files generated: {files_now - files_before}\")\n",
    "            break\n",
    "        \n",
    "        print(f\"   Checking... ({elapsed}s elapsed, baseline: {files_before} files)\", end='\\r')\n",
    "        time.sleep(check_interval)\n",
    "        elapsed += check_interval\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - files may still be flushing\")\n",
    "        print(f\"   Run Cell 11 to check manually\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: CHECK AZURE FILES (Optional - Manual Check)\n",
    "# ============================================================================\n",
    "# Use the helper function from Cell 4 to check for files\n",
    "result = check_azure_files(\n",
    "    storage_account_name, storage_account_key, container_name,\n",
    "    source_catalog, source_schema, source_table, target_table,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Provide guidance\n",
    "if len(result['data_files']) == 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  No data files found yet.\")\n",
    "    print(f\"   üí° Possible reasons:\")\n",
    "    print(f\"   - Changefeed not created yet (run Cell 9)\")\n",
    "    print(f\"   - Path configuration mismatch (check Cell 1 variables)\")\n",
    "    print(f\"   - Azure credentials issue (check External Location)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Files are ready! Proceed to Cell 10 to read with Databricks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "read_cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: READ CDC EVENTS IN DATABRICKS\n",
    "# ============================================================================\n",
    "# Select and run CDC ingestion function based on both modes (from Cell 1)\n",
    "# Functions are defined in Cell 5\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"üî∑ CDC Configuration:\")\n",
    "print(f\"   Processing Mode: {cdc_mode}\")\n",
    "print(f\"   Column Family Mode: {column_family_mode}\")\n",
    "print()\n",
    "\n",
    "# Select function based on BOTH cdc_mode and column_family_mode\n",
    "if cdc_mode == \"append_only\" and column_family_mode == \"single_cf\":\n",
    "    print(f\"üìò Running: ingest_cdc_append_only_single_family()\")\n",
    "    print(f\"   - All CDC events will be stored as rows\")\n",
    "    print(f\"   - No column family merging needed\\n\")\n",
    "    \n",
    "    query = ingest_cdc_append_only_single_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        spark=spark\n",
    "    )\n",
    "\n",
    "elif cdc_mode == \"append_only\" and column_family_mode == \"multi_cf\":\n",
    "    print(f\"üìô Running: ingest_cdc_append_only_multi_family()\")\n",
    "    print(f\"   - All CDC events will be stored as rows\")\n",
    "    print(f\"   - Column family fragments will be merged\\n\")\n",
    "    \n",
    "    if not primary_key_columns:\n",
    "        raise ValueError(\"primary_key_columns required for multi_cf mode\")\n",
    "    \n",
    "    query = ingest_cdc_append_only_multi_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        primary_key_columns=primary_key_columns,\n",
    "        spark=spark\n",
    "    )\n",
    "\n",
    "elif cdc_mode == \"update_delete\" and column_family_mode == \"single_cf\":\n",
    "    print(f\"üìó Running: ingest_cdc_with_merge_single_family()\")\n",
    "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
    "    print(f\"   - No column family merging needed\\n\")\n",
    "    \n",
    "    if not primary_key_columns:\n",
    "        raise ValueError(\"primary_key_columns required for update_delete mode\")\n",
    "    \n",
    "    result = ingest_cdc_with_merge_single_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        primary_key_columns=primary_key_columns,\n",
    "        spark=spark\n",
    "    )\n",
    "    \n",
    "    query = result[\"query\"]\n",
    "\n",
    "elif cdc_mode == \"update_delete\" and column_family_mode == \"multi_cf\":\n",
    "    print(f\"üìï Running: ingest_cdc_with_merge_multi_family()\")\n",
    "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
    "    print(f\"   - Column family fragments will be merged\\n\")\n",
    "    \n",
    "    if not primary_key_columns:\n",
    "        raise ValueError(\"primary_key_columns required for update_delete + multi_cf mode\")\n",
    "    \n",
    "    result = ingest_cdc_with_merge_multi_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        primary_key_columns=primary_key_columns,\n",
    "        spark=spark\n",
    "    )\n",
    "    \n",
    "    query = result[\"query\"]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Invalid mode combination:\\n\"\n",
    "        f\"  cdc_mode='{cdc_mode}' (valid: 'append_only', 'update_delete')\\n\"\n",
    "        f\"  column_family_mode='{column_family_mode}' (valid: 'single_cf', 'multi_cf')\\n\"\n",
    "        f\"Change modes in Cell 1.\"\n",
    "    )\n",
    "\n",
    "# Wait for completion (if not already complete)\n",
    "if cdc_mode == \"append_only\":\n",
    "    query.awaitTermination()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"‚úÖ CDC INGESTION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"   Mode: {cdc_mode} + {column_family_mode}\")\n",
    "    print(f\"   Target: {target_catalog}.{target_schema}.{target_table}\")\n",
    "    print()\n",
    "    print(f\"üìä Query your data: SELECT * FROM {target_catalog}.{target_schema}.{target_table}\")\n",
    "else:\n",
    "    # update_delete mode already completed inside the function\n",
    "    print(f\"üìä Query your data: SELECT * FROM {target_catalog}.{target_schema}.{target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: QUERY CDC RESULTS\n",
    "# ============================================================================\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "\n",
    "# Get total count\n",
    "df = spark.read.table(target_table_fqn)\n",
    "total_count = df.count()\n",
    "\n",
    "print(\"üìä CDC Event Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows: {total_count}\")\n",
    "print(f\"CDC Processing Mode: {cdc_mode}\")\n",
    "print(f\"Column Family Mode: {column_family_mode}\")\n",
    "print()\n",
    "\n",
    "# Show operation breakdown (works for both modes now!)\n",
    "print(\"Rows by last CDC operation:\")\n",
    "ops_df = df.groupBy(\"_cdc_operation\").count().orderBy(\"_cdc_operation\")\n",
    "ops_df.show()\n",
    "\n",
    "print(\"\\nüìã Sample rows (showing first 5):\")\n",
    "df.select(\n",
    "    \"ycsb_key\", \n",
    "    \"field0\", \n",
    "    \"_cdc_operation\", \n",
    "    \"_cdc_timestamp\"\n",
    ").orderBy(\"_cdc_timestamp\").show(5, truncate=False)\n",
    "\n",
    "if cdc_mode == \"append_only\":\n",
    "    print(\"\\n‚úÖ CDC data successfully loaded (append_only mode)\")\n",
    "    print(\"   üìä All CDC events stored as rows\")\n",
    "    print(\"   üìä _cdc_operation shows: DELETE, UPSERT for each event\")\n",
    "    print(\"   üìä Row count = all events (including DELETEs and multiple UPDATEs)\")\n",
    "elif cdc_mode == \"update_delete\":\n",
    "    print(\"\\n‚úÖ CDC data successfully loaded (update_delete mode)\")\n",
    "    print(\"   üìä MERGE operations applied: DELETEs removed, UPDATEs applied, INSERTs added\")\n",
    "    print(\"   üìä _cdc_operation shows: UPSERT (last operation on each row)\")\n",
    "    print(\"   üìä Row count = current state (deduplicated)\")\n",
    "\n",
    "print(\"\\nüí° Key Takeaway:\")\n",
    "print(\"   - Using pathGlobFilter to exclude .RESOLVED files avoids DECIMAL errors\")\n",
    "print(\"   - _cdc_operation is preserved in both modes for monitoring\")\n",
    "print(\"\\nüìç Next: Run Cell 14 to verify source and target tables are in sync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: VERIFY SOURCE AND TARGET ARE IN SYNC\n",
    "# ============================================================================\n",
    "print(\"üîç Verifying source and target tables are in sync...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get source table stats (CockroachDB)\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    source_stats = get_table_stats(conn, source_table)\n",
    "    source_sum = get_column_sum(conn, source_table, 'ycsb_key')\n",
    "    \n",
    "    # Get target table stats (Databricks Delta)\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    target_df = spark.read.table(target_table_fqn)\n",
    "    \n",
    "    # Calculate target stats using Spark\n",
    "    from pyspark.sql import functions as F\n",
    "    target_stats_df = target_df.agg(\n",
    "        F.min(\"ycsb_key\").alias(\"min_key\"),\n",
    "        F.max(\"ycsb_key\").alias(\"max_key\"),\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    target_stats = {\n",
    "        'min_key': target_stats_df['min_key'],\n",
    "        'max_key': target_stats_df['max_key'],\n",
    "        'count': target_stats_df['count']\n",
    "    }\n",
    "    \n",
    "    # Calculate sum using Spark helper function\n",
    "    target_sum = get_column_sum_spark(target_df, 'ycsb_key')\n",
    "    \n",
    "    # Display comparison\n",
    "    print(f\"\\nüìä Source Table (CockroachDB): {source_catalog}.{source_schema}.{source_table}\")\n",
    "    print(f\"   Min key: {source_stats['min_key']}\")\n",
    "    print(f\"   Max key: {source_stats['max_key']}\")\n",
    "    print(f\"   Count:   {source_stats['count']}\")\n",
    "    print(f\"   Sum (ycsb_key): {source_sum}\")\n",
    "    \n",
    "    print(f\"\\nüìä Target Table (Databricks Delta): {target_table_fqn}\")\n",
    "    print(f\"   Min key: {target_stats['min_key']}\")\n",
    "    print(f\"   Max key: {target_stats['max_key']}\")\n",
    "    print(f\"   Count:   {target_stats['count']}\")\n",
    "    print(f\"   Sum (ycsb_key): {target_sum}\")\n",
    "    \n",
    "    # Verify all numeric columns (YCSB schema: field0-9)\n",
    "    print(\"\\nüìä Column Sums Comparison (All Fields):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    columns_to_verify = ['ycsb_key', 'field0', 'field1', 'field2', 'field3',\n",
    "                         'field4', 'field5', 'field6', 'field7', 'field8', 'field9']\n",
    "    \n",
    "    all_columns_match = True\n",
    "    for col in columns_to_verify:\n",
    "        try:\n",
    "            source_col_sum = get_column_sum(conn, source_table, col)\n",
    "            target_col_sum = get_column_sum_spark(target_df, col)\n",
    "            col_matches = source_col_sum == target_col_sum\n",
    "            match_icon = \"‚úÖ\" if col_matches else \"‚ùå\"\n",
    "            \n",
    "            # Format with commas for readability\n",
    "            source_str = f\"{source_col_sum:,}\" if source_col_sum else \"NULL\"\n",
    "            target_str = f\"{target_col_sum:,}\" if target_col_sum else \"NULL\"\n",
    "            \n",
    "            print(f\"{match_icon} {col:12s}: Source={source_str:>20s} | Target={target_str:>20s}\")\n",
    "            \n",
    "            if not col_matches:\n",
    "                all_columns_match = False\n",
    "                diff = (target_col_sum or 0) - (source_col_sum or 0)\n",
    "                print(f\"   ‚ö†Ô∏è  Difference: {diff:+,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  {col:12s}: Error calculating sum - {e}\")\n",
    "            all_columns_match = False\n",
    "    \n",
    "    if all_columns_match:\n",
    "        print(\"\\n‚úÖ All column sums match!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Some column sums do not match - check data integrity\")\n",
    "    \n",
    "finally:\n",
    "    # Always close the connection, even if there's an error\n",
    "    conn.close()\n",
    "\n",
    "# Check CDC sync status (mode-aware verification)\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"Mode: {cdc_mode.upper()}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare keys and counts\n",
    "min_key_matches = source_stats['min_key'] == target_stats['min_key']\n",
    "max_key_matches = source_stats['max_key'] == target_stats['max_key']\n",
    "count_matches = source_stats['count'] == target_stats['count']\n",
    "sum_matches = source_sum == target_sum\n",
    "\n",
    "if cdc_mode == \"append_only\":\n",
    "    # In append_only mode, max_key is the key indicator of sync\n",
    "    # Min key and count are expected to differ due to deletes/updates not being applied\n",
    "    \n",
    "    if max_key_matches:\n",
    "        print(\"‚úÖ CDC PIPELINE IS WORKING!\")\n",
    "        print(f\"   Max key matches: {source_stats['max_key']}\")\n",
    "        print(f\"   Sum matches: {source_sum}\")\n",
    "        print()\n",
    "        \n",
    "        if not min_key_matches or not count_matches:\n",
    "            print(\"üìã Append-Only Mode (Expected Differences):\")\n",
    "            if not min_key_matches:\n",
    "                print(f\"   ‚ÑπÔ∏è  Min key differs (Source={source_stats['min_key']}, Target={target_stats['min_key']})\")\n",
    "                print(f\"      ‚Üí This is EXPECTED: DELETE events are captured but not applied\")\n",
    "            if not count_matches:\n",
    "                print(f\"   ‚ÑπÔ∏è  Row count differs (Source={source_stats['count']}, Target={target_stats['count']})\")\n",
    "                print(f\"      ‚Üí This is EXPECTED: All CDC events (INSERT/UPDATE/DELETE) are stored as rows\")\n",
    "            print()\n",
    "            print(\"üí° To apply deletes and updates:\")\n",
    "            print(\"   - Change cdc_mode='update_delete' in Cell 1\")\n",
    "            print(\"   - Or manually deduplicate using SQL window functions\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  MAX KEY MISMATCH - CDC may be lagging\")\n",
    "        print(\"\\n   Key Statistics:\")\n",
    "        print(f\"   - Source max: {source_stats['max_key']}\")\n",
    "        print(f\"   - Target max: {target_stats['max_key']}\")\n",
    "        print(f\"   - Difference: {source_stats['max_key'] - target_stats['max_key']:+d}\")\n",
    "        \n",
    "        print(\"\\n   üí° Possible reasons:\")\n",
    "        print(\"   - Auto Loader hasn't picked up all files yet (re-run Cell 12)\")\n",
    "        print(\"   - MERGE logic issue (check Cell 12 output)\")\n",
    "        print(\"   - Run diagnostic cell (Cell 15) to inspect target table\")\n",
    "\n",
    "elif cdc_mode == \"update_delete\":\n",
    "    # In update_delete mode, ALL stats should match exactly\n",
    "    # DELETE operations are applied, UPDATE operations modify existing rows\n",
    "    \n",
    "    if min_key_matches and max_key_matches and count_matches and sum_matches:\n",
    "        print(\"‚úÖ CDC PIPELINE IS WORKING PERFECTLY!\")\n",
    "        print(\"   All statistics match:\")\n",
    "        print(f\"   ‚úÖ Min key: {source_stats['min_key']}\")\n",
    "        print(f\"   ‚úÖ Max key: {source_stats['max_key']}\")\n",
    "        print(f\"   ‚úÖ Count:   {source_stats['count']}\")\n",
    "        print(f\"   ‚úÖ Sum:     {source_sum}\")\n",
    "        print()\n",
    "        print(\"üìã Update-Delete Mode:\")\n",
    "        print(\"   ‚úÖ DELETE events are applied (rows removed)\")\n",
    "        print(\"   ‚úÖ UPDATE events are applied (rows modified)\")\n",
    "        print(\"   ‚úÖ INSERT events are applied (rows added)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  SYNC MISMATCH - Tables are out of sync\")\n",
    "        print(\"\\n   Key Statistics:\")\n",
    "        if not min_key_matches:\n",
    "            print(f\"   ‚ùå Min key: Source={source_stats['min_key']}, Target={target_stats['min_key']}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Min key: {source_stats['min_key']}\")\n",
    "        \n",
    "        if not max_key_matches:\n",
    "            print(f\"   ‚ùå Max key: Source={source_stats['max_key']}, Target={target_stats['max_key']}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Max key: {source_stats['max_key']}\")\n",
    "        \n",
    "        if not count_matches:\n",
    "            print(f\"   ‚ùå Count: Source={source_stats['count']}, Target={target_stats['count']}\")\n",
    "            print(f\"      Difference: {target_stats['count'] - source_stats['count']:+d} rows\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Count: {source_stats['count']}\")\n",
    "        \n",
    "        if not sum_matches:\n",
    "            print(f\"   ‚ùå Sum: Source={source_sum}, Target={target_sum}\")\n",
    "            print(f\"      Difference: {target_sum - source_sum:+d}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Sum: {source_sum}\")\n",
    "        \n",
    "        print(\"\\n   üí° Possible reasons:\")\n",
    "        print(\"   - Auto Loader hasn't picked up all files yet (re-run Cell 12)\")\n",
    "        print(\"   - MERGE logic issue (check Cell 12 output for errors)\")\n",
    "        print(\"   - DELETE rows stored as data (run Cell 16 to fix)\")\n",
    "        print(\"   - Run diagnostic cell (Cell 15) to inspect target table\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Unknown mode: {cdc_mode}\")\n",
    "    print(\"   Cannot verify sync status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95471f6f",
   "metadata": {},
   "source": [
    "# Stream CockroachDB CDC to Databricks (Azure)\n",
    "\n",
    "This notebook demonstrates how to stream CockroachDB changefeeds to Databricks using Azure Blob Storage.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- CockroachDB cluster (Cloud or self-hosted)\n",
    "- Azure Storage Account with hierarchical namespace enabled\n",
    "- Databricks workspace with Unity Catalog\n",
    "- Unity Catalog External Location configured for your storage account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ee614",
   "metadata": {},
   "source": [
    "## CDC Mode Selection\n",
    "\n",
    "This notebook supports **4 CDC ingestion modes** by combining two independent settings:\n",
    "\n",
    "### 1. CDC Processing Mode (`cdc_mode`)\n",
    "How CDC events are processed in the target table:\n",
    "\n",
    "- **`append_only`**: Store all CDC events as rows (audit log)\n",
    "  - **Behavior**: All events (INSERT/UPDATE/DELETE) are appended as new rows\n",
    "  - **Use case**: History tracking, time-series analysis, audit logs\n",
    "  - **Storage**: Higher (keeps all historical events)\n",
    "\n",
    "- **`update_delete`**: Apply MERGE logic (current state replication)\n",
    "  - **Behavior**: DELETE removes rows, UPDATE modifies rows in-place\n",
    "  - **Use case**: Current state synchronization, production replication\n",
    "  - **Storage**: Lower (only latest state per key)\n",
    "\n",
    "### 2. Column Family Mode (`column_family_mode`)\n",
    "Table structure and changefeed configuration:\n",
    "\n",
    "- **`single_cf`**: Standard table (1 column family, default)\n",
    "  - **Changefeed**: `split_column_families=false`\n",
    "  - **Files**: 1 Parquet file per CDC event\n",
    "  - **Use case**: Most tables, simpler configuration, better performance\n",
    "\n",
    "- **`multi_cf`**: Multiple column families (for wide tables)\n",
    "  - **Changefeed**: `split_column_families=true`\n",
    "  - **Files**: Multiple Parquet files per CDC event (fragments need merging)\n",
    "  - **Use case**: Wide tables (50+ columns), selective column access patterns\n",
    "\n",
    "### Function Selection Matrix\n",
    "\n",
    "The notebook automatically selects the appropriate ingestion function based on your configuration:\n",
    "\n",
    "| CDC Mode | Column Family Mode | Function Called |\n",
    "|----------|-------------------|-----------------|\n",
    "| `append_only` | `single_cf` | `ingest_cdc_append_only_single_family()` |\n",
    "| `append_only` | `multi_cf` | `ingest_cdc_append_only_multi_family()` |\n",
    "| `update_delete` | `single_cf` | `ingest_cdc_with_merge_single_family()` |\n",
    "| `update_delete` | `multi_cf` | `ingest_cdc_with_merge_multi_family()` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6607912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Configuration file path (adjust as needed)\n",
    "config_file = \"/Users/robert.lee/github/lakeflow-community-connectors/sources/cockroachdb/.env/cockroachdb_cdc_tutorial_config.json\"\n",
    "\n",
    "# Try to load from file, fallback to embedded config\n",
    "try:\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(f\"‚úÖ Configuration loaded from: {config_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Using embedded configuration (config file error: {e})\")\n",
    "    config = None\n",
    "\n",
    "# Embedded configuration (fallback)\n",
    "if config is None:\n",
    "    config = {\n",
    "      \"cockroachdb\": {\n",
    "        \"host\": \"replace_me\",\n",
    "        \"port\": 26257,\n",
    "        \"user\": \"replace_me\",\n",
    "        \"password\": \"replace_me\",\n",
    "        \"database\": \"defaultdb\"\n",
    "      },\n",
    "      \"cockroachdb_source\": {\n",
    "        \"catalog\": \"defaultdb\",\n",
    "        \"schema\": \"public\",\n",
    "        \"table_name\": \"usertable\",\n",
    "      },\n",
    "      \"azure_storage\": {\n",
    "        \"account_name\": \"replace_me\",\n",
    "        \"account_key\": \"replace_me\",\n",
    "        \"container_name\": \"changefeed-events\"\n",
    "      },\n",
    "      \"databricks_target\": {\n",
    "        \"catalog\": \"main\",\n",
    "        \"schema\": \"replace_me\",\n",
    "        \"table_name\": \"usertable\",\n",
    "      },\n",
    "      \"cdc_config\": {\n",
    "        \"mode\": \"append_only\",\n",
    "        \"column_family_mode\": \"multi_cf\",\n",
    "        \"primary_key_columns\": [\"ycsb_key\"],\n",
    "        \"auto_suffix_mode_family\": True,\n",
    "      },\n",
    "      \"workload_config\": {\n",
    "        \"snapshot_count\": 10,\n",
    "        \"insert_count\": 10,\n",
    "        \"update_count\": 9,\n",
    "        \"delete_count\": 8,\n",
    "      }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "\n",
    "# Extract configuration values\n",
    "cockroachdb_host = config[\"cockroachdb\"][\"host\"]\n",
    "cockroachdb_port = config[\"cockroachdb\"][\"port\"]\n",
    "cockroachdb_user = config[\"cockroachdb\"][\"user\"]\n",
    "cockroachdb_password = config[\"cockroachdb\"][\"password\"]\n",
    "cockroachdb_database = config[\"cockroachdb\"][\"database\"]\n",
    "\n",
    "source_catalog = config[\"cockroachdb_source\"][\"catalog\"]\n",
    "source_schema = config[\"cockroachdb_source\"][\"schema\"]\n",
    "source_table = config[\"cockroachdb_source\"][\"table_name\"]\n",
    "\n",
    "storage_account_name = config[\"azure_storage\"][\"account_name\"]\n",
    "storage_account_key = config[\"azure_storage\"][\"account_key\"]\n",
    "storage_account_key_encoded = quote(storage_account_key, safe='')\n",
    "container_name = config[\"azure_storage\"][\"container_name\"]\n",
    "\n",
    "target_catalog = config[\"databricks_target\"][\"catalog\"]\n",
    "target_schema = config[\"databricks_target\"][\"schema\"]\n",
    "target_table = config[\"databricks_target\"][\"table_name\"]\n",
    "\n",
    "cdc_mode = config[\"cdc_config\"][\"mode\"]\n",
    "column_family_mode = config[\"cdc_config\"][\"column_family_mode\"]\n",
    "primary_key_columns = config[\"cdc_config\"][\"primary_key_columns\"]\n",
    "\n",
    "snapshot_count = config[\"workload_config\"][\"snapshot_count\"]\n",
    "insert_count = config[\"workload_config\"][\"insert_count\"]\n",
    "update_count = config[\"workload_config\"][\"update_count\"]\n",
    "delete_count = config[\"workload_config\"][\"delete_count\"]\n",
    "\n",
    "# Auto-suffix table names with mode and column family if enabled\n",
    "auto_suffix = config[\"cdc_config\"].get(\"auto_suffix_mode_family\", False)\n",
    "if auto_suffix:\n",
    "    suffix = f\"_{cdc_mode}_{column_family_mode}\"\n",
    "    \n",
    "    # Add suffix to source_table if not already present\n",
    "    if not source_table.endswith(suffix):\n",
    "        source_table = f\"{source_table}{suffix}\"\n",
    "    \n",
    "    # Add suffix to target_table if not already present\n",
    "    if not target_table.endswith(suffix):\n",
    "        target_table = f\"{target_table}{suffix}\"\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   CDC Processing Mode: {cdc_mode}\")\n",
    "print(f\"   Column Family Mode: {column_family_mode}\")\n",
    "print(f\"   Primary Keys: {primary_key_columns}\")\n",
    "print(f\"   Target Table: {target_table}\")\n",
    "print(f\"   CDC Workload: {snapshot_count} snapshot ‚Üí +{insert_count} INSERTs, ~{update_count} UPDATEs, -{delete_count} DELETEs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ace4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "%pip install pg8000 azure-storage-blob --quiet\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45ccf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: CONNECT TO COCKROACHDB\n",
    "# ============================================================================\n",
    "import pg8000\n",
    "import ssl\n",
    "\n",
    "def get_cockroachdb_connection():\n",
    "    \"\"\"Create connection to CockroachDB using pg8000\"\"\"\n",
    "    # Create SSL context (required for CockroachDB Cloud)\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "    \n",
    "    # Parse host (in case port is accidentally included in host string)\n",
    "    host = cockroachdb_host.split(':')[0] if ':' in cockroachdb_host else cockroachdb_host\n",
    "    \n",
    "    conn = pg8000.connect(\n",
    "        user=cockroachdb_user,\n",
    "        password=cockroachdb_password,\n",
    "        host=host,\n",
    "        port=cockroachdb_port,\n",
    "        database=cockroachdb_database,\n",
    "        ssl_context=ssl_context\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_cockroachdb_connection()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT version()\")\n",
    "        version = cur.fetchone()[0]\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"‚úÖ Connected to CockroachDB\")\n",
    "    print(f\"   Version: {version[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbf1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: HELPER FUNCTIONS (CockroachDB & Azure)\n",
    "# ============================================================================\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_table_stats(conn, table_name):\n",
    "    \"\"\"\n",
    "    Get min key, max key, and count for a table.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        table_name: Name of the table\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'min_key', 'max_key', 'count', 'is_empty'\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f\"SELECT MIN(ycsb_key), MAX(ycsb_key), COUNT(*) FROM {table_name}\")\n",
    "        result = cur.fetchone()\n",
    "        min_key, max_key, count = result\n",
    "        \n",
    "        return {\n",
    "            'min_key': min_key,\n",
    "            'max_key': max_key,\n",
    "            'count': count,\n",
    "            'is_empty': min_key is None and max_key is None\n",
    "        }\n",
    "\n",
    "\n",
    "def check_azure_files(storage_account_name, storage_account_key, container_name, \n",
    "                      source_catalog, source_schema, source_table, target_table, \n",
    "                      verbose=True):\n",
    "    \"\"\"\n",
    "    Check for changefeed files in Azure Blob Storage.\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        storage_account_key: Azure storage account key\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_table: Target table name\n",
    "        verbose: Print detailed output\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'data_files' and 'resolved_files' lists\n",
    "    \"\"\"\n",
    "    # Connect to Azure\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "    blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service.get_container_client(container_name)\n",
    "    \n",
    "    # Build path - list ALL files recursively under this changefeed path\n",
    "    prefix = f\"parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}/\"\n",
    "    \n",
    "    # List all blobs recursively (no date filtering)\n",
    "    blobs = list(container_client.list_blobs(name_starts_with=prefix))\n",
    "    \n",
    "    # Categorize files (using same filtering logic as cockroachdb.py)\n",
    "    # Data files: .parquet files, excluding:\n",
    "    #   - .RESOLVED files (CDC watermarks)\n",
    "    #   - _metadata/ directory (schema files)\n",
    "    #   - Files starting with _ (_SUCCESS, _committed_*, etc.)\n",
    "    data_files = [\n",
    "        b for b in blobs \n",
    "        if b.name.endswith('.parquet') \n",
    "        and '.RESOLVED' not in b.name\n",
    "        and '/_metadata/' not in b.name\n",
    "        and not b.name.split('/')[-1].startswith('_')\n",
    "    ]\n",
    "    resolved_files = [b for b in blobs if '.RESOLVED' in b.name]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìÅ Files in Azure changefeed path:\")\n",
    "        print(f\"   Path: {prefix}\")\n",
    "        print(f\"   üìÑ Data files: {len(data_files)}\")\n",
    "        print(f\"   üïê Resolved files: {len(resolved_files)}\")\n",
    "        print(f\"   üìä Total: {len(blobs)}\")\n",
    "        \n",
    "        if data_files:\n",
    "            print(f\"\\n   Example data file:\")\n",
    "            print(f\"   {data_files[0].name}\")\n",
    "    \n",
    "    return {\n",
    "        'data_files': data_files,\n",
    "        'resolved_files': resolved_files,\n",
    "        'total': len(blobs)\n",
    "    }\n",
    "\n",
    "\n",
    "def wait_for_changefeed_files(storage_account_name, storage_account_key, container_name,\n",
    "                               source_catalog, source_schema, source_table, target_table,\n",
    "                               max_wait=120, check_interval=5):\n",
    "    \"\"\"\n",
    "    Wait for changefeed files to appear in Azure with timeout.\n",
    "    \n",
    "    Args:\n",
    "        max_wait: Maximum seconds to wait (default: 120)\n",
    "        check_interval: Seconds between checks (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        True if files found, False if timeout\n",
    "    \"\"\"\n",
    "    print(f\"‚è≥ Waiting for initial snapshot files to appear in Azure...\")\n",
    "    \n",
    "    elapsed = 0\n",
    "    while elapsed < max_wait:\n",
    "        result = check_azure_files(\n",
    "            storage_account_name, storage_account_key, container_name,\n",
    "            source_catalog, source_schema, source_table, target_table,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        if result['data_files']:\n",
    "            print(f\"‚úÖ Files appeared after {elapsed} seconds!\")\n",
    "            print(f\"   Found {len(result['data_files'])} data files\")\n",
    "            print(f\"   Example: {result['data_files'][0].name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"   Checking... ({elapsed}s elapsed)\", end='\\r')\n",
    "        time.sleep(check_interval)\n",
    "        elapsed += check_interval\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - files may still be generating\")\n",
    "    print(f\"   Run Cell 9 to check manually\")\n",
    "    return False\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded (CockroachDB & Azure)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f87ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: DATABRICKS STREAMING MODES (CDC Ingestion Functions)\n",
    "# ============================================================================\n",
    "# This cell contains different CDC ingestion functions for various scenarios.\n",
    "# Select the appropriate function based on your use case:\n",
    "#\n",
    "# 1. ingest_cdc_append_only_single_family() - Simple append_only (this tutorial)\n",
    "# 2. ingest_cdc_with_merge_single_family() - Apply updates/deletes (implemented below)\n",
    "# 3. TODO: ingest_cdc_append_only_multi_family() - Column families support\n",
    "# 4. TODO: ingest_cdc_with_merge_multi_family() - Full CDC with column families\n",
    "# ============================================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def ingest_cdc_append_only_single_family(\n",
    "    storage_account_name, container_name, \n",
    "    source_catalog, source_schema, source_table, \n",
    "    target_catalog, target_schema, target_table,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingest CDC events in APPEND-ONLY mode for single column family tables.\n",
    "    \n",
    "    This function:\n",
    "    - Reads Parquet CDC files from Azure using Auto Loader\n",
    "    - Filters out .RESOLVED files and metadata\n",
    "    - Transforms CockroachDB CDC columns (__crdb__*) to standard format\n",
    "    - Writes all events (INSERT/UPDATE/DELETE) as rows to Delta table\n",
    "    - Does NOT apply deletes or deduplicate updates (append_only)\n",
    "    \n",
    "    Use this for:\n",
    "    - Audit logs and full history tracking\n",
    "    - Tables WITHOUT column families (split_column_families=false)\n",
    "    - Simple CDC pipelines without MERGE logic\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_catalog: Databricks catalog\n",
    "        target_schema: Databricks schema\n",
    "        target_table: Target table name\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        StreamingQuery object\n",
    "    \"\"\"\n",
    "    # Build paths\n",
    "    source_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "    checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}\"\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    \n",
    "    print(\"üìñ Ingesting CDC events (Append-Only Mode)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Mode: APPEND-ONLY (Single Column Family)\")\n",
    "    print(f\"Source: {source_catalog}.{source_schema}.{source_table} (CockroachDB)\")\n",
    "    print(f\"Target: {target_table_fqn} (Databricks Delta)\")\n",
    "    print(f\"Source path: {source_path}/ (all dates, recursively)\")\n",
    "    print(f\"File filter: *{source_table}*.parquet\")\n",
    "    print(f\"   ‚úÖ Includes: Data files\")\n",
    "    print(f\"   ‚ùå Excludes: .RESOLVED, _metadata/, _SUCCESS, etc.\")\n",
    "    print()\n",
    "    \n",
    "    # Read with Auto Loader (production-grade filtering)\n",
    "    raw_df = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/schema\")\n",
    "        .option(\"pathGlobFilter\", f\"*{source_table}*.parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Schema inferred from data files\")\n",
    "    print(\"   (Filtering matches cockroachdb.py production code)\")\n",
    "    print()\n",
    "    \n",
    "    # Transform: CockroachDB CDC ‚Üí Standard CDC format\n",
    "    df = raw_df.select(\n",
    "        \"*\",\n",
    "        # Convert __crdb__updated (nanoseconds) to timestamp\n",
    "        F.from_unixtime(\n",
    "            F.col(\"__crdb__updated\").cast(\"double\").cast(\"bigint\") / 1000000000\n",
    "        ).cast(\"timestamp\").alias(\"_cdc_timestamp\"),\n",
    "        # Map event type\n",
    "        F.when(F.col(\"__crdb__event_type\") == \"d\", \"DELETE\")\n",
    "         .otherwise(\"UPSERT\")\n",
    "         .alias(\"_cdc_operation\")\n",
    "    ).drop(\"__crdb__updated\", \"__crdb__event_type\")\n",
    "    \n",
    "    # Write to Delta table (append_only)\n",
    "    query = (df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}/data\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(target_table_fqn)\n",
    "    )\n",
    "    \n",
    "    print(\"‚è≥ Processing CDC events...\")\n",
    "    return query\n",
    "\n",
    "\n",
    "def ingest_cdc_with_merge_single_family(\n",
    "    storage_account_name, container_name,\n",
    "    source_catalog, source_schema, source_table,\n",
    "    target_catalog, target_schema, target_table,\n",
    "    primary_key_columns,  # NEW: Required for MERGE join condition\n",
    "    spark\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingest CDC events with MERGE logic for single column family tables.\n",
    "    \n",
    "    This function:\n",
    "    - Reads Parquet CDC files from Azure using Auto Loader\n",
    "    - Filters out .RESOLVED files and metadata\n",
    "    - Transforms CockroachDB CDC columns (__crdb__*) to standard format\n",
    "    - Deduplicates events within each microbatch (handles column family fragments)\n",
    "    - Applies MERGE logic to target Delta table:\n",
    "      * UPDATE: When key exists and timestamp is newer\n",
    "      * DELETE: When key exists and operation is DELETE\n",
    "      * INSERT: When key doesn't exist and operation is UPSERT\n",
    "    - Preserves _cdc_operation column for monitoring and observability\n",
    "    \n",
    "    Use this for:\n",
    "    - Applications needing current state (not history)\n",
    "    - Tables WITHOUT column families (split_column_families=false)\n",
    "    - Production CDC pipelines with UPDATE/DELETE support\n",
    "    - Lower storage requirements (only latest state)\n",
    "    \n",
    "    Target table will contain:\n",
    "    - All data columns from source\n",
    "    - _cdc_operation: \"UPSERT\" (shows last operation on each row)\n",
    "    - _cdc_timestamp: Timestamp of last CDC event\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_catalog: Databricks catalog\n",
    "        target_schema: Databricks schema\n",
    "        target_table: Target table name\n",
    "        primary_key_columns: List of primary key column names (e.g., ['ycsb_key'])\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        Dict with query, staging_table, target_table, raw_count, deduped_count, merged\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F, Window\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    # Build paths\n",
    "    source_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "    checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}_merge\"\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    \n",
    "    print(\"üìñ Ingesting CDC events (MERGE Mode)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Mode: MERGE (Apply UPDATE/DELETE)\")\n",
    "    print(f\"Source: {source_catalog}.{source_schema}.{source_table} (CockroachDB)\")\n",
    "    print(f\"Target: {target_table_fqn} (Databricks Delta)\")\n",
    "    print(f\"Primary keys: {primary_key_columns}\")\n",
    "    print(f\"Source path: {source_path}/ (all dates, recursively)\")\n",
    "    print(f\"File filter: *{source_table}*.parquet\")\n",
    "    print(f\"   ‚úÖ Includes: Data files\")\n",
    "    print(f\"   ‚ùå Excludes: .RESOLVED, _metadata/, _SUCCESS, etc.\")\n",
    "    print()\n",
    "    \n",
    "    # Read with Auto Loader (production-grade filtering)\n",
    "    raw_df = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/schema\")\n",
    "        .option(\"pathGlobFilter\", f\"*{source_table}*.parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Schema inferred from data files\")\n",
    "    print(\"   (Filtering matches cockroachdb.py production code)\")\n",
    "    print()\n",
    "    \n",
    "    # Transform: CockroachDB CDC ‚Üí Standard CDC format\n",
    "    transformed_df = raw_df.select(\n",
    "        \"*\",\n",
    "        # Convert __crdb__updated (nanoseconds) to timestamp\n",
    "        F.from_unixtime(\n",
    "            F.col(\"__crdb__updated\").cast(\"double\").cast(\"bigint\") / 1000000000\n",
    "        ).cast(\"timestamp\").alias(\"_cdc_timestamp\"),\n",
    "        # Map event type\n",
    "        F.when(F.col(\"__crdb__event_type\") == \"d\", \"DELETE\")\n",
    "         .otherwise(\"UPSERT\")\n",
    "         .alias(\"_cdc_operation\")\n",
    "    ).drop(\"__crdb__event_type\", \"__crdb__updated\")\n",
    "    \n",
    "    print(\"‚úÖ CDC transformations applied (streaming compatible)\")\n",
    "    print(\"   ‚ÑπÔ∏è  Deduplication will happen in Stage 2 (batch mode)\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 1: Stream to Staging Table (Serverless Compatible - No Python UDFs)\n",
    "    # ========================================================================\n",
    "    staging_table_fqn = f\"{target_table_fqn}_staging\"\n",
    "    \n",
    "    print(\"üî∑ STAGE 1: Streaming to staging table (no Python UDFs)\")\n",
    "    print(f\"   Staging: {staging_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Write to staging table (pure Spark, no foreachBatch, no window functions)\n",
    "    query = (transformed_df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}/data\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(staging_table_fqn)  # ‚Üê No Python UDFs, Serverless compatible!\n",
    "    )\n",
    "    \n",
    "    print(\"‚è≥ Streaming CDC events to staging table...\")\n",
    "    query.awaitTermination()\n",
    "    print(\"‚úÖ Stream completed\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 2: Batch MERGE from Staging to Target (Runs on Driver)\n",
    "    # ========================================================================\n",
    "    print(\"üî∑ STAGE 2: Applying MERGE logic (batch operation)\")\n",
    "    print(f\"   Source: {staging_table_fqn}\")\n",
    "    print(f\"   Target: {target_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Read staging table (batch mode - window functions allowed!)\n",
    "    staging_df_raw = spark.read.table(staging_table_fqn)\n",
    "    staging_count_raw = staging_df_raw.count()\n",
    "    print(f\"   üìä Raw staging events: {staging_count_raw}\")\n",
    "    \n",
    "    if staging_count_raw == 0:\n",
    "        print(\"   ‚ÑπÔ∏è  No new events to process\")\n",
    "        return {\"query\": query, \"staging_table\": staging_table_fqn, \"merged\": 0}\n",
    "    \n",
    "    # Deduplicate by primary key (batch mode, matches cockroachdb.py logic)\n",
    "    # Keep only LATEST event per primary key based on timestamp\n",
    "    from pyspark.sql import Window\n",
    "    \n",
    "    print(f\"   üîÑ Deduplicating by primary keys: {primary_key_columns}...\")\n",
    "    window_spec = Window.partitionBy(*primary_key_columns).orderBy(F.col(\"_cdc_timestamp\").desc())\n",
    "    staging_df = (staging_df_raw\n",
    "        .withColumn(\"_row_num\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"_row_num\") == 1)\n",
    "        .drop(\"_row_num\")\n",
    "    )\n",
    "    \n",
    "    staging_count = staging_df.count()\n",
    "    duplicates_removed = staging_count_raw - staging_count\n",
    "    print(f\"   ‚úÖ Deduplicated: {staging_count} unique events ({duplicates_removed} duplicates removed)\")\n",
    "    \n",
    "    if staging_count == 0:\n",
    "        print(\"   ‚ÑπÔ∏è  All events were duplicates\")\n",
    "        return {\"query\": query, \"staging_table\": staging_table_fqn, \"merged\": 0}\n",
    "    \n",
    "    # Create target table if it doesn't exist\n",
    "    if not spark.catalog.tableExists(target_table_fqn):\n",
    "        print(f\"   üìù Creating new target table: {target_table_fqn}\")\n",
    "        \n",
    "        # CRITICAL: Proper DELETE handling for initial table creation\n",
    "        # This matches cockroachdb.py reference implementation\n",
    "        \n",
    "        # 1. Get keys that have DELETE events\n",
    "        delete_keys = staging_df.filter(F.col(\"_cdc_operation\") == \"DELETE\") \\\n",
    "            .select(*primary_key_columns) \\\n",
    "            .distinct()\n",
    "        delete_count = delete_keys.count()\n",
    "        \n",
    "        # 2. Get all non-DELETE rows\n",
    "        active_rows = staging_df.filter(F.col(\"_cdc_operation\") != \"DELETE\")\n",
    "        active_count = active_rows.count()\n",
    "        \n",
    "        # 3. Exclude rows with keys that are deleted (left anti join)\n",
    "        # This handles case where key has UPSERT at T1, DELETE at T2\n",
    "        rows_after_delete = active_rows.join(\n",
    "            delete_keys,\n",
    "            on=primary_key_columns,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        after_delete_count = rows_after_delete.count()\n",
    "        \n",
    "        # Note: staging_df is already deduplicated, so final_rows = rows_after_delete\n",
    "        final_rows = rows_after_delete\n",
    "        final_count = after_delete_count\n",
    "        \n",
    "        if delete_count > 0:\n",
    "            print(f\"   ‚ÑπÔ∏è  Found {delete_count} DELETE events\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Active rows before DELETE: {active_count}\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Active rows after DELETE: {after_delete_count}\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Rows removed by DELETE: {active_count - after_delete_count}\")\n",
    "        \n",
    "        # Keep ALL columns including _cdc_operation for monitoring\n",
    "        final_rows.write.format(\"delta\").saveAsTable(target_table_fqn)\n",
    "        merged_count = final_count\n",
    "        print(f\"   ‚úÖ Created table with {merged_count} initial rows\")\n",
    "        print(f\"      Schema includes _cdc_operation for observability\\n\")\n",
    "    else:\n",
    "        # Get Delta table and apply MERGE\n",
    "        delta_table = DeltaTable.forName(spark, target_table_fqn)\n",
    "        \n",
    "        # Check if _cdc_operation exists in target (might be missing from old tables)\n",
    "        target_columns = set(spark.read.table(target_table_fqn).columns)\n",
    "        if \"_cdc_operation\" not in target_columns:\n",
    "            print(f\"   ‚ö†Ô∏è  Target table missing _cdc_operation column (old schema)\")\n",
    "            print(f\"   üîß Adding _cdc_operation column for observability...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                ALTER TABLE {target_table_fqn} \n",
    "                ADD COLUMN _cdc_operation STRING\n",
    "            \"\"\")\n",
    "            print(f\"   ‚úÖ Column added\\n\")\n",
    "        \n",
    "        # Build join condition dynamically\n",
    "        join_condition = \" AND \".join([f\"target.{col} = source.{col}\" for col in primary_key_columns])\n",
    "        \n",
    "        # Get all data columns (KEEP _cdc_operation for observability!)\n",
    "        data_columns = [col for col in staging_df.columns]\n",
    "        \n",
    "        # Build UPDATE/INSERT clauses dynamically\n",
    "        update_set = {col: f\"source.{col}\" for col in data_columns}\n",
    "        insert_values = {col: f\"source.{col}\" for col in data_columns}\n",
    "        \n",
    "        print(f\"   üîÑ Executing MERGE...\")\n",
    "        print(f\"      Join: {join_condition}\")\n",
    "        print(f\"      ‚ÑπÔ∏è  _cdc_operation will be preserved for monitoring\")\n",
    "        \n",
    "        # Apply MERGE (runs on driver, not workers)\n",
    "        (delta_table.alias(\"target\").merge(\n",
    "            staging_df.alias(\"source\"),\n",
    "            join_condition\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"source._cdc_operation = 'UPSERT' AND source._cdc_timestamp > target._cdc_timestamp\",\n",
    "            set=update_set\n",
    "        )\n",
    "        .whenMatchedDelete(\n",
    "            condition=\"source._cdc_operation = 'DELETE'\"\n",
    "        )\n",
    "        .whenNotMatchedInsert(\n",
    "            condition=\"source._cdc_operation = 'UPSERT'\",\n",
    "            values=insert_values\n",
    "        )\n",
    "        .execute())\n",
    "        \n",
    "        merged_count = staging_count\n",
    "        print(f\"   ‚úÖ MERGE complete: processed {merged_count} events\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ CDC INGESTION COMPLETE (TWO-STAGE MERGE)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä Raw events: {staging_count_raw}\")\n",
    "    print(f\"üìä After deduplication: {staging_count} unique events\")\n",
    "    print(f\"üìä Staging table: {staging_table_fqn}\")\n",
    "    print(f\"üìä Target table:  {target_table_fqn}\")\n",
    "    print()\n",
    "    print(\"üìã Target table includes:\")\n",
    "    print(\"   - All data columns from source\")\n",
    "    print(\"   - _cdc_operation: UPSERT (for monitoring)\")\n",
    "    print(\"   - _cdc_timestamp: Last CDC event timestamp\")\n",
    "    print()\n",
    "    print(\"üí° TIP: Staging table can be dropped after successful MERGE:\")\n",
    "    print(f\"   spark.sql('DROP TABLE IF EXISTS {staging_table_fqn}')\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"staging_table\": staging_table_fqn,\n",
    "        \"target_table\": target_table_fqn,\n",
    "        \"raw_count\": staging_count_raw,\n",
    "        \"deduped_count\": staging_count,\n",
    "        \"merged\": merged_count\n",
    "    }\n",
    "\n",
    "\n",
    "def merge_column_family_fragments(df, primary_key_columns, spark):\n",
    "    \"\"\"\n",
    "    Merge column family fragments into complete rows (streaming-compatible).\n",
    "    \n",
    "    When split_column_families=true, CockroachDB creates multiple CDC events per row update,\n",
    "    one for each column family. This function merges these fragments by:\n",
    "    1. Grouping by (primary_key + _cdc_timestamp + _cdc_operation)\n",
    "    2. Using first(col, ignorenulls=True) to coalesce NULL values from different fragments\n",
    "    \n",
    "    Each fragment has:\n",
    "    - Primary key columns (always present)\n",
    "    - Data for ONE column family (other columns are NULL)\n",
    "    - Same _cdc_timestamp and _cdc_operation\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame with potential column family fragments\n",
    "        primary_key_columns: List of primary key column names (e.g., ['ycsb_key'])\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        Merged DataFrame with complete rows\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    # Get all columns\n",
    "    all_columns = df.columns\n",
    "    \n",
    "    # Metadata columns to preserve (not aggregate as data)\n",
    "    metadata_columns = {'_cdc_operation', '_cdc_timestamp', '_rescued_data'}\n",
    "    \n",
    "    # Data columns = all columns except PK and metadata\n",
    "    data_columns = [\n",
    "        col for col in all_columns\n",
    "        if col not in primary_key_columns \n",
    "        and col not in metadata_columns\n",
    "    ]\n",
    "    \n",
    "    # Group by: PK + timestamp + operation (preserves all distinct CDC events)\n",
    "    group_by_cols = primary_key_columns + ['_cdc_timestamp', '_cdc_operation']\n",
    "    \n",
    "    # Build aggregation expressions\n",
    "    # Use first(col, ignorenulls=True) to merge NULL values from fragments\n",
    "    agg_exprs = [\n",
    "        F.first(col, ignorenulls=True).alias(col) \n",
    "        for col in data_columns\n",
    "    ]\n",
    "    \n",
    "    # Add metadata columns that aren't in the grouping key\n",
    "    for col in metadata_columns:\n",
    "        if col in all_columns and col not in group_by_cols:\n",
    "            agg_exprs.append(F.first(col, ignorenulls=True).alias(col))\n",
    "    \n",
    "    # Apply merge\n",
    "    df_merged = df.groupBy(*group_by_cols).agg(*agg_exprs)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def ingest_cdc_append_only_multi_family(\n",
    "    storage_account_name, container_name,\n",
    "    source_catalog, source_schema, source_table,\n",
    "    target_catalog, target_schema, target_table,\n",
    "    primary_key_columns,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingest CDC events in APPEND-ONLY mode with COLUMN FAMILY support.\n",
    "    \n",
    "    **Two-Stage Approach (Serverless Compatible)**:\n",
    "    - Stage 1: Stream raw CDC events to staging table (no aggregations)\n",
    "    - Stage 2: Batch merge column family fragments to target table\n",
    "    \n",
    "    This function:\n",
    "    - Reads Parquet CDC files from Azure using Auto Loader\n",
    "    - Filters out .RESOLVED files and metadata\n",
    "    - Transforms CockroachDB CDC columns (__crdb__*) to standard format\n",
    "    - MERGES column family fragments (split_column_families=true) in batch mode\n",
    "    - Writes all events (INSERT/UPDATE/DELETE) as rows to Delta table\n",
    "    - Does NOT apply deletes or deduplicate updates (append_only)\n",
    "    \n",
    "    Use this for:\n",
    "    - Audit logs with column family tables\n",
    "    - Tables WITH column families (split_column_families=true)\n",
    "    - Full history tracking with wide tables\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_catalog: Databricks catalog\n",
    "        target_schema: Databricks schema\n",
    "        target_table: Target table name\n",
    "        primary_key_columns: List of primary key column names (required for fragment merging)\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        StreamingQuery object\n",
    "    \"\"\"\n",
    "    # Build paths\n",
    "    source_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "    checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}\"\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    \n",
    "    print(\"üìñ Ingesting CDC events (Append-Only + Column Families)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Mode: APPEND-ONLY (Multi Column Family)\")\n",
    "    print(f\"Source: {source_catalog}.{source_schema}.{source_table} (CockroachDB)\")\n",
    "    print(f\"Target: {target_table_fqn} (Databricks Delta)\")\n",
    "    print(f\"Primary keys: {primary_key_columns}\")\n",
    "    print(f\"Source path: {source_path}/ (all dates, recursively)\")\n",
    "    print(f\"File filter: *{source_table}*.parquet\")\n",
    "    print()\n",
    "    \n",
    "    # Read with Auto Loader\n",
    "    raw_df = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/schema\")\n",
    "        .option(\"pathGlobFilter\", f\"*{source_table}*.parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Schema inferred from data files\")\n",
    "    print()\n",
    "    \n",
    "    # Transform: CockroachDB CDC ‚Üí Standard CDC format\n",
    "    transformed_df = raw_df.select(\n",
    "        \"*\",\n",
    "        F.from_unixtime(\n",
    "            F.col(\"__crdb__updated\").cast(\"double\").cast(\"bigint\") / 1000000000\n",
    "        ).cast(\"timestamp\").alias(\"_cdc_timestamp\"),\n",
    "        F.when(F.col(\"__crdb__event_type\") == \"d\", \"DELETE\")\n",
    "         .otherwise(\"UPSERT\")\n",
    "         .alias(\"_cdc_operation\")\n",
    "    ).drop(\"__crdb__event_type\", \"__crdb__updated\")\n",
    "    \n",
    "    print(\"‚úÖ CDC transformations applied (streaming compatible)\")\n",
    "    print(\"   ‚ÑπÔ∏è  Column family merge will happen in Stage 2 (batch mode)\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 1: Stream to Staging Table (Serverless Compatible - No Aggregations)\n",
    "    # ========================================================================\n",
    "    staging_table_fqn = f\"{target_table_fqn}_staging_cf\"\n",
    "    \n",
    "    print(\"üî∑ STAGE 1: Streaming to staging table (no aggregations)\")\n",
    "    print(f\"   Staging: {staging_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Write to staging table (pure Spark, no aggregations)\n",
    "    query = (transformed_df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}/data\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(staging_table_fqn)  # ‚Üê No aggregations, Serverless compatible!\n",
    "    )\n",
    "    \n",
    "    print(\"‚è≥ Streaming CDC events to staging table...\")\n",
    "    query.awaitTermination()\n",
    "    print(\"‚úÖ Stream completed\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 2: Merge Column Families in Batch Mode\n",
    "    # ========================================================================\n",
    "    print(\"üî∑ STAGE 2: Merging column family fragments (batch mode)\")\n",
    "    print(f\"   Reading from staging: {staging_table_fqn}\")\n",
    "    print(f\"   Writing to target: {target_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Read staging table in batch mode\n",
    "    staging_df = spark.table(staging_table_fqn)\n",
    "    \n",
    "    # Merge column family fragments (batch mode - no streaming limitations!)\n",
    "    print(\"üîß Merging column family fragments...\")\n",
    "    print(f\"   Grouping by: {primary_key_columns} + _cdc_timestamp + _cdc_operation\")\n",
    "    print(f\"   Using first(col, ignorenulls=True) to coalesce fragments\")\n",
    "    merged_df = merge_column_family_fragments(staging_df, primary_key_columns, spark)\n",
    "    print(\"‚úÖ Column family fragments merged\")\n",
    "    print()\n",
    "    \n",
    "    # Write to final target table (batch mode, append_only)\n",
    "    print(f\"üíæ Writing merged events to {target_table_fqn}...\")\n",
    "    merged_df.write.format(\"delta\").mode(\"append\").saveAsTable(target_table_fqn)\n",
    "    print(\"‚úÖ Append-only write complete\")\n",
    "    print()\n",
    "    \n",
    "    # Clean up staging table\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {staging_table_fqn}\")\n",
    "    print(f\"üßπ Staging table dropped: {staging_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    return query\n",
    "\n",
    "\n",
    "def ingest_cdc_with_merge_multi_family(\n",
    "    storage_account_name, container_name,\n",
    "    source_catalog, source_schema, source_table,\n",
    "    target_catalog, target_schema, target_table,\n",
    "    primary_key_columns,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingest CDC events with MERGE logic and COLUMN FAMILY support.\n",
    "    \n",
    "    **Two-Stage Approach (Serverless Compatible)**:\n",
    "    - Stage 1: Stream raw CDC events to staging table (no aggregations)\n",
    "    - Stage 2: Batch merge column families + deduplicate + MERGE to target\n",
    "    \n",
    "    This function:\n",
    "    - Reads Parquet CDC files from Azure using Auto Loader\n",
    "    - Filters out .RESOLVED files and metadata\n",
    "    - Transforms CockroachDB CDC columns (__crdb__*) to standard format\n",
    "    - MERGES column family fragments (split_column_families=true) in batch mode\n",
    "    - Streams to staging table (Serverless-compatible)\n",
    "    - Deduplicates by primary key in batch mode\n",
    "    - Applies MERGE logic to target Delta table\n",
    "    \n",
    "    Use this for:\n",
    "    - Current state replication with column families\n",
    "    - Tables WITH column families (split_column_families=true)\n",
    "    - Production CDC with UPDATE/DELETE support\n",
    "    \n",
    "    Target table will contain:\n",
    "    - All data columns from source\n",
    "    - _cdc_operation: \"UPSERT\" (shows last operation)\n",
    "    - _cdc_timestamp: Timestamp of last CDC event\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_catalog: Databricks catalog\n",
    "        target_schema: Databricks schema\n",
    "        target_table: Target table name\n",
    "        primary_key_columns: List of primary key column names (required for fragments + MERGE)\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        Dict with query, staging_table, target_table, raw_count, deduped_count, merged\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F, Window\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    # Build paths\n",
    "    source_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "    checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}_merge_cf\"\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    \n",
    "    print(\"üìñ Ingesting CDC events (MERGE + Column Families)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Mode: MERGE with Column Families\")\n",
    "    print(f\"Source: {source_catalog}.{source_schema}.{source_table} (CockroachDB)\")\n",
    "    print(f\"Target: {target_table_fqn} (Databricks Delta)\")\n",
    "    print(f\"Primary keys: {primary_key_columns}\")\n",
    "    print(f\"Source path: {source_path}/ (all dates, recursively)\")\n",
    "    print()\n",
    "    \n",
    "    # Read with Auto Loader\n",
    "    raw_df = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/schema\")\n",
    "        .option(\"pathGlobFilter\", f\"*{source_table}*.parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Schema inferred from data files\")\n",
    "    print()\n",
    "    \n",
    "    # Transform: CockroachDB CDC ‚Üí Standard CDC format\n",
    "    transformed_df = raw_df.select(\n",
    "        \"*\",\n",
    "        F.from_unixtime(\n",
    "            F.col(\"__crdb__updated\").cast(\"double\").cast(\"bigint\") / 1000000000\n",
    "        ).cast(\"timestamp\").alias(\"_cdc_timestamp\"),\n",
    "        F.when(F.col(\"__crdb__event_type\") == \"d\", \"DELETE\")\n",
    "         .otherwise(\"UPSERT\")\n",
    "         .alias(\"_cdc_operation\")\n",
    "    ).drop(\"__crdb__event_type\", \"__crdb__updated\")\n",
    "    \n",
    "    print(\"‚úÖ CDC transformations applied (streaming compatible)\")\n",
    "    print(\"   ‚ÑπÔ∏è  Column family merge will happen in Stage 2 (batch mode)\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 1: Stream to Staging Table (Serverless Compatible - No Aggregations)\n",
    "    # ========================================================================\n",
    "    staging_table_fqn = f\"{target_table_fqn}_staging_cf\"\n",
    "    \n",
    "    print(\"üî∑ STAGE 1: Streaming to staging table (no aggregations)\")\n",
    "    print(f\"   Staging: {staging_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    query = (transformed_df.writeStream  # ‚Üê Stream raw events (no column family merge yet)\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}/data\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(staging_table_fqn)  # ‚Üê No aggregations, Serverless compatible!\n",
    "    )\n",
    "    \n",
    "    print(\"‚è≥ Streaming CDC events to staging table...\")\n",
    "    query.awaitTermination()\n",
    "    print(\"‚úÖ Stream completed\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 2: Batch MERGE from Staging to Target\n",
    "    # ========================================================================\n",
    "    print(\"üî∑ STAGE 2: Applying MERGE logic (batch operation)\")\n",
    "    print(f\"   Source: {staging_table_fqn}\")\n",
    "    print(f\"   Target: {target_table_fqn}\")\n",
    "    print()\n",
    "    \n",
    "    # Read staging table (batch mode)\n",
    "    staging_df_raw = spark.read.table(staging_table_fqn)\n",
    "    staging_count_raw = staging_df_raw.count()\n",
    "    print(f\"   üìä Raw staging events: {staging_count_raw}\")\n",
    "    \n",
    "    if staging_count_raw == 0:\n",
    "        print(\"   ‚ÑπÔ∏è  No new events to process\")\n",
    "        return {\"query\": query, \"staging_table\": staging_table_fqn, \"merged\": 0}\n",
    "    \n",
    "    # Merge column family fragments (batch mode - no streaming limitations!)\n",
    "    print(f\"   üîß Merging column family fragments...\")\n",
    "    print(f\"      Grouping by: {primary_key_columns} + _cdc_timestamp + _cdc_operation\")\n",
    "    staging_df_merged = merge_column_family_fragments(staging_df_raw, primary_key_columns, spark)\n",
    "    print(f\"   ‚úÖ Column family fragments merged\")\n",
    "    \n",
    "    # Deduplicate by primary key (keep latest event)\n",
    "    print(f\"   üîÑ Deduplicating by primary keys: {primary_key_columns}...\")\n",
    "    window_spec = Window.partitionBy(*primary_key_columns).orderBy(F.col(\"_cdc_timestamp\").desc())\n",
    "    staging_df = (staging_df_merged  # ‚Üê Use merged DataFrame\n",
    "        .withColumn(\"_row_num\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"_row_num\") == 1)\n",
    "        .drop(\"_row_num\")\n",
    "    )\n",
    "    \n",
    "    staging_count = staging_df.count()\n",
    "    fragments_removed = staging_count_raw - staging_df_merged.count()\n",
    "    duplicates_removed = staging_df_merged.count() - staging_count\n",
    "    print(f\"   ‚úÖ Column family fragments coalesced: {fragments_removed} fragments merged\")\n",
    "    print(f\"   ‚úÖ Deduplicated: {staging_count} unique events ({duplicates_removed} duplicates removed)\")\n",
    "    \n",
    "    if staging_count == 0:\n",
    "        print(\"   ‚ÑπÔ∏è  All events were duplicates\")\n",
    "        return {\"query\": query, \"staging_table\": staging_table_fqn, \"merged\": 0}\n",
    "    \n",
    "    # Create target table if it doesn't exist\n",
    "    if not spark.catalog.tableExists(target_table_fqn):\n",
    "        print(f\"   üìù Creating new target table: {target_table_fqn}\")\n",
    "        \n",
    "        # CRITICAL: Proper DELETE handling for initial table creation\n",
    "        # This matches cockroachdb.py reference implementation\n",
    "        \n",
    "        # 1. Get keys that have DELETE events\n",
    "        delete_keys = staging_df.filter(F.col(\"_cdc_operation\") == \"DELETE\") \\\n",
    "            .select(*primary_key_columns) \\\n",
    "            .distinct()\n",
    "        delete_count = delete_keys.count()\n",
    "        \n",
    "        # 2. Get all non-DELETE rows\n",
    "        active_rows = staging_df.filter(F.col(\"_cdc_operation\") != \"DELETE\")\n",
    "        active_count = active_rows.count()\n",
    "        \n",
    "        # 3. Exclude rows with keys that are deleted (left anti join)\n",
    "        # This handles case where key has UPSERT at T1, DELETE at T2\n",
    "        rows_after_delete = active_rows.join(\n",
    "            delete_keys,\n",
    "            on=primary_key_columns,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        after_delete_count = rows_after_delete.count()\n",
    "        \n",
    "        # Note: staging_df is already deduplicated, so final_rows = rows_after_delete\n",
    "        final_rows = rows_after_delete\n",
    "        final_count = after_delete_count\n",
    "        \n",
    "        if delete_count > 0:\n",
    "            print(f\"   ‚ÑπÔ∏è  Found {delete_count} DELETE events\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Active rows before DELETE: {active_count}\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Active rows after DELETE: {after_delete_count}\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Rows removed by DELETE: {active_count - after_delete_count}\")\n",
    "        \n",
    "        final_rows.write.format(\"delta\").saveAsTable(target_table_fqn)\n",
    "        merged_count = final_count\n",
    "        print(f\"   ‚úÖ Created table with {merged_count} initial rows\")\n",
    "        print(f\"      Schema includes _cdc_operation for observability\\n\")\n",
    "    else:\n",
    "        # Get Delta table and apply MERGE\n",
    "        delta_table = DeltaTable.forName(spark, target_table_fqn)\n",
    "        \n",
    "        # Check if _cdc_operation exists in target\n",
    "        target_columns = set(spark.read.table(target_table_fqn).columns)\n",
    "        if \"_cdc_operation\" not in target_columns:\n",
    "            print(f\"   ‚ö†Ô∏è  Target table missing _cdc_operation column\")\n",
    "            print(f\"   üîß Adding _cdc_operation column...\")\n",
    "            spark.sql(f\"ALTER TABLE {target_table_fqn} ADD COLUMN _cdc_operation STRING\")\n",
    "            print(f\"   ‚úÖ Column added\\n\")\n",
    "        \n",
    "        # Build join condition\n",
    "        join_condition = \" AND \".join([f\"target.{col} = source.{col}\" for col in primary_key_columns])\n",
    "        \n",
    "        # Get all columns\n",
    "        data_columns = [col for col in staging_df.columns]\n",
    "        update_set = {col: f\"source.{col}\" for col in data_columns}\n",
    "        insert_values = {col: f\"source.{col}\" for col in data_columns}\n",
    "        \n",
    "        print(f\"   üîÑ Executing MERGE...\")\n",
    "        print(f\"      Join: {join_condition}\")\n",
    "        print(f\"      ‚ÑπÔ∏è  _cdc_operation preserved for monitoring\")\n",
    "        \n",
    "        # Apply MERGE\n",
    "        (delta_table.alias(\"target\").merge(\n",
    "            staging_df.alias(\"source\"),\n",
    "            join_condition\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"source._cdc_operation = 'UPSERT' AND source._cdc_timestamp > target._cdc_timestamp\",\n",
    "            set=update_set\n",
    "        )\n",
    "        .whenMatchedDelete(\n",
    "            condition=\"source._cdc_operation = 'DELETE'\"\n",
    "        )\n",
    "        .whenNotMatchedInsert(\n",
    "            condition=\"source._cdc_operation = 'UPSERT'\",\n",
    "            values=insert_values\n",
    "        )\n",
    "        .execute())\n",
    "        \n",
    "        merged_count = staging_count\n",
    "        print(f\"   ‚úÖ MERGE complete: processed {merged_count} events\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ CDC INGESTION COMPLETE (MERGE + COLUMN FAMILIES)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä Raw events: {staging_count_raw}\")\n",
    "    print(f\"üìä After deduplication: {staging_count} unique events\")\n",
    "    print(f\"üìä Staging table: {staging_table_fqn}\")\n",
    "    print(f\"üìä Target table:  {target_table_fqn}\")\n",
    "    print()\n",
    "    print(\"üí° TIP: Staging table can be dropped after successful MERGE:\")\n",
    "    print(f\"   spark.sql('DROP TABLE IF EXISTS {staging_table_fqn}')\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"staging_table\": staging_table_fqn,\n",
    "        \"target_table\": target_table_fqn,\n",
    "        \"raw_count\": staging_count_raw,\n",
    "        \"deduped_count\": staging_count,\n",
    "        \"merged\": merged_count\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Databricks streaming modes loaded (4 functions available)\")\n",
    "print(\"   1. ingest_cdc_append_only_single_family (append_only, no column families)\")\n",
    "print(\"   2. ingest_cdc_with_merge_single_family (update_delete, no column families)\")\n",
    "print(\"   3. ingest_cdc_append_only_multi_family (append_only, WITH column families)\")\n",
    "print(\"   4. ingest_cdc_with_merge_multi_family (update_delete, WITH column families)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: CREATE TEST TABLE (Mode-Aware: Single or Multiple Column Families)\n",
    "# ============================================================================\n",
    "# Create table structure based on column_family_mode:\n",
    "# - single_cf: 1 column family (default, better performance)\n",
    "# - multi_cf: 3 column families (for testing split_column_families=true)\n",
    "\n",
    "if column_family_mode == \"multi_cf\":\n",
    "    # Create table with MULTIPLE column families for testing split_column_families=true\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {source_table} (\n",
    "        ycsb_key INT PRIMARY KEY,\n",
    "        -- Family 1: Frequently accessed fields\n",
    "        field0 TEXT,\n",
    "        field1 TEXT,\n",
    "        field2 TEXT,\n",
    "        FAMILY frequently_read (ycsb_key, field0, field1, field2),\n",
    "        \n",
    "        -- Family 2: Medium-frequency fields\n",
    "        field3 TEXT,\n",
    "        field4 TEXT,\n",
    "        field5 TEXT,\n",
    "        FAMILY medium_read (field3, field4, field5),\n",
    "        \n",
    "        -- Family 3: Rarely accessed fields\n",
    "        field6 TEXT,\n",
    "        field7 TEXT,\n",
    "        field8 TEXT,\n",
    "        field9 TEXT,\n",
    "        FAMILY rarely_read (field6, field7, field8, field9)\n",
    "    )\n",
    "    \"\"\"\n",
    "    family_info = \"3 column families (frequently_read, medium_read, rarely_read)\"\n",
    "else:\n",
    "    # Create table with SINGLE column family (default)\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {source_table} (\n",
    "        ycsb_key INT PRIMARY KEY,\n",
    "        field0 TEXT,\n",
    "        field1 TEXT,\n",
    "        field2 TEXT,\n",
    "        field3 TEXT,\n",
    "        field4 TEXT,\n",
    "        field5 TEXT,\n",
    "        field6 TEXT,\n",
    "        field7 TEXT,\n",
    "        field8 TEXT,\n",
    "        field9 TEXT\n",
    "    )\n",
    "    \"\"\"\n",
    "    family_info = \"1 column family (default primary)\"\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(create_table_sql)\n",
    "        conn.commit()\n",
    "    print(f\"‚úÖ Table '{source_table}' created (or already exists)\")\n",
    "    print(f\"   Column Family Mode: {column_family_mode}\")\n",
    "    print(f\"   Column families: {family_info}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: INSERT SAMPLE DATA (Snapshot Phase)\n",
    "# ============================================================================\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    # Check if table is empty using helper function\n",
    "    stats = get_table_stats(conn, source_table)\n",
    "    \n",
    "    if stats['is_empty']:\n",
    "        # Table is empty - insert snapshot data\n",
    "        print(f\"üìä Table is empty. Inserting {snapshot_count} initial rows (snapshot phase)...\")\n",
    "        \n",
    "        with conn.cursor() as cur:\n",
    "            # Use generate_series for efficient bulk insert\n",
    "            insert_sql = f\"\"\"\n",
    "            INSERT INTO {source_table} \n",
    "            (ycsb_key, field0, field1, field2, field3, field4, field5, field6, field7, field8, field9)\n",
    "            SELECT \n",
    "                i AS ycsb_key,\n",
    "                'snapshot_value_' || i || '_0' AS field0,\n",
    "                'snapshot_value_' || i || '_1' AS field1,\n",
    "                'snapshot_value_' || i || '_2' AS field2,\n",
    "                'snapshot_value_' || i || '_3' AS field3,\n",
    "                'snapshot_value_' || i || '_4' AS field4,\n",
    "                'snapshot_value_' || i || '_5' AS field5,\n",
    "                'snapshot_value_' || i || '_6' AS field6,\n",
    "                'snapshot_value_' || i || '_7' AS field7,\n",
    "                'snapshot_value_' || i || '_8' AS field8,\n",
    "                'snapshot_value_' || i || '_9' AS field9\n",
    "            FROM generate_series(0, %s - 1) AS i\n",
    "            \"\"\"\n",
    "            \n",
    "            cur.execute(insert_sql, (snapshot_count,))\n",
    "            conn.commit()\n",
    "        \n",
    "        print(f\"‚úÖ Sample data inserted using generate_series\")\n",
    "        print(f\"   Rows inserted: {snapshot_count} (keys 0 to {snapshot_count - 1})\")\n",
    "    else:\n",
    "        # Table already has data - skip insert\n",
    "        print(f\"‚ÑπÔ∏è  Table already contains data - skipping snapshot insert\")\n",
    "        print(f\"   Current key range: {stats['min_key']} to {stats['max_key']}\")\n",
    "        print(f\"   Tip: If you want to re-run the snapshot, drop the table first (see Cleanup cells)\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: CREATE CHANGEFEED\n",
    "# ============================================================================\n",
    "# Build Azure Blob Storage URI with table-specific path\n",
    "# Note: For Azure, path goes in URI (not as path_prefix query parameter like S3)\n",
    "path = f\"parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "changefeed_path = f\"azure://{container_name}/{path}?AZURE_ACCOUNT_NAME={storage_account_name}&AZURE_ACCOUNT_KEY={storage_account_key_encoded}\"\n",
    "\n",
    "# Build changefeed options based on column_family_mode\n",
    "if column_family_mode == \"multi_cf\":\n",
    "    # Include split_column_families for multi-family mode\n",
    "    changefeed_options = \"\"\"\n",
    "    format='parquet',\n",
    "    updated,\n",
    "    resolved='10s',\n",
    "    split_column_families\n",
    "\"\"\"\n",
    "else:\n",
    "    # Standard options for single-family mode\n",
    "    changefeed_options = \"\"\"\n",
    "    format='parquet',\n",
    "    updated,\n",
    "    resolved='10s'\n",
    "\"\"\"\n",
    "\n",
    "# Create changefeed SQL\n",
    "create_changefeed_sql = f\"\"\"\n",
    "CREATE CHANGEFEED FOR TABLE {source_table}\n",
    "INTO '{changefeed_path}'\n",
    "WITH {changefeed_options}\n",
    "\"\"\"\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        # Check for existing changefeed with THIS specific destination path\n",
    "        # (checks for source table AND full path to ensure uniqueness)\n",
    "        path_pattern = f\"%{source_table}%{source_catalog}/{source_schema}/{source_table}/{target_table}%\"\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "            SELECT job_id, status, description\n",
    "            FROM [SHOW CHANGEFEED JOBS] \n",
    "            WHERE description LIKE %s\n",
    "            AND status IN ('running', 'paused')\n",
    "            LIMIT 1\n",
    "        \"\"\", (path_pattern,))\n",
    "        \n",
    "        existing = cur.fetchone()\n",
    "        \n",
    "        if existing:\n",
    "            job_id, status, description = existing\n",
    "            print(f\"‚úÖ Changefeed already exists for this source ‚Üí target mapping\")\n",
    "            print(f\"   Job ID: {job_id}\")\n",
    "            print(f\"   Status: {status}\")\n",
    "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
    "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
    "            if column_family_mode == \"multi_cf\":\n",
    "                print(f\"   Expected: Column family fragments\")\n",
    "            print(f\"\")\n",
    "            print(f\"üí° Tip: Run Cell 9 to generate UPDATE/DELETE events\")\n",
    "            print(f\"   Then check Cell 10 to verify new files appear\")\n",
    "        else:\n",
    "            # Create new changefeed\n",
    "            cur.execute(create_changefeed_sql)\n",
    "            result = cur.fetchone()\n",
    "            job_id = result[0]\n",
    "            \n",
    "            print(f\"‚úÖ Changefeed created\")\n",
    "            print(f\"   Job ID: {job_id}\")\n",
    "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
    "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
    "            print(f\"   Format: Parquet\")\n",
    "            if column_family_mode == \"multi_cf\":\n",
    "                print(f\"   Split column families: TRUE (fragments will be generated)\")\n",
    "            else:\n",
    "                print(f\"   Split column families: FALSE (single file per event)\")\n",
    "            print(f\"   Destination: Azure Blob Storage\")\n",
    "            print(f\"\")\n",
    "            \n",
    "            # Wait for files to appear using helper function\n",
    "            wait_for_changefeed_files(\n",
    "                storage_account_name, storage_account_key, container_name,\n",
    "                source_catalog, source_schema, source_table, target_table,\n",
    "                max_wait=300, check_interval=5\n",
    "            )\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: RUN CDC WORKLOAD (UPDATE & DELETE)\n",
    "# ============================================================================\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Capture baseline file count BEFORE generating CDC events\n",
    "print(\"üìä Capturing baseline file count...\")\n",
    "result_before = check_azure_files(\n",
    "    storage_account_name, storage_account_key, container_name,\n",
    "    source_catalog, source_schema, source_table, target_table,\n",
    "    verbose=False\n",
    ")\n",
    "files_before = len(result_before['data_files'])\n",
    "print(f\"   Current files: {files_before}\")\n",
    "print()\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    # Get current table state using helper function\n",
    "    stats_before = get_table_stats(conn, source_table)\n",
    "    min_key = stats_before['min_key']\n",
    "    max_key = stats_before['max_key']\n",
    "    count_before = stats_before['count']\n",
    "    \n",
    "    print(f\"üìä Current table state:\")\n",
    "    print(f\"   Min key: {min_key}, Max key: {max_key}, Total rows: {count_before}\")\n",
    "    print()\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        \n",
    "        # 1. INSERT: Add new rows starting from max_key + 1 (using generate_series)\n",
    "        print(f\"‚ûï Running {insert_count} INSERTs (keys {max_key + 1} to {max_key + insert_count})...\")\n",
    "        insert_sql = f\"\"\"\n",
    "        INSERT INTO {source_table} \n",
    "        (ycsb_key, field0, field1, field2, field3, field4, field5, field6, field7, field8, field9)\n",
    "        SELECT \n",
    "            i AS ycsb_key,\n",
    "            'inserted_value_' || i || '_0' AS field0,\n",
    "            'inserted_value_' || i || '_1' AS field1,\n",
    "            'inserted_value_' || i || '_2' AS field2,\n",
    "            'inserted_value_' || i || '_3' AS field3,\n",
    "            'inserted_value_' || i || '_4' AS field4,\n",
    "            'inserted_value_' || i || '_5' AS field5,\n",
    "            'inserted_value_' || i || '_6' AS field6,\n",
    "            'inserted_value_' || i || '_7' AS field7,\n",
    "            'inserted_value_' || i || '_8' AS field8,\n",
    "            'inserted_value_' || i || '_9' AS field9\n",
    "        FROM generate_series(%s, %s) AS i\n",
    "        \"\"\"\n",
    "        cur.execute(insert_sql, (max_key + 1, max_key + insert_count))\n",
    "        \n",
    "        # 2. UPDATE: Update existing rows starting from min_key (single UPDATE statement)\n",
    "        print(f\"üìù Running {update_count} UPDATEs (keys {min_key} to {min_key + update_count - 1})...\")\n",
    "        timestamp = int(time.time())\n",
    "        cur.execute(f\"\"\"\n",
    "            UPDATE {source_table}\n",
    "            SET field0 = %s\n",
    "            WHERE ycsb_key >= %s AND ycsb_key < %s\n",
    "        \"\"\", (f\"updated_at_{timestamp}\", min_key, min_key + update_count))\n",
    "        \n",
    "        # 3. DELETE: Delete oldest rows starting from min_key (single DELETE)\n",
    "        delete_max = min_key + delete_count - 1\n",
    "        print(f\"üóëÔ∏è  Running {delete_count} DELETEs (keys {min_key} to {delete_max})...\")\n",
    "        cur.execute(f\"\"\"\n",
    "            DELETE FROM {source_table}\n",
    "            WHERE ycsb_key >= %s AND ycsb_key <= %s\n",
    "        \"\"\", (min_key, delete_max))\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    # Get final table state using helper function\n",
    "    stats_after = get_table_stats(conn, source_table)\n",
    "    min_key_after = stats_after['min_key']\n",
    "    max_key_after = stats_after['max_key']\n",
    "    count_after = stats_after['count']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Workload complete\")\n",
    "    print(f\"   Inserts: {insert_count}\")\n",
    "    print(f\"   Updates: {update_count}\")\n",
    "    print(f\"   Deletes: {delete_count}\")\n",
    "    print(f\"   Before: {count_before} rows (keys {min_key}-{max_key})\")\n",
    "    print(f\"   After:  {count_after} rows (keys {min_key_after}-{max_key_after})\")\n",
    "    print(f\"   Net change: {count_after - count_before:+d} rows\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    # Wait for new CDC files to appear in Azure (positive confirmation)\n",
    "    print(f\"‚è≥ Waiting for new CDC files to appear in Azure...\")\n",
    "    print(f\"   Baseline: {files_before} files\")\n",
    "    print()\n",
    "    \n",
    "    # Poll for new files (max 90 seconds)\n",
    "    max_wait = 90\n",
    "    check_interval = 10\n",
    "    elapsed = 0\n",
    "    \n",
    "    while elapsed < max_wait:\n",
    "        result = check_azure_files(\n",
    "            storage_account_name, storage_account_key, container_name,\n",
    "            source_catalog, source_schema, source_table, target_table,\n",
    "            verbose=False\n",
    "        )\n",
    "        files_now = len(result['data_files'])\n",
    "        \n",
    "        if files_now > files_before:\n",
    "            print(f\"‚úÖ New CDC files appeared after {elapsed} seconds!\")\n",
    "            print(f\"   Baseline (before workload): {files_before} files\")\n",
    "            print(f\"   Current (after workload): {files_now} files\")\n",
    "            print(f\"   New files generated: {files_now - files_before}\")\n",
    "            break\n",
    "        \n",
    "        print(f\"   Checking... ({elapsed}s elapsed, baseline: {files_before} files)\", end='\\r')\n",
    "        time.sleep(check_interval)\n",
    "        elapsed += check_interval\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - files may still be flushing\")\n",
    "        print(f\"   Run Cell 9 to check manually\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a93f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: CHECK AZURE FILES (Optional - Manual Check)\n",
    "# ============================================================================\n",
    "# Use the helper function from Cell 4 to check for files\n",
    "result = check_azure_files(\n",
    "    storage_account_name, storage_account_key, container_name,\n",
    "    source_catalog, source_schema, source_table, target_table,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Provide guidance\n",
    "if len(result['data_files']) == 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  No data files found yet.\")\n",
    "    print(f\"   üí° Possible reasons:\")\n",
    "    print(f\"   - Changefeed not created yet (run Cell 9)\")\n",
    "    print(f\"   - Path configuration mismatch (check Cell 1 variables)\")\n",
    "    print(f\"   - Azure credentials issue (check External Location)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Files are ready! Proceed to Cell 10 to read with Databricks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: READ CDC EVENTS IN DATABRICKS\n",
    "# ============================================================================\n",
    "# Select and run CDC ingestion function based on both modes (from Cell 1)\n",
    "# Functions are defined in Cell 5\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"üî∑ CDC Configuration:\")\n",
    "print(f\"   Processing Mode: {cdc_mode}\")\n",
    "print(f\"   Column Family Mode: {column_family_mode}\")\n",
    "print()\n",
    "\n",
    "# Select function based on BOTH cdc_mode and column_family_mode\n",
    "if cdc_mode == \"append_only\" and column_family_mode == \"single_cf\":\n",
    "    print(f\"üìò Running: ingest_cdc_append_only_single_family()\")\n",
    "    print(f\"   - All CDC events will be stored as rows\")\n",
    "    print(f\"   - No column family merging needed\\n\")\n",
    "    \n",
    "    query = ingest_cdc_append_only_single_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        spark=spark\n",
    "    )\n",
    "\n",
    "elif cdc_mode == \"append_only\" and column_family_mode == \"multi_cf\":\n",
    "    print(f\"üìô Running: ingest_cdc_append_only_multi_family()\")\n",
    "    print(f\"   - All CDC events will be stored as rows\")\n",
    "    print(f\"   - Column family fragments will be merged\\n\")\n",
    "    \n",
    "    if not primary_key_columns:\n",
    "        raise ValueError(\"primary_key_columns required for multi_cf mode\")\n",
    "    \n",
    "    query = ingest_cdc_append_only_multi_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        primary_key_columns=primary_key_columns,\n",
    "        spark=spark\n",
    "    )\n",
    "\n",
    "elif cdc_mode == \"update_delete\" and column_family_mode == \"single_cf\":\n",
    "    print(f\"üìó Running: ingest_cdc_with_merge_single_family()\")\n",
    "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
    "    print(f\"   - No column family merging needed\\n\")\n",
    "    \n",
    "    if not primary_key_columns:\n",
    "        raise ValueError(\"primary_key_columns required for update_delete mode\")\n",
    "    \n",
    "    result = ingest_cdc_with_merge_single_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        primary_key_columns=primary_key_columns,\n",
    "        spark=spark\n",
    "    )\n",
    "    \n",
    "    query = result[\"query\"]\n",
    "\n",
    "elif cdc_mode == \"update_delete\" and column_family_mode == \"multi_cf\":\n",
    "    print(f\"üìï Running: ingest_cdc_with_merge_multi_family()\")\n",
    "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
    "    print(f\"   - Column family fragments will be merged\\n\")\n",
    "    \n",
    "    if not primary_key_columns:\n",
    "        raise ValueError(\"primary_key_columns required for update_delete + multi_cf mode\")\n",
    "    \n",
    "    result = ingest_cdc_with_merge_multi_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        primary_key_columns=primary_key_columns,\n",
    "        spark=spark\n",
    "    )\n",
    "    \n",
    "    query = result[\"query\"]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Invalid mode combination:\\n\"\n",
    "        f\"  cdc_mode='{cdc_mode}' (valid: 'append_only', 'update_delete')\\n\"\n",
    "        f\"  column_family_mode='{column_family_mode}' (valid: 'single_cf', 'multi_cf')\\n\"\n",
    "        f\"Change modes in Cell 1.\"\n",
    "    )\n",
    "\n",
    "# Wait for completion (if not already complete)\n",
    "if cdc_mode == \"append_only\":\n",
    "    query.awaitTermination()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"‚úÖ CDC INGESTION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"   Mode: {cdc_mode} + {column_family_mode}\")\n",
    "    print(f\"   Target: {target_catalog}.{target_schema}.{target_table}\")\n",
    "    print()\n",
    "    print(f\"üìä Query your data: SELECT * FROM {target_catalog}.{target_schema}.{target_table}\")\n",
    "else:\n",
    "    # update_delete mode already completed inside the function\n",
    "    print(f\"üìä Query your data: SELECT * FROM {target_catalog}.{target_schema}.{target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: QUERY CDC RESULTS\n",
    "# ============================================================================\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "\n",
    "# Get total count\n",
    "df = spark.read.table(target_table_fqn)\n",
    "total_count = df.count()\n",
    "\n",
    "print(\"üìä CDC Event Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows: {total_count}\")\n",
    "print(f\"CDC Processing Mode: {cdc_mode}\")\n",
    "print(f\"Column Family Mode: {column_family_mode}\")\n",
    "print()\n",
    "\n",
    "# Show operation breakdown (works for both modes now!)\n",
    "print(\"Rows by last CDC operation:\")\n",
    "ops_df = df.groupBy(\"_cdc_operation\").count().orderBy(\"_cdc_operation\")\n",
    "ops_df.show()\n",
    "\n",
    "print(\"\\nüìã Sample rows (showing first 5):\")\n",
    "df.select(\n",
    "    \"ycsb_key\", \n",
    "    \"field0\", \n",
    "    \"_cdc_operation\", \n",
    "    \"_cdc_timestamp\"\n",
    ").orderBy(\"_cdc_timestamp\").show(5, truncate=False)\n",
    "\n",
    "if cdc_mode == \"append_only\":\n",
    "    print(\"\\n‚úÖ CDC data successfully loaded (append_only mode)\")\n",
    "    print(\"   üìä All CDC events stored as rows\")\n",
    "    print(\"   üìä _cdc_operation shows: DELETE, UPSERT for each event\")\n",
    "    print(\"   üìä Row count = all events (including DELETEs and multiple UPDATEs)\")\n",
    "elif cdc_mode == \"update_delete\":\n",
    "    print(\"\\n‚úÖ CDC data successfully loaded (update_delete mode)\")\n",
    "    print(\"   üìä MERGE operations applied: DELETEs removed, UPDATEs applied, INSERTs added\")\n",
    "    print(\"   üìä _cdc_operation shows: UPSERT (last operation on each row)\")\n",
    "    print(\"   üìä Row count = current state (deduplicated)\")\n",
    "\n",
    "print(\"\\nüí° Key Takeaway:\")\n",
    "print(\"   - Using pathGlobFilter to exclude .RESOLVED files avoids DECIMAL errors\")\n",
    "print(\"   - _cdc_operation is preserved in both modes for monitoring\")\n",
    "print(\"\\nüìç Next: Run Cell 13 to verify source and target tables are in sync\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8f6b5",
   "metadata": {},
   "source": [
    "## Optional: Cleanup\n",
    "\n",
    "Run the cells below if you want to clean up the test resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7250ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ‚ö†Ô∏è  SAFETY STOP: Cleanup Section\n",
    "# ============================================================================\n",
    "# This cell prevents accidental cleanup when running \"Run All\"\n",
    "# \n",
    "# To cleanup resources, manually run each cell below INDIVIDUALLY:\n",
    "#   - Cell 16: Cancel changefeed\n",
    "#   - Cell 17: Drop CockroachDB source table  \n",
    "#   - Cell 18: Drop Databricks target table & checkpoint\n",
    "#   - Cell 19: Clear Azure changefeed data (optional - use for complete reset)\n",
    "# ============================================================================\n",
    "\n",
    "raise RuntimeError(\n",
    "    \"\\n\"\n",
    "    \"‚ö†Ô∏è  CLEANUP SAFETY STOP\\n\"\n",
    "    \"\\n\"\n",
    "    \"The cells below will DELETE your resources.\\n\"\n",
    "    \"Do NOT run all cells - run each cleanup cell individually.\\n\"\n",
    "    \"\\n\"\n",
    "    \"üí° TIP: If Cell 13 shows sync issues due to old data,\\n\"\n",
    "    \"   run Cell 19 to clear Azure changefeed data completely.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP CELL 1: CANCEL CHANGEFEED\n",
    "# ============================================================================\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        # Find changefeed job for THIS specific source ‚Üí target mapping\n",
    "        # (matches the same path pattern used in Cell 7)\n",
    "        path_pattern = f\"%{source_table}%{source_catalog}/{source_schema}/{source_table}/{target_table}%\"\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "            SELECT job_id \n",
    "            FROM [SHOW CHANGEFEED JOBS] \n",
    "            WHERE description LIKE %s\n",
    "            AND status IN ('running', 'paused')\n",
    "            LIMIT 1\n",
    "        \"\"\", (path_pattern,))\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        if result:\n",
    "            job_id = result[0]\n",
    "            cur.execute(f\"CANCEL JOB {job_id}\")\n",
    "            print(f\"‚úÖ Changefeed {job_id} cancelled\")\n",
    "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
    "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è  No active changefeed found for this source ‚Üí target mapping\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP CELL 2: DROP SOURCE TABLE (CockroachDB)\n",
    "# ============================================================================\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f\"DROP TABLE IF EXISTS {source_table} CASCADE\")\n",
    "        conn.commit()\n",
    "    print(f\"‚úÖ Table '{source_table}' dropped from CockroachDB\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ed2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP CELL 19: CLEAR AZURE CHANGEFEED DATA (Optional)\n",
    "# ============================================================================\n",
    "# ‚ö†Ô∏è  WARNING: This will DELETE all changefeed data in Azure for this table!\n",
    "#\n",
    "# Use this when:\n",
    "# - You want to start completely fresh\n",
    "# - Old data from previous runs is causing sync issues\n",
    "# - You changed the table schema (e.g., VARCHAR ‚Üí INT)\n",
    "#\n",
    "# Uses Azure SDK (same as Cell 11 for checking files)\n",
    "# ============================================================================\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Build Azure path (must match Cell 7 changefeed path)\n",
    "changefeed_path = f\"parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}/\"\n",
    "\n",
    "print(f\"üóëÔ∏è  Deleting Azure changefeed data...\")\n",
    "print(f\"=\" * 80)\n",
    "print(f\"Container: {container_name}\")\n",
    "print(f\"Path: {changefeed_path}\")\n",
    "print()\n",
    "\n",
    "# Connect to Azure (same as Cell 9)\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service.get_container_client(container_name)\n",
    "\n",
    "# List all blobs with this prefix\n",
    "print(f\"üîç Scanning for files...\")\n",
    "blobs = list(container_client.list_blobs(name_starts_with=changefeed_path))\n",
    "\n",
    "if not blobs:\n",
    "    print(f\"‚ÑπÔ∏è  No files found at: {changefeed_path}\")\n",
    "    print(f\"   Files may have already been deleted, or path is incorrect\")\n",
    "    print()\n",
    "    print(f\"üí° To check what's in the container, run Cell 9\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(blobs)} items to delete\")\n",
    "    \n",
    "    # Show sample items\n",
    "    data_files = [b for b in blobs if b.size > 0 and '.parquet' in b.name]\n",
    "    resolved_files = [b for b in blobs if '.RESOLVED' in b.name]\n",
    "    directories = [b for b in blobs if b.size == 0]\n",
    "    \n",
    "    print(f\"   üìÑ Data files: {len(data_files)}\")\n",
    "    print(f\"   üïê Resolved files: {len(resolved_files)}\")\n",
    "    print(f\"   üìÅ Directories: {len(directories)}\")\n",
    "    print()\n",
    "    \n",
    "    # Delete all blobs with this prefix\n",
    "    # Note: Azure SDK doesn't have recursive delete - we list all blobs and delete each one\n",
    "    print(f\"üîÑ Deleting {len(blobs)} items...\")\n",
    "    deleted = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for blob in blobs:\n",
    "        try:\n",
    "            container_client.delete_blob(blob.name)\n",
    "            deleted += 1\n",
    "            if deleted % 50 == 0:\n",
    "                print(f\"   Deleted {deleted}/{len(blobs)} items...\", end='\\r')\n",
    "        except Exception as e:\n",
    "            # Some errors are expected (e.g., directories already removed)\n",
    "            error_str = str(e)\n",
    "            if \"DirectoryIsNotEmpty\" not in error_str and \"BlobNotFound\" not in error_str:\n",
    "                failed += 1\n",
    "                print(f\"\\n   ‚ö†Ô∏è  Failed: {blob.name[:60]}... - {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Deleted {deleted} items from Azure                    \")\n",
    "    if failed > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Failed to delete {failed} items\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"‚úÖ Cleanup complete!\")\n",
    "    print()\n",
    "    print(f\"üí° Next steps:\")\n",
    "    print(f\"   1. Drop the Databricks target table (Cell 17)\")\n",
    "    print(f\"   2. Re-run from Cell 6 (Snapshot) to start fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP CELL 3: DROP TARGET TABLE & CHECKPOINT (Databricks)\n",
    "# ============================================================================\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}\"  # Must match Cell 10\n",
    "\n",
    "# Drop Delta table\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
    "print(f\"‚úÖ Delta table '{target_table_fqn}' dropped\")\n",
    "\n",
    "# Remove checkpoint\n",
    "try:\n",
    "    dbutils.fs.rm(checkpoint_path, True)\n",
    "    print(f\"‚úÖ Checkpoint '{checkpoint_path}' removed\")\n",
    "except:\n",
    "    print(f\"‚ÑπÔ∏è  Checkpoint not found (may have been already removed)\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bdda3d",
   "metadata": {},
   "source": [
    "# Debug Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\n",
    "    \"\\n\"\n",
    "    \"‚ö†Ô∏è  DEBUG SAFETY STOP\\n\"\n",
    "    \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71eb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEBUG CELL 1: Quick Missing Keys Check\n",
    "# ============================================================================\n",
    "# Lightweight check to see if specific keys exist in CockroachDB and staging\n",
    "# Update missing_keys list based on Cell 14/15 output\n",
    "# ============================================================================\n",
    "\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "staging_table_cf = f\"{target_table_fqn}_staging_cf\"\n",
    "missing_keys = [17, 18, 19]  # ‚Üê Update this based on Cell 14/15 output\n",
    "\n",
    "print(\"üîç Quick Debug: Checking missing keys...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check CockroachDB\n",
    "print(f\"\\nüìä CockroachDB ({source_table}):\")\n",
    "with get_cockroachdb_connection() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    for key in missing_keys:\n",
    "        cursor.execute(f\"SELECT * FROM {source_table} WHERE ycsb_key = %s\", (key,))\n",
    "        result = cursor.fetchone()\n",
    "        print(f\"   Key {key}: {'‚úÖ EXISTS' if result else '‚ùå NOT FOUND (deleted)'}\")\n",
    "\n",
    "# Check Staging Table\n",
    "print(f\"\\nüìä Staging Table ({staging_table_cf}):\")\n",
    "if spark.catalog.tableExists(staging_table_cf):\n",
    "    staging_df = spark.read.table(staging_table_cf)\n",
    "    for key in missing_keys:\n",
    "        count = staging_df.filter(F.col(\"ycsb_key\") == key).count()\n",
    "        print(f\"   Key {key}: {count} row(s)\")\n",
    "    \n",
    "    print(\"\\nüí° Next steps:\")\n",
    "    print(\"   - If keys exist in CockroachDB but not in staging:\")\n",
    "    print(\"     ‚Üí Re-run Cell 12 to pick up new CDC files\")\n",
    "    print(\"   - If keys exist in staging but not in target:\")\n",
    "    print(\"     ‚Üí Check Cell 12 output for MERGE errors\")\n",
    "    print(\"     ‚Üí Run DEBUG CELL 2 for detailed analysis\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Staging table doesn't exist (Cell 12 dropped it)\")\n",
    "    print(\"   üí° Re-run Cell 12 to recreate staging for debugging\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70044870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEBUG CELL 2: Inspect Target Table\n",
    "# ============================================================================\n",
    "# Comprehensive analysis of target table:\n",
    "# - CDC operation distribution\n",
    "# - Key distribution and gaps\n",
    "# - Duplicate detection\n",
    "# - Sample records\n",
    "# \n",
    "# Use this to diagnose sync issues and data quality problems\n",
    "# ============================================================================\n",
    "\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "\n",
    "print(f\"üìä Target Table Analysis: {target_table_fqn}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read target table\n",
    "df = spark.read.table(target_table_fqn)\n",
    "total_rows = df.count()\n",
    "print(f\"\\nüìà Total Rows: {total_rows:,}\")\n",
    "\n",
    "# Group by CDC operation type\n",
    "if \"_cdc_operation\" in df.columns:\n",
    "    print(\"\\nüîç CDC Operations:\")\n",
    "    df.groupBy(\"_cdc_operation\").count().orderBy(\"_cdc_operation\").show()\n",
    "    \n",
    "    # Check if DELETE rows are stored as data (should NOT happen)\n",
    "    delete_count = df.filter(F.col(\"_cdc_operation\") == \"DELETE\").count()\n",
    "    if delete_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Found {delete_count} DELETE rows stored as data!\")\n",
    "        print(\"   This is a bug - DELETE rows should not be in the target table.\")\n",
    "        print(\"   üí° Run Cell 16 to fix (drops and recreates table)\")\n",
    "\n",
    "# Show key distribution\n",
    "if \"ycsb_key\" in df.columns:\n",
    "    print(\"\\nüîç Key Distribution:\")\n",
    "    key_dist = df.groupBy(\"ycsb_key\").count().orderBy(\"ycsb_key\")\n",
    "    key_dist.show(50)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = key_dist.filter(\"count > 1\")\n",
    "    dup_count = duplicates.count()\n",
    "    if dup_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {dup_count} duplicate keys!\")\n",
    "        duplicates.show()\n",
    "        print(\"\\n   üí° This indicates deduplication failure in MERGE logic\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No duplicate keys found\")\n",
    "    \n",
    "    # Show key range and gaps\n",
    "    key_stats = df.select(\n",
    "        F.min(\"ycsb_key\").alias(\"min_key\"),\n",
    "        F.max(\"ycsb_key\").alias(\"max_key\"),\n",
    "        F.count(\"ycsb_key\").alias(\"count\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    expected_count = key_stats[\"max_key\"] - key_stats[\"min_key\"] + 1\n",
    "    actual_count = key_stats[\"count\"]\n",
    "    missing_count = expected_count - actual_count\n",
    "    \n",
    "    print(f\"\\nüìä Key Range Analysis:\")\n",
    "    print(f\"   Min key: {key_stats['min_key']}\")\n",
    "    print(f\"   Max key: {key_stats['max_key']}\")\n",
    "    print(f\"   Expected rows (if contiguous): {expected_count}\")\n",
    "    print(f\"   Actual rows: {actual_count}\")\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Missing {missing_count} keys (gaps in range)\")\n",
    "        print(f\"\\n   üí° Run DEBUG CELL 1 or CELL 3 to investigate specific keys\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No gaps (keys are contiguous)\")\n",
    "\n",
    "# Show sample records\n",
    "print(\"\\nüîç Sample Records (ordered by key):\")\n",
    "df.orderBy(\"ycsb_key\").show(30, truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° If you see issues:\")\n",
    "print(\"   - DELETE rows stored as data ‚Üí Run Cell 16 (recreate table)\")\n",
    "print(\"   - Duplicate keys ‚Üí Check MERGE deduplication logic\")\n",
    "print(\"   - Missing keys ‚Üí Run DEBUG CELL 1 or CELL 3\")\n",
    "print(\"   - Gaps in key range ‚Üí Keys were deleted (normal for update_delete mode)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEBUG CELL 3: Detailed Missing Keys Investigation\n",
    "# ============================================================================\n",
    "# Detailed investigation of missing keys:\n",
    "# - Checks CockroachDB source\n",
    "# - Checks staging table (if it exists)\n",
    "# - Shows CDC operation and timestamps\n",
    "# - Provides detailed troubleshooting steps\n",
    "#\n",
    "# Update missing_keys list based on Cell 14/15 or DEBUG CELL 2 output\n",
    "# ============================================================================\n",
    "\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "staging_table_cf = f\"{target_table_fqn}_staging_cf\"\n",
    "\n",
    "# Update this list based on what's missing\n",
    "missing_keys = [17, 18, 19]  # ‚Üê Update based on DEBUG CELL 2 output\n",
    "\n",
    "print(\"üîç Detailed Missing Keys Investigation\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Investigating keys: {missing_keys}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Check CockroachDB Source\n",
    "# ============================================================================\n",
    "print(\"üìä STEP 1: Checking CockroachDB Source\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with get_cockroachdb_connection() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for key in missing_keys:\n",
    "        cursor.execute(f\"SELECT * FROM {source_table} WHERE ycsb_key = %s\", (key,))\n",
    "        result = cursor.fetchone()\n",
    "        \n",
    "        if result:\n",
    "            print(f\"‚úÖ Key {key}: EXISTS in CockroachDB\")\n",
    "            # Show first 3 fields for verification\n",
    "            print(f\"   Sample data: {result[:min(3, len(result))]}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Key {key}: NOT FOUND in CockroachDB\")\n",
    "            print(f\"   ‚Üí This key was deleted (expected for update_delete mode)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Check Staging Table\n",
    "# ============================================================================\n",
    "print(f\"\\nüìä STEP 2: Checking Staging Table\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if spark.catalog.tableExists(staging_table_cf):\n",
    "    staging_df = spark.read.table(staging_table_cf)\n",
    "    print(f\"‚úÖ Staging table exists: {staging_table_cf}\")\n",
    "    print()\n",
    "    \n",
    "    for key in missing_keys:\n",
    "        key_rows = staging_df.filter(F.col(\"ycsb_key\") == key)\n",
    "        count = key_rows.count()\n",
    "        \n",
    "        if count > 0:\n",
    "            print(f\"‚úÖ Key {key}: {count} row(s) in staging table\")\n",
    "            print(\"   Details:\")\n",
    "            key_rows.select(\n",
    "                \"ycsb_key\", \n",
    "                \"_cdc_timestamp\", \n",
    "                \"_cdc_operation\", \n",
    "                \"field0\", \n",
    "                \"field1\"\n",
    "            ).show(truncate=False)\n",
    "        else:\n",
    "            print(f\"‚ùå Key {key}: NOT in staging table\")\n",
    "    \n",
    "    # Show staging table summary\n",
    "    print(\"\\nüìà Staging Table Summary:\")\n",
    "    staging_df.groupBy(\"_cdc_operation\").count().show()\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Staging table doesn't exist: {staging_table_cf}\")\n",
    "    print(\"   This means Cell 12 completed and dropped the staging table\")\n",
    "    print(\"\\nüí° To debug further:\")\n",
    "    print(\"   1. Re-run Cell 12 (it will process new files and recreate staging)\")\n",
    "    print(\"   2. Run this cell again to check staging table\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Troubleshooting Recommendations\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° TROUBLESHOOTING GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìã If keys EXIST in CockroachDB but NOT in staging:\")\n",
    "print(\"   ‚Üí CDC files haven't been picked up by Auto Loader yet\")\n",
    "print(\"   ‚úÖ Solution: Re-run Cell 12 to process new CDC files\")\n",
    "\n",
    "print(\"\\nüìã If keys EXIST in staging but NOT in target:\")\n",
    "print(\"   ‚Üí MERGE logic failed or conditions are wrong\")\n",
    "print(\"   ‚úÖ Solution: Check Cell 12 output for MERGE errors\")\n",
    "print(\"   ‚úÖ Alternative: Check MERGE conditions in Cell 6\")\n",
    "\n",
    "print(\"\\nüìã If keys DON'T EXIST in CockroachDB:\")\n",
    "print(\"   ‚Üí Keys were deleted (normal for update_delete mode)\")\n",
    "print(\"   ‚úÖ Expected: Target should also not have these keys\")\n",
    "print(\"   ‚ö†Ô∏è  If target HAS these keys: MERGE delete logic isn't working\")\n",
    "\n",
    "print(\"\\nüìã If keys DON'T EXIST anywhere:\")\n",
    "print(\"   ‚Üí Keys were never created, or CDC didn't capture them\")\n",
    "print(\"   ‚úÖ Check: Run Cell 10 again to verify workload ran correctly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Optional: Cleanup\n",
    "\n",
    "Run the cells below if you want to clean up the test resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e067303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ‚ö†Ô∏è  SAFETY STOP: Cleanup Section\n",
    "# ============================================================================\n",
    "# This cell prevents accidental cleanup when running \"Run All\"\n",
    "# \n",
    "# To cleanup resources, manually run each cell below INDIVIDUALLY:\n",
    "#   - Cell 16: Cancel changefeed\n",
    "#   - Cell 17: Drop CockroachDB source table  \n",
    "#   - Cell 18: Drop Databricks target table & checkpoint\n",
    "#   - Cell 19: Clear Azure changefeed data (optional - use for complete reset)\n",
    "# ============================================================================\n",
    "\n",
    "raise RuntimeError(\n",
    "    \"\\n\"\n",
    "    \"‚ö†Ô∏è  CLEANUP SAFETY STOP\\n\"\n",
    "    \"\\n\"\n",
    "    \"The cells below will DELETE your resources.\\n\"\n",
    "    \"Do NOT run all cells - run each cleanup cell individually.\\n\"\n",
    "    \"\\n\"\n",
    "    \"üí° TIP: If Cell 13 shows sync issues due to old data,\\n\"\n",
    "    \"   run Cell 19 to clear Azure changefeed data completely.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cancel_changefeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP CELL 1: CANCEL CHANGEFEED\n",
    "# ============================================================================\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        # Find changefeed job for THIS specific source ‚Üí target mapping\n",
    "        # (matches the same path pattern used in Cell 7)\n",
    "        path_pattern = f\"%{source_table}%{source_catalog}/{source_schema}/{source_table}/{target_table}%\"\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "            SELECT job_id \n",
    "            FROM [SHOW CHANGEFEED JOBS] \n",
    "            WHERE description LIKE %s\n",
    "            AND status IN ('running', 'paused')\n",
    "            LIMIT 1\n",
    "        \"\"\", (path_pattern,))\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        if result:\n",
    "            job_id = result[0]\n",
    "            cur.execute(f\"CANCEL JOB {job_id}\")\n",
    "            print(f\"‚úÖ Changefeed {job_id} cancelled\")\n",
    "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
    "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è  No active changefeed found for this source ‚Üí target mapping\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drop_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP CELL 2: DROP SOURCE TABLE (CockroachDB)\n",
    "# ============================================================================\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f\"DROP TABLE IF EXISTS {source_table} CASCADE\")\n",
    "        conn.commit()\n",
    "    print(f\"‚úÖ Table '{source_table}' dropped from CockroachDB\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP CELL 19: CLEAR AZURE CHANGEFEED DATA (Optional)\n",
    "# ============================================================================\n",
    "# ‚ö†Ô∏è  WARNING: This will DELETE all changefeed data in Azure for this table!\n",
    "#\n",
    "# Use this when:\n",
    "# - You want to start completely fresh\n",
    "# - Old data from previous runs is causing sync issues\n",
    "# - You changed the table schema (e.g., VARCHAR ‚Üí INT)\n",
    "#\n",
    "# Uses Azure SDK (same as Cell 11 for checking files)\n",
    "# ============================================================================\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Build Azure path (must match Cell 7 changefeed path)\n",
    "changefeed_path = f\"parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}/\"\n",
    "\n",
    "print(f\"üóëÔ∏è  Deleting Azure changefeed data...\")\n",
    "print(f\"=\" * 80)\n",
    "print(f\"Container: {container_name}\")\n",
    "print(f\"Path: {changefeed_path}\")\n",
    "print()\n",
    "\n",
    "# Connect to Azure (same as Cell 9)\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service.get_container_client(container_name)\n",
    "\n",
    "# List all blobs with this prefix\n",
    "print(f\"üîç Scanning for files...\")\n",
    "blobs = list(container_client.list_blobs(name_starts_with=changefeed_path))\n",
    "\n",
    "if not blobs:\n",
    "    print(f\"‚ÑπÔ∏è  No files found at: {changefeed_path}\")\n",
    "    print(f\"   Files may have already been deleted, or path is incorrect\")\n",
    "    print()\n",
    "    print(f\"üí° To check what's in the container, run Cell 9\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(blobs)} items to delete\")\n",
    "    \n",
    "    # Show sample items\n",
    "    data_files = [b for b in blobs if b.size > 0 and '.parquet' in b.name]\n",
    "    resolved_files = [b for b in blobs if '.RESOLVED' in b.name]\n",
    "    directories = [b for b in blobs if b.size == 0]\n",
    "    \n",
    "    print(f\"   üìÑ Data files: {len(data_files)}\")\n",
    "    print(f\"   üïê Resolved files: {len(resolved_files)}\")\n",
    "    print(f\"   üìÅ Directories: {len(directories)}\")\n",
    "    print()\n",
    "    \n",
    "    # Delete all blobs with this prefix\n",
    "    # Note: Azure SDK doesn't have recursive delete - we list all blobs and delete each one\n",
    "    print(f\"üîÑ Deleting {len(blobs)} items...\")\n",
    "    deleted = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for blob in blobs:\n",
    "        try:\n",
    "            container_client.delete_blob(blob.name)\n",
    "            deleted += 1\n",
    "            if deleted % 50 == 0:\n",
    "                print(f\"   Deleted {deleted}/{len(blobs)} items...\", end='\\r')\n",
    "        except Exception as e:\n",
    "            # Some errors are expected (e.g., directories already removed)\n",
    "            error_str = str(e)\n",
    "            if \"DirectoryIsNotEmpty\" not in error_str and \"BlobNotFound\" not in error_str:\n",
    "                failed += 1\n",
    "                print(f\"\\n   ‚ö†Ô∏è  Failed: {blob.name[:60]}... - {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Deleted {deleted} items from Azure                    \")\n",
    "    if failed > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Failed to delete {failed} items\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"‚úÖ Cleanup complete!\")\n",
    "    print()\n",
    "    print(f\"üí° Next steps:\")\n",
    "    print(f\"   1. Drop the Databricks target table (Cell 17)\")\n",
    "    print(f\"   2. Re-run from Cell 6 (Snapshot) to start fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP CELL 3: DROP TARGET TABLE & CHECKPOINT (Databricks)\n",
    "# ============================================================================\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}\"  # Must match Cell 10\n",
    "\n",
    "# Drop Delta table\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
    "print(f\"‚úÖ Delta table '{target_table_fqn}' dropped\")\n",
    "\n",
    "# Remove checkpoint\n",
    "try:\n",
    "    dbutils.fs.rm(checkpoint_path, True)\n",
    "    print(f\"‚úÖ Checkpoint '{checkpoint_path}' removed\")\n",
    "except:\n",
    "    print(f\"‚ÑπÔ∏è  Checkpoint not found (may have been already removed)\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1fae06",
   "metadata": {},
   "source": [
    "# Debug Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\n",
    "    \"\\n\"\n",
    "    \"‚ö†Ô∏è  DEBUG SAFETY STOP\\n\"\n",
    "    \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEBUG CELL 1: Quick Missing Keys Check\n",
    "# ============================================================================\n",
    "# Lightweight check to see if specific keys exist in CockroachDB and staging\n",
    "# Update missing_keys list based on Cell 14/15 output\n",
    "# ============================================================================\n",
    "\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "staging_table_cf = f\"{target_table_fqn}_staging_cf\"\n",
    "missing_keys = [17, 18, 19]  # ‚Üê Update this based on Cell 14/15 output\n",
    "\n",
    "print(\"üîç Quick Debug: Checking missing keys...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check CockroachDB\n",
    "print(f\"\\nüìä CockroachDB ({source_table}):\")\n",
    "with get_cockroachdb_connection() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    for key in missing_keys:\n",
    "        cursor.execute(f\"SELECT * FROM {source_table} WHERE ycsb_key = %s\", (key,))\n",
    "        result = cursor.fetchone()\n",
    "        print(f\"   Key {key}: {'‚úÖ EXISTS' if result else '‚ùå NOT FOUND (deleted)'}\")\n",
    "\n",
    "# Check Staging Table\n",
    "print(f\"\\nüìä Staging Table ({staging_table_cf}):\")\n",
    "if spark.catalog.tableExists(staging_table_cf):\n",
    "    staging_df = spark.read.table(staging_table_cf)\n",
    "    for key in missing_keys:\n",
    "        count = staging_df.filter(F.col(\"ycsb_key\") == key).count()\n",
    "        print(f\"   Key {key}: {count} row(s)\")\n",
    "    \n",
    "    print(\"\\nüí° Next steps:\")\n",
    "    print(\"   - If keys exist in CockroachDB but not in staging:\")\n",
    "    print(\"     ‚Üí Re-run Cell 12 to pick up new CDC files\")\n",
    "    print(\"   - If keys exist in staging but not in target:\")\n",
    "    print(\"     ‚Üí Check Cell 12 output for MERGE errors\")\n",
    "    print(\"     ‚Üí Run DEBUG CELL 2 for detailed analysis\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Staging table doesn't exist (Cell 12 dropped it)\")\n",
    "    print(\"   üí° Re-run Cell 12 to recreate staging for debugging\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136cd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEBUG CELL 2: Inspect Target Table\n",
    "# ============================================================================\n",
    "# Comprehensive analysis of target table:\n",
    "# - CDC operation distribution\n",
    "# - Key distribution and gaps\n",
    "# - Duplicate detection\n",
    "# - Sample records\n",
    "# \n",
    "# Use this to diagnose sync issues and data quality problems\n",
    "# ============================================================================\n",
    "\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "\n",
    "print(f\"üìä Target Table Analysis: {target_table_fqn}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read target table\n",
    "df = spark.read.table(target_table_fqn)\n",
    "total_rows = df.count()\n",
    "print(f\"\\nüìà Total Rows: {total_rows:,}\")\n",
    "\n",
    "# Group by CDC operation type\n",
    "if \"_cdc_operation\" in df.columns:\n",
    "    print(\"\\nüîç CDC Operations:\")\n",
    "    df.groupBy(\"_cdc_operation\").count().orderBy(\"_cdc_operation\").show()\n",
    "    \n",
    "    # Check if DELETE rows are stored as data (should NOT happen)\n",
    "    delete_count = df.filter(F.col(\"_cdc_operation\") == \"DELETE\").count()\n",
    "    if delete_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Found {delete_count} DELETE rows stored as data!\")\n",
    "        print(\"   This is a bug - DELETE rows should not be in the target table.\")\n",
    "        print(\"   üí° Run Cell 16 to fix (drops and recreates table)\")\n",
    "\n",
    "# Show key distribution\n",
    "if \"ycsb_key\" in df.columns:\n",
    "    print(\"\\nüîç Key Distribution:\")\n",
    "    key_dist = df.groupBy(\"ycsb_key\").count().orderBy(\"ycsb_key\")\n",
    "    key_dist.show(50)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = key_dist.filter(\"count > 1\")\n",
    "    dup_count = duplicates.count()\n",
    "    if dup_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {dup_count} duplicate keys!\")\n",
    "        duplicates.show()\n",
    "        print(\"\\n   üí° This indicates deduplication failure in MERGE logic\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No duplicate keys found\")\n",
    "    \n",
    "    # Show key range and gaps\n",
    "    key_stats = df.select(\n",
    "        F.min(\"ycsb_key\").alias(\"min_key\"),\n",
    "        F.max(\"ycsb_key\").alias(\"max_key\"),\n",
    "        F.count(\"ycsb_key\").alias(\"count\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    expected_count = key_stats[\"max_key\"] - key_stats[\"min_key\"] + 1\n",
    "    actual_count = key_stats[\"count\"]\n",
    "    missing_count = expected_count - actual_count\n",
    "    \n",
    "    print(f\"\\nüìä Key Range Analysis:\")\n",
    "    print(f\"   Min key: {key_stats['min_key']}\")\n",
    "    print(f\"   Max key: {key_stats['max_key']}\")\n",
    "    print(f\"   Expected rows (if contiguous): {expected_count}\")\n",
    "    print(f\"   Actual rows: {actual_count}\")\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Missing {missing_count} keys (gaps in range)\")\n",
    "        print(f\"\\n   üí° Run DEBUG CELL 1 or CELL 3 to investigate specific keys\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No gaps (keys are contiguous)\")\n",
    "\n",
    "# Show sample records\n",
    "print(\"\\nüîç Sample Records (ordered by key):\")\n",
    "df.orderBy(\"ycsb_key\").show(30, truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° If you see issues:\")\n",
    "print(\"   - DELETE rows stored as data ‚Üí Run Cell 16 (recreate table)\")\n",
    "print(\"   - Duplicate keys ‚Üí Check MERGE deduplication logic\")\n",
    "print(\"   - Missing keys ‚Üí Run DEBUG CELL 1 or CELL 3\")\n",
    "print(\"   - Gaps in key range ‚Üí Keys were deleted (normal for update_delete mode)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85799f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEBUG CELL 3: Detailed Missing Keys Investigation\n",
    "# ============================================================================\n",
    "# Detailed investigation of missing keys:\n",
    "# - Checks CockroachDB source\n",
    "# - Checks staging table (if it exists)\n",
    "# - Shows CDC operation and timestamps\n",
    "# - Provides detailed troubleshooting steps\n",
    "#\n",
    "# Update missing_keys list based on Cell 14/15 or DEBUG CELL 2 output\n",
    "# ============================================================================\n",
    "\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "staging_table_cf = f\"{target_table_fqn}_staging_cf\"\n",
    "\n",
    "# Update this list based on what's missing\n",
    "missing_keys = [17, 18, 19]  # ‚Üê Update based on DEBUG CELL 2 output\n",
    "\n",
    "print(\"üîç Detailed Missing Keys Investigation\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Investigating keys: {missing_keys}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Check CockroachDB Source\n",
    "# ============================================================================\n",
    "print(\"üìä STEP 1: Checking CockroachDB Source\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with get_cockroachdb_connection() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for key in missing_keys:\n",
    "        cursor.execute(f\"SELECT * FROM {source_table} WHERE ycsb_key = %s\", (key,))\n",
    "        result = cursor.fetchone()\n",
    "        \n",
    "        if result:\n",
    "            print(f\"‚úÖ Key {key}: EXISTS in CockroachDB\")\n",
    "            # Show first 3 fields for verification\n",
    "            print(f\"   Sample data: {result[:min(3, len(result))]}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Key {key}: NOT FOUND in CockroachDB\")\n",
    "            print(f\"   ‚Üí This key was deleted (expected for update_delete mode)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Check Staging Table\n",
    "# ============================================================================\n",
    "print(f\"\\nüìä STEP 2: Checking Staging Table\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if spark.catalog.tableExists(staging_table_cf):\n",
    "    staging_df = spark.read.table(staging_table_cf)\n",
    "    print(f\"‚úÖ Staging table exists: {staging_table_cf}\")\n",
    "    print()\n",
    "    \n",
    "    for key in missing_keys:\n",
    "        key_rows = staging_df.filter(F.col(\"ycsb_key\") == key)\n",
    "        count = key_rows.count()\n",
    "        \n",
    "        if count > 0:\n",
    "            print(f\"‚úÖ Key {key}: {count} row(s) in staging table\")\n",
    "            print(\"   Details:\")\n",
    "            key_rows.select(\n",
    "                \"ycsb_key\", \n",
    "                \"_cdc_timestamp\", \n",
    "                \"_cdc_operation\", \n",
    "                \"field0\", \n",
    "                \"field1\"\n",
    "            ).show(truncate=False)\n",
    "        else:\n",
    "            print(f\"‚ùå Key {key}: NOT in staging table\")\n",
    "    \n",
    "    # Show staging table summary\n",
    "    print(\"\\nüìà Staging Table Summary:\")\n",
    "    staging_df.groupBy(\"_cdc_operation\").count().show()\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Staging table doesn't exist: {staging_table_cf}\")\n",
    "    print(\"   This means Cell 12 completed and dropped the staging table\")\n",
    "    print(\"\\nüí° To debug further:\")\n",
    "    print(\"   1. Re-run Cell 12 (it will process new files and recreate staging)\")\n",
    "    print(\"   2. Run this cell again to check staging table\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Troubleshooting Recommendations\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° TROUBLESHOOTING GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìã If keys EXIST in CockroachDB but NOT in staging:\")\n",
    "print(\"   ‚Üí CDC files haven't been picked up by Auto Loader yet\")\n",
    "print(\"   ‚úÖ Solution: Re-run Cell 12 to process new CDC files\")\n",
    "\n",
    "print(\"\\nüìã If keys EXIST in staging but NOT in target:\")\n",
    "print(\"   ‚Üí MERGE logic failed or conditions are wrong\")\n",
    "print(\"   ‚úÖ Solution: Check Cell 12 output for MERGE errors\")\n",
    "print(\"   ‚úÖ Alternative: Check MERGE conditions in Cell 6\")\n",
    "\n",
    "print(\"\\nüìã If keys DON'T EXIST in CockroachDB:\")\n",
    "print(\"   ‚Üí Keys were deleted (normal for update_delete mode)\")\n",
    "print(\"   ‚úÖ Expected: Target should also not have these keys\")\n",
    "print(\"   ‚ö†Ô∏è  If target HAS these keys: MERGE delete logic isn't working\")\n",
    "\n",
    "print(\"\\nüìã If keys DON'T EXIST anywhere:\")\n",
    "print(\"   ‚Üí Keys were never created, or CDC didn't capture them\")\n",
    "print(\"   ‚úÖ Check: Run Cell 10 again to verify workload ran correctly\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_311_dogfood (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
