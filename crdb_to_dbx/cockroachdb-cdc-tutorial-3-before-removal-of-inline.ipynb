{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Stream CockroachDB CDC to Databricks (Azure)\n",
    "\n",
    "This notebook demonstrates how to stream CockroachDB changefeeds to Databricks using Azure Blob Storage.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- CockroachDB cluster (Cloud or self-hosted)\n",
    "- Azure Storage Account with hierarchical namespace enabled\n",
    "- Databricks workspace with Unity Catalog\n",
    "- Unity Catalog External Location configured for your storage account\n",
    "\n",
    "**Note:** This notebook uses the **YCSB (Yahoo! Cloud Serving Benchmark)** schema as the default table structure, with `ycsb_key` as the primary key and `field0-9` columns. The default schema name is `public`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c81271",
   "metadata": {},
   "source": [
    "## CDC Mode Selection\n",
    "\n",
    "This notebook supports **4 CDC ingestion modes** by combining two independent settings:\n",
    "\n",
    "### 1. CDC Processing Mode (`cdc_mode`)\n",
    "How CDC events are processed in the target table:\n",
    "\n",
    "- **`append_only`**: Store all CDC events as rows (audit log)\n",
    "  - **Behavior**: All events (INSERT/UPDATE/DELETE) are appended as new rows\n",
    "  - **Use case**: History tracking, time-series analysis, audit logs\n",
    "  - **Storage**: Higher (keeps all historical events)\n",
    "\n",
    "- **`update_delete`**: Apply MERGE logic (current state replication)\n",
    "  - **Behavior**: DELETE removes rows, UPDATE modifies rows in-place\n",
    "  - **Use case**: Current state synchronization, production replication\n",
    "  - **Storage**: Lower (only latest state per key)\n",
    "\n",
    "### 2. Column Family Mode (`column_family_mode`)\n",
    "Table structure and changefeed configuration:\n",
    "\n",
    "- **`single_cf`**: Standard table (1 column family, default)\n",
    "  - **Changefeed**: `split_column_families=false`\n",
    "  - **Files**: 1 Parquet file per CDC event\n",
    "  - **Use case**: Most tables, simpler configuration, better performance\n",
    "\n",
    "- **`multi_cf`**: Multiple column families (for wide tables)\n",
    "  - **Changefeed**: `split_column_families=true`\n",
    "  - **Files**: Multiple Parquet files per CDC event (fragments need merging)\n",
    "  - **Use case**: Wide tables (50+ columns), selective column access patterns\n",
    "\n",
    "### Function Selection Matrix\n",
    "\n",
    "The notebook automatically selects the appropriate ingestion function based on your configuration:\n",
    "\n",
    "| CDC Mode | Column Family Mode | Function Called |\n",
    "|----------|-------------------|-----------------|\n",
    "| `append_only` | `single_cf` | `ingest_cdc_append_only_single_family()` |\n",
    "| `append_only` | `multi_cf` | `ingest_cdc_append_only_multi_family()` |\n",
    "| `update_delete` | `single_cf` | `ingest_cdc_with_merge_single_family()` |\n",
    "| `update_delete` | `multi_cf` | `ingest_cdc_with_merge_multi_family()` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Configuration file path (adjust as needed)\n",
    "config_file = \"/Users/robert.lee/github/lakeflow-community-connectors/sources/cockroachdb/.env/cockroachdb_cdc_tutorial_config_update_delete_multi_cf.json\"\n",
    "\n",
    "#config_file = \"/Users/robert.lee/github/lakeflow-community-connectors/sources/cockroachdb/.env/cockroachdb_cdc_tutorial_config_update_delete_single_cf.json\"\n",
    "\n",
    "#config_file = \"/Users/robert.lee/github/lakeflow-community-connectors/sources/cockroachdb/.env/cockroachdb_cdc_tutorial_config.json\"\n",
    "\n",
    "# Try to load from file, fallback to embedded config\n",
    "try:\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(f\"‚úÖ Configuration loaded from: {config_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Using embedded configuration (config file error: {e})\")\n",
    "    config = None\n",
    "\n",
    "# Embedded configuration (fallback)\n",
    "if config is None:\n",
    "    config = {\n",
    "      \"cockroachdb\": {\n",
    "        \"host\": \"replace_me\",\n",
    "        \"port\": 26257,\n",
    "        \"user\": \"replace_me\",\n",
    "        \"password\": \"replace_me\",\n",
    "        \"database\": \"defaultdb\"\n",
    "      },\n",
    "      \"cockroachdb_source\": {\n",
    "        \"catalog\": \"defaultdb\",\n",
    "        \"schema\": \"public\",\n",
    "        \"table_name\": \"usertable\",\n",
    "        \"_schema_note\": \"Default schema is 'public'. Table uses YCSB structure (ycsb_key, field0-9)\",\n",
    "      },\n",
    "      \"azure_storage\": {\n",
    "        \"account_name\": \"replace_me\",\n",
    "        \"account_key\": \"replace_me\",\n",
    "        \"container_name\": \"changefeed-events\"\n",
    "      },\n",
    "      \"databricks_target\": {\n",
    "        \"catalog\": \"main\",\n",
    "        \"schema\": \"replace_me\",\n",
    "        \"table_name\": \"usertable\",\n",
    "      },\n",
    "      \"cdc_config\": {\n",
    "        \"mode\": \"append_only\",\n",
    "        \"column_family_mode\": \"multi_cf\",\n",
    "        \"primary_key_columns\": [\"ycsb_key\"],\n",
    "        \"auto_suffix_mode_family\": True,\n",
    "      },\n",
    "      \"workload_config\": {\n",
    "        \"snapshot_count\": 10,\n",
    "        \"insert_count\": 10,\n",
    "        \"update_count\": 9,\n",
    "        \"delete_count\": 8,\n",
    "      }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "\n",
    "# Extract configuration values\n",
    "cockroachdb_host = config[\"cockroachdb\"][\"host\"]\n",
    "cockroachdb_port = config[\"cockroachdb\"][\"port\"]\n",
    "cockroachdb_user = config[\"cockroachdb\"][\"user\"]\n",
    "cockroachdb_password = config[\"cockroachdb\"][\"password\"]\n",
    "cockroachdb_database = config[\"cockroachdb\"][\"database\"]\n",
    "\n",
    "source_catalog = config[\"cockroachdb_source\"][\"catalog\"]\n",
    "source_schema = config[\"cockroachdb_source\"][\"schema\"]\n",
    "source_table = config[\"cockroachdb_source\"][\"table_name\"]\n",
    "\n",
    "storage_account_name = config[\"azure_storage\"][\"account_name\"]\n",
    "storage_account_key = config[\"azure_storage\"][\"account_key\"]\n",
    "storage_account_key_encoded = quote(storage_account_key, safe='')\n",
    "container_name = config[\"azure_storage\"][\"container_name\"]\n",
    "\n",
    "target_catalog = config[\"databricks_target\"][\"catalog\"]\n",
    "target_schema = config[\"databricks_target\"][\"schema\"]\n",
    "target_table = config[\"databricks_target\"][\"table_name\"]\n",
    "\n",
    "cdc_mode = config[\"cdc_config\"][\"mode\"]\n",
    "column_family_mode = config[\"cdc_config\"][\"column_family_mode\"]\n",
    "primary_key_columns = config[\"cdc_config\"][\"primary_key_columns\"]\n",
    "\n",
    "snapshot_count = config[\"workload_config\"][\"snapshot_count\"]\n",
    "insert_count = config[\"workload_config\"][\"insert_count\"]\n",
    "update_count = config[\"workload_config\"][\"update_count\"]\n",
    "delete_count = config[\"workload_config\"][\"delete_count\"]\n",
    "\n",
    "# Auto-suffix table names with mode and column family if enabled\n",
    "auto_suffix = config[\"cdc_config\"].get(\"auto_suffix_mode_family\", False)\n",
    "if auto_suffix:\n",
    "    suffix = f\"_{cdc_mode}_{column_family_mode}\"\n",
    "    \n",
    "    # Add suffix to source_table if not already present\n",
    "    if not source_table.endswith(suffix):\n",
    "        source_table = f\"{source_table}{suffix}\"\n",
    "    \n",
    "    # Add suffix to target_table if not already present\n",
    "    if not target_table.endswith(suffix):\n",
    "        target_table = f\"{target_table}{suffix}\"\n",
    "\n",
    "    # Update config dict with suffixed table names\n",
    "    config[\"cockroachdb_source\"][\"table_name\"] = source_table\n",
    "    config[\"databricks_target\"][\"table_name\"] = target_table\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   CDC Processing Mode: {cdc_mode}\")\n",
    "print(f\"   Column Family Mode: {column_family_mode}\")\n",
    "print(f\"   Primary Keys: {primary_key_columns}\")\n",
    "print(f\"   Target Table: {target_table}\")\n",
    "print(f\"   CDC Workload: {snapshot_count} snapshot ‚Üí +{insert_count} INSERTs, ~{update_count} UPDATEs, -{delete_count} DELETEs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pg8000 azure-storage-blob --quiet\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pg8000\n",
    "import ssl\n",
    "\n",
    "def get_cockroachdb_connection():\n",
    "    \"\"\"Create connection to CockroachDB using pg8000\"\"\"\n",
    "    # Create SSL context (required for CockroachDB Cloud)\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "    \n",
    "    # Parse host (in case port is accidentally included in host string)\n",
    "    host = cockroachdb_host.split(':')[0] if ':' in cockroachdb_host else cockroachdb_host\n",
    "    \n",
    "    conn = pg8000.connect(\n",
    "        user=cockroachdb_user,\n",
    "        password=cockroachdb_password,\n",
    "        host=host,\n",
    "        port=cockroachdb_port,\n",
    "        database=cockroachdb_database,\n",
    "        ssl_context=ssl_context\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_cockroachdb_connection()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT version()\")\n",
    "        version = cur.fetchone()[0]\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"‚úÖ Connected to CockroachDB\")\n",
    "    print(f\"   Version: {version[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c40c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_table_stats(conn, table_name):\n",
    "    \"\"\"\n",
    "    Get min key, max key, and count for a table.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        table_name: Name of the table\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'min_key', 'max_key', 'count', 'is_empty'\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f\"SELECT MIN(ycsb_key), MAX(ycsb_key), COUNT(*) FROM {table_name}\")\n",
    "        result = cur.fetchone()\n",
    "        min_key, max_key, count = result\n",
    "        \n",
    "        return {\n",
    "            'min_key': min_key,\n",
    "            'max_key': max_key,\n",
    "            'count': count,\n",
    "            'is_empty': min_key is None and max_key is None\n",
    "        }\n",
    "\n",
    "\n",
    "def check_azure_files(storage_account_name, storage_account_key, container_name, \n",
    "                      source_catalog, source_schema, source_table, target_table, \n",
    "                      verbose=True):\n",
    "    \"\"\"\n",
    "    Check for changefeed files in Azure Blob Storage.\n",
    "    \n",
    "    Args:\n",
    "        storage_account_name: Azure storage account name\n",
    "        storage_account_key: Azure storage account key\n",
    "        container_name: Azure container name\n",
    "        source_catalog: CockroachDB catalog (database)\n",
    "        source_schema: CockroachDB schema\n",
    "        source_table: Source table name\n",
    "        target_table: Target table name\n",
    "        verbose: Print detailed output\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'data_files' and 'resolved_files' lists\n",
    "    \"\"\"\n",
    "    # Connect to Azure\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "    blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service.get_container_client(container_name)\n",
    "    \n",
    "    # Build path - list ALL files recursively under this changefeed path\n",
    "    prefix = f\"parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}/\"\n",
    "    \n",
    "    # List all blobs recursively (no date filtering)\n",
    "    blobs = list(container_client.list_blobs(name_starts_with=prefix))\n",
    "    \n",
    "    # Categorize files (using same filtering logic as cockroachdb.py)\n",
    "    # Data files: .parquet files, excluding:\n",
    "    #   - .RESOLVED files (CDC watermarks)\n",
    "    #   - _metadata/ directory (schema files)\n",
    "    #   - Files starting with _ (_SUCCESS, _committed_*, etc.)\n",
    "    data_files = [\n",
    "        b for b in blobs \n",
    "        if b.name.endswith('.parquet') \n",
    "        and '.RESOLVED' not in b.name\n",
    "        and '/_metadata/' not in b.name\n",
    "        and not b.name.split('/')[-1].startswith('_')\n",
    "    ]\n",
    "    resolved_files = [b for b in blobs if '.RESOLVED' in b.name]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìÅ Files in Azure changefeed path:\")\n",
    "        print(f\"   Path: {prefix}\")\n",
    "        print(f\"   üìÑ Data files: {len(data_files)}\")\n",
    "        print(f\"   üïê Resolved files: {len(resolved_files)}\")\n",
    "        print(f\"   üìä Total: {len(blobs)}\")\n",
    "        \n",
    "        if data_files:\n",
    "            print(f\"\\n   Example data file:\")\n",
    "            print(f\"   {data_files[0].name}\")\n",
    "    \n",
    "    return {\n",
    "        'data_files': data_files,\n",
    "        'resolved_files': resolved_files,\n",
    "        'total': len(blobs)\n",
    "    }\n",
    "\n",
    "\n",
    "def wait_for_changefeed_files(storage_account_name, storage_account_key, container_name,\n",
    "                               source_catalog, source_schema, source_table, target_table,\n",
    "                               max_wait=120, check_interval=5, stabilization_wait=5):\n",
    "    \"\"\"\n",
    "    Wait for changefeed files to appear in Azure with timeout and stabilization period.\n",
    "    \n",
    "    This function:\n",
    "    1. Polls Azure until first file(s) appear\n",
    "    2. Once files are detected, waits for additional files (important for column families)\n",
    "    3. Exits when no new files appear for 'stabilization_wait' seconds\n",
    "    \n",
    "    Args:\n",
    "        max_wait: Maximum seconds to wait for initial files (default: 120)\n",
    "        check_interval: Seconds between checks (default: 5)\n",
    "        stabilization_wait: Seconds to wait for file count to stabilize (default: 5)\n",
    "                           Important for column family mode where multiple files are written\n",
    "    \n",
    "    Returns:\n",
    "        True if files found, False if timeout\n",
    "    \"\"\"\n",
    "    print(f\"‚è≥ Waiting for initial snapshot files to appear in Azure...\")\n",
    "    \n",
    "    elapsed = 0\n",
    "    files_found = False\n",
    "    last_file_count = 0\n",
    "    stable_elapsed = 0\n",
    "    \n",
    "    while elapsed < max_wait:\n",
    "        result = check_azure_files(\n",
    "            storage_account_name, storage_account_key, container_name,\n",
    "            source_catalog, source_schema, source_table, target_table,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        current_file_count = len(result['data_files'])\n",
    "        \n",
    "        if not files_found and current_file_count > 0:\n",
    "            # First files detected - switch to stabilization mode\n",
    "            files_found = True\n",
    "            last_file_count = current_file_count\n",
    "            stable_elapsed = 0\n",
    "            print(f\"\\n‚úÖ First files appeared after {elapsed} seconds!\")\n",
    "            print(f\"   Found {current_file_count} file(s) so far...\")\n",
    "            print(f\"   Waiting {stabilization_wait}s for more files (column family fragments)...\")\n",
    "        \n",
    "        elif files_found:\n",
    "            # In stabilization mode - check if file count is stable\n",
    "            if current_file_count > last_file_count:\n",
    "                # More files arrived - reset stabilization timer\n",
    "                print(f\"   üìÑ File count increased: {last_file_count} ‚Üí {current_file_count}\")\n",
    "                last_file_count = current_file_count\n",
    "                stable_elapsed = 0\n",
    "            else:\n",
    "                # File count unchanged - increment stabilization timer\n",
    "                stable_elapsed += check_interval\n",
    "                \n",
    "                if stable_elapsed >= stabilization_wait:\n",
    "                    # Stabilization period complete - all files have landed\n",
    "                    print(f\"\\n‚úÖ File count stable at {current_file_count} for {stabilization_wait}s\")\n",
    "                    print(f\"   Total wait time: {elapsed + stable_elapsed}s\")\n",
    "                    print(f\"   Example: {result['data_files'][0].name}\")\n",
    "                    return True\n",
    "        \n",
    "        if not files_found:\n",
    "            print(f\"   Checking... ({elapsed}s elapsed)\", end='\\r')\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "        elapsed += check_interval\n",
    "    \n",
    "    if files_found:\n",
    "        # Files were found but stabilization didn't complete within max_wait\n",
    "        print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s (found {last_file_count} files but more may still be generating)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - no files appeared\")\n",
    "    \n",
    "    print(f\"   Run Cell 11 to check manually\")\n",
    "    return files_found  # Return True if we found at least some files\n",
    "\n",
    "\n",
    "\n",
    "def get_column_sum(conn, table_name, column_name):\n",
    "    \"\"\"\n",
    "    Get the sum of a numeric column in a table.\n",
    "    Text columns have non-numeric characters stripped before casting.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        table_name: Name of the table\n",
    "        column_name: Name of the column to sum\n",
    "    \n",
    "    Returns:\n",
    "        Sum of the column (handles mixed text/numeric values)\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        # Strip non-numeric chars, handle empty strings, cast to BIGINT\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT SUM(\n",
    "                CASE \n",
    "                    WHEN regexp_replace({column_name}::TEXT, '[^0-9]', '', 'g') = '' THEN 0\n",
    "                    ELSE regexp_replace({column_name}::TEXT, '[^0-9]', '', 'g')::BIGINT\n",
    "                END\n",
    "            ) \n",
    "            FROM {table_name}\n",
    "        \"\"\")\n",
    "        result = cur.fetchone()\n",
    "        return result[0]\n",
    "\n",
    "\n",
    "\n",
    "def get_column_sum_spark(df, column_name):\n",
    "    \"\"\"\n",
    "    Get the sum of a numeric column in a Spark DataFrame.\n",
    "    Text columns have non-numeric characters stripped before casting.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        column_name: Name of the column to sum\n",
    "    \n",
    "    Returns:\n",
    "        Sum of the column (handles mixed text/numeric values)\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    result = df.select(\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.regexp_replace(F.col(column_name).cast('string'), '[^0-9]', '') == '',\n",
    "                0\n",
    "            ).otherwise(\n",
    "                F.regexp_replace(F.col(column_name).cast('string'), '[^0-9]', '').cast('bigint')\n",
    "            )\n",
    "        ).alias('sum')\n",
    "    ).collect()[0]['sum']\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded (CockroachDB & Azure)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6719c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_column_family_fragments(\n",
    "    df,\n",
    "    primary_key_columns,\n",
    "    spark=None,\n",
    "    metadata_columns=None,\n",
    "    debug=False,\n",
    "    is_streaming=None,\n",
    "    deduplicate_to_latest_state=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Merge column family fragments into complete rows.\n",
    "    \n",
    "    When split_column_families=true, CockroachDB writes one Parquet file per column family,\n",
    "    resulting in multiple fragment records per logical row. This function merges these\n",
    "    fragments by grouping on primary key and taking the first non-null value for each column.\n",
    "    \n",
    "    **Streaming vs Batch Mode:**\n",
    "    - **Streaming DataFrames** (from Autoloader): Always applies merge (can't detect beforehand)\n",
    "    - **Batch DataFrames** (from spark.read): Auto-detects fragmentation, skips if not needed\n",
    "    - Set `is_streaming=True` to force streaming mode (skips detection)\n",
    "    - Set `is_streaming=False` to force batch mode (enables detection)\n",
    "    \n",
    "    **Deduplication Mode (NEW):**\n",
    "    - `deduplicate_to_latest_state=False` (default): Merges fragments within same event, preserves all events\n",
    "    - `deduplicate_to_latest_state=True`: Coalesces columns across time + deduplicates to latest state\n",
    "      * Use when you have multiple UPDATE events for the same key\n",
    "      * Preserves old values for columns not touched by newer events\n",
    "      * Example: Event1 has field3=3, Event2 updates field0 but leaves field3=NULL\n",
    "        ‚Üí Result keeps field3=3 from Event1 (not NULL from Event2)\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame with potential column family fragments\n",
    "        primary_key_columns: List of primary key column names (e.g., ['ycsb_key'])\n",
    "        spark: SparkSession (optional, for backward compatibility)\n",
    "        metadata_columns: Optional list of metadata columns to preserve\n",
    "                         (default: __crdb__*, _cdc_*, _source_*, _rescued_data)\n",
    "        debug: Enable debug output showing merge statistics\n",
    "        is_streaming: Optional boolean to force streaming/batch mode\n",
    "                     (default: auto-detect based on df.isStreaming)\n",
    "        deduplicate_to_latest_state: If True, coalesce columns across time and deduplicate to latest row\n",
    "                                    (default: False - preserves all CDC events)\n",
    "        \n",
    "    Returns:\n",
    "        Merged Spark DataFrame with complete rows\n",
    "        \n",
    "    Example (Standard Mode):\n",
    "        ```python\n",
    "        df_merged = merge_column_family_fragments(\n",
    "            df_raw,\n",
    "            primary_key_columns=['ycsb_key'],\n",
    "            debug=True\n",
    "        )\n",
    "        ```\n",
    "        \n",
    "    Example (Deduplication Mode - NEW):\n",
    "        ```python\n",
    "        df_latest = merge_column_family_fragments(\n",
    "            df_staging,\n",
    "            primary_key_columns=['ycsb_key'],\n",
    "            deduplicate_to_latest_state=True,  # ‚Üê Preserves old values\n",
    "            debug=True\n",
    "        )\n",
    "        ```\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    # Auto-detect streaming mode if not explicitly set\n",
    "    if is_streaming is None:\n",
    "        is_streaming = df.isStreaming\n",
    "    \n",
    "    # Default metadata columns to preserve\n",
    "    if metadata_columns is None:\n",
    "        metadata_columns = [\n",
    "            '__crdb__event_type', '__crdb__updated', '_rescued_data',\n",
    "            '_cdc_operation', '_cdc_timestamp', '_cdc_updated', '_source_file', '_processing_time',\n",
    "            '_metadata',  # Unity Catalog metadata\n",
    "            'after', 'before', 'key', 'updated',  # JSON envelope columns\n",
    "            '_after_json', '_before_json', '_debug_after_first_10', '_debug_before_first_10'\n",
    "        ]\n",
    "    \n",
    "    # Get all columns from DataFrame\n",
    "    all_columns = df.columns\n",
    "    \n",
    "    # Identify data columns (everything except PK and metadata)\n",
    "    data_columns = [\n",
    "        col for col in all_columns \n",
    "        if col not in primary_key_columns and col not in metadata_columns\n",
    "        and not col.startswith('__crdb__')\n",
    "        and not col.startswith('_cdc_')\n",
    "        and not col.startswith('_source_')\n",
    "        and not col.startswith('_rescued_')\n",
    "    ]\n",
    "    \n",
    "    if debug:\n",
    "        mode_str = \"Streaming\" if is_streaming else \"Batch\"\n",
    "        print(f\"\\nüîç Column Family Merge ({mode_str} Mode)\")\n",
    "        print(f\"   Primary key columns: {primary_key_columns}\")\n",
    "        print(f\"   Data columns: {len(data_columns)} columns\")\n",
    "        if len(data_columns) <= 10:\n",
    "            print(f\"     {data_columns}\")\n",
    "        else:\n",
    "            print(f\"     {data_columns[:5]}... (showing first 5)\")\n",
    "    \n",
    "    # For batch mode: Check if merge is needed\n",
    "    if not is_streaming:\n",
    "        try:\n",
    "            # Determine timestamp column for fragmentation detection\n",
    "            timestamp_col_for_check = None\n",
    "            if '_cdc_timestamp' in all_columns:\n",
    "                timestamp_col_for_check = '_cdc_timestamp'\n",
    "            elif '_cdc_updated' in all_columns:\n",
    "                timestamp_col_for_check = '_cdc_updated'\n",
    "            elif '__crdb__updated' in all_columns:\n",
    "                timestamp_col_for_check = '__crdb__updated'\n",
    "            elif 'updated' in all_columns:\n",
    "                timestamp_col_for_check = 'updated'\n",
    "            \n",
    "            # Try to detect fragmentation\n",
    "            total_rows = df.count()\n",
    "            \n",
    "            if timestamp_col_for_check and '_cdc_operation' in all_columns:\n",
    "                unique_events = df.select(primary_key_columns + [timestamp_col_for_check, '_cdc_operation']).distinct().count()\n",
    "            elif timestamp_col_for_check:\n",
    "                unique_events = df.select(primary_key_columns + [timestamp_col_for_check]).distinct().count()\n",
    "            elif '_cdc_operation' in all_columns:\n",
    "                unique_events = df.select(primary_key_columns + ['_cdc_operation']).distinct().count()\n",
    "            else:\n",
    "                unique_events = df.select(primary_key_columns).distinct().count()\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"\\nüìä Fragmentation Detection:\")\n",
    "                print(f\"   Total rows: {total_rows:,}\")\n",
    "                print(f\"   Unique events: {unique_events:,}\")\n",
    "                print(f\"   Duplication ratio: {total_rows / unique_events if unique_events > 0 else 0:.1f}x\")\n",
    "            \n",
    "            # If no duplicates, return original DataFrame\n",
    "            if total_rows == unique_events:\n",
    "                if debug:\n",
    "                    print(f\"\\n‚úÖ No column family fragmentation detected\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"\\n‚ö†Ô∏è  Detection failed (treating as streaming): {e}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # DEDUPLICATION MODE: Coalesce columns across time + deduplicate to latest state\n",
    "    # ============================================================================\n",
    "    if deduplicate_to_latest_state:\n",
    "        if debug:\n",
    "            print(f\"\\nüîÑ Applying cross-time coalescing + deduplication...\")\n",
    "            print(f\"   (Preserves latest non-NULL value per column across all events)\")\n",
    "        \n",
    "        # Determine timestamp column for ordering\n",
    "        timestamp_col_for_coalesce = None\n",
    "        if '_cdc_timestamp' in all_columns:\n",
    "            timestamp_col_for_coalesce = '_cdc_timestamp'\n",
    "        elif '_cdc_updated' in all_columns:\n",
    "            timestamp_col_for_coalesce = '_cdc_updated'\n",
    "        elif '__crdb__updated' in all_columns:\n",
    "            timestamp_col_for_coalesce = '__crdb__updated'\n",
    "        elif 'updated' in all_columns:\n",
    "            timestamp_col_for_coalesce = 'updated'\n",
    "        \n",
    "        if not timestamp_col_for_coalesce:\n",
    "            if debug:\n",
    "                print(f\"   ‚ö†Ô∏è  No timestamp column found - cannot coalesce across time\")\n",
    "                print(f\"   Falling back to standard merge mode\")\n",
    "        else:\n",
    "            # Step 1: Coalesce columns across time (keep latest non-NULL value per column)\n",
    "            if debug:\n",
    "                print(f\"   Step 1: Coalescing columns by primary keys: {primary_key_columns}...\")\n",
    "                print(f\"           Using last_value(col, ignorenulls=True) per column\")\n",
    "            \n",
    "            # Window spec: partition by PK, order by timestamp, look at all rows\n",
    "            window_spec_coalesce = (Window.partitionBy(*primary_key_columns)\n",
    "                .orderBy(F.col(timestamp_col_for_coalesce))\n",
    "                .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))\n",
    "            \n",
    "            # For each data column, coalesce to latest non-NULL value\n",
    "            for col in data_columns:\n",
    "                df = df.withColumn(\n",
    "                    col,\n",
    "                    F.last(F.col(col), ignorenulls=True).over(window_spec_coalesce)\n",
    "                )\n",
    "            \n",
    "            # Also coalesce _cdc_operation to the LATEST value (for DELETE handling)\n",
    "            if '_cdc_operation' in all_columns:\n",
    "                df = df.withColumn(\n",
    "                    \"_cdc_operation\",\n",
    "                    F.last(F.col(\"_cdc_operation\"), ignorenulls=True).over(window_spec_coalesce)\n",
    "                )\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"   ‚úÖ Columns coalesced (latest non-NULL value per column)\")\n",
    "            \n",
    "            # Step 2: Deduplicate by primary key (keep LATEST row, which now has ALL coalesced columns)\n",
    "            if debug:\n",
    "                print(f\"   Step 2: Deduplicating by primary keys: {primary_key_columns}...\")\n",
    "            \n",
    "            window_spec_dedup = Window.partitionBy(*primary_key_columns).orderBy(F.col(timestamp_col_for_coalesce).desc())\n",
    "            df_merged = (df\n",
    "                .withColumn(\"_row_num\", F.row_number().over(window_spec_dedup))\n",
    "                .filter(F.col(\"_row_num\") == 1)\n",
    "                .drop(\"_row_num\")\n",
    "            )\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"   ‚úÖ Deduplication complete!\")\n",
    "                print(f\"      Result: Latest state per primary key with all column values preserved\")\n",
    "            \n",
    "            return df_merged\n",
    "    \n",
    "    # ============================================================================\n",
    "    # STANDARD MODE: Merge fragments within events, preserve all CDC events\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Determine which timestamp column to use for grouping\n",
    "    timestamp_col = None\n",
    "    if '_cdc_timestamp' in all_columns:\n",
    "        timestamp_col = '_cdc_timestamp'\n",
    "    elif '_cdc_updated' in all_columns:\n",
    "        timestamp_col = '_cdc_updated'\n",
    "    elif '__crdb__updated' in all_columns:\n",
    "        timestamp_col = '__crdb__updated'\n",
    "    elif 'updated' in all_columns:\n",
    "        timestamp_col = 'updated'\n",
    "    \n",
    "    # Build aggregation expressions\n",
    "    agg_exprs = []\n",
    "    \n",
    "    # Determine grouping columns\n",
    "    if timestamp_col and '_cdc_operation' in all_columns:\n",
    "        # Group by PK + timestamp + operation to preserve all distinct CDC events\n",
    "        group_by_cols = primary_key_columns + [timestamp_col, '_cdc_operation']\n",
    "        \n",
    "        # For aggregation, use first() with ignorenulls to combine NULL values from fragments\n",
    "        for col in data_columns:\n",
    "            agg_exprs.append(F.first(col, ignorenulls=True).alias(col))\n",
    "        \n",
    "        # Metadata columns: also use first()\n",
    "        for col in metadata_columns:\n",
    "            if col in all_columns and col not in group_by_cols:\n",
    "                agg_exprs.append(F.first(col, ignorenulls=True).alias(col))\n",
    "    elif timestamp_col:\n",
    "        # Fallback: Group by PK + timestamp only\n",
    "        group_by_cols = primary_key_columns + [timestamp_col]\n",
    "        \n",
    "        for col in data_columns:\n",
    "            agg_exprs.append(F.first(col, ignorenulls=True).alias(col))\n",
    "        \n",
    "        for col in metadata_columns:\n",
    "            if col in all_columns and col not in group_by_cols:\n",
    "                agg_exprs.append(F.first(col, ignorenulls=True).alias(col))\n",
    "    else:\n",
    "        # No timestamp column - group by PK + operation if available\n",
    "        if '_cdc_operation' in all_columns:\n",
    "            group_by_cols = primary_key_columns + ['_cdc_operation']\n",
    "        else:\n",
    "            group_by_cols = primary_key_columns\n",
    "        \n",
    "        for col in data_columns:\n",
    "            agg_exprs.append(F.first(col, ignorenulls=True).alias(col))\n",
    "        \n",
    "        for col in metadata_columns:\n",
    "            if col in all_columns and col not in group_by_cols:\n",
    "                agg_exprs.append(F.first(col, ignorenulls=True).alias(col))\n",
    "    \n",
    "    # Group and aggregate\n",
    "    if not agg_exprs:\n",
    "        # No columns to aggregate beyond grouping key\n",
    "        if debug:\n",
    "            print(f\"\\n‚ö†Ô∏è  No additional columns to aggregate beyond grouping key\")\n",
    "        df_merged = df.select(*group_by_cols).distinct()\n",
    "    else:\n",
    "        df_merged = df.groupBy(*group_by_cols).agg(*agg_exprs)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n‚úÖ Merge transformation applied!\")\n",
    "    \n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table structure based on column_family_mode:\n",
    "# - single_cf: 1 column family (default, better performance)\n",
    "# - multi_cf: 3 column families (for testing split_column_families=true)\n",
    "\n",
    "if column_family_mode == \"multi_cf\":\n",
    "    # Create table with MULTIPLE column families for testing split_column_families=true\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {source_table} (\n",
    "        ycsb_key INT PRIMARY KEY,\n",
    "        -- Family 1: Frequently accessed fields\n",
    "        field0 TEXT,\n",
    "        field1 TEXT,\n",
    "        field2 TEXT,\n",
    "        FAMILY frequently_read (ycsb_key, field0, field1, field2),\n",
    "        \n",
    "        -- Family 2: Medium-frequency fields\n",
    "        field3 TEXT,\n",
    "        field4 TEXT,\n",
    "        field5 TEXT,\n",
    "        FAMILY medium_read (field3, field4, field5),\n",
    "        \n",
    "        -- Family 3: Rarely accessed fields\n",
    "        field6 TEXT,\n",
    "        field7 TEXT,\n",
    "        field8 TEXT,\n",
    "        field9 TEXT,\n",
    "        FAMILY rarely_read (field6, field7, field8, field9)\n",
    "    )\n",
    "    \"\"\"\n",
    "    family_info = \"3 column families (frequently_read, medium_read, rarely_read)\"\n",
    "else:\n",
    "    # Create table with SINGLE column family (default)\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {source_table} (\n",
    "        ycsb_key INT PRIMARY KEY,\n",
    "        field0 TEXT,\n",
    "        field1 TEXT,\n",
    "        field2 TEXT,\n",
    "        field3 TEXT,\n",
    "        field4 TEXT,\n",
    "        field5 TEXT,\n",
    "        field6 TEXT,\n",
    "        field7 TEXT,\n",
    "        field8 TEXT,\n",
    "        field9 TEXT\n",
    "    )\n",
    "    \"\"\"\n",
    "    family_info = \"1 column family (default primary)\"\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(create_table_sql)\n",
    "        conn.commit()\n",
    "    print(f\"‚úÖ Table '{source_table}' created (or already exists)\")\n",
    "    print(f\"   Column Family Mode: {column_family_mode}\")\n",
    "    print(f\"   Column families: {family_info}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    # Check if table is empty using helper function\n",
    "    stats = get_table_stats(conn, source_table)\n",
    "    \n",
    "    if stats['is_empty']:\n",
    "        # Table is empty - insert snapshot data\n",
    "        print(f\"üìä Table is empty. Inserting {snapshot_count} initial rows (snapshot phase)...\")\n",
    "        \n",
    "        with conn.cursor() as cur:\n",
    "            # Use generate_series for efficient bulk insert\n",
    "            insert_sql = f\"\"\"\n",
    "            INSERT INTO {source_table} \n",
    "            (ycsb_key, field0, field1, field2, field3, field4, field5, field6, field7, field8, field9)\n",
    "            SELECT \n",
    "                i AS ycsb_key,\n",
    "                'snapshot_value_' || i || '_0' AS field0,\n",
    "                'snapshot_value_' || i || '_1' AS field1,\n",
    "                'snapshot_value_' || i || '_2' AS field2,\n",
    "                'snapshot_value_' || i || '_3' AS field3,\n",
    "                'snapshot_value_' || i || '_4' AS field4,\n",
    "                'snapshot_value_' || i || '_5' AS field5,\n",
    "                'snapshot_value_' || i || '_6' AS field6,\n",
    "                'snapshot_value_' || i || '_7' AS field7,\n",
    "                'snapshot_value_' || i || '_8' AS field8,\n",
    "                'snapshot_value_' || i || '_9' AS field9\n",
    "            FROM generate_series(0, %s - 1) AS i\n",
    "            \"\"\"\n",
    "            \n",
    "            cur.execute(insert_sql, (snapshot_count,))\n",
    "            conn.commit()\n",
    "        \n",
    "        print(f\"‚úÖ Sample data inserted using generate_series\")\n",
    "        print(f\"   Rows inserted: {snapshot_count} (keys 0 to {snapshot_count - 1})\")\n",
    "    else:\n",
    "        # Table already has data - skip insert\n",
    "        print(f\"‚ÑπÔ∏è  Table already contains data - skipping snapshot insert\")\n",
    "        print(f\"   Current key range: {stats['min_key']} to {stats['max_key']}\")\n",
    "        print(f\"   Tip: If you want to re-run the snapshot, drop the table first (see Cleanup cells)\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_changefeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Azure Blob Storage URI with table-specific path\n",
    "# Note: For Azure, path goes in URI (not as path_prefix query parameter like S3)\n",
    "path = f\"parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
    "changefeed_path = f\"azure://{container_name}/{path}?AZURE_ACCOUNT_NAME={storage_account_name}&AZURE_ACCOUNT_KEY={storage_account_key_encoded}\"\n",
    "\n",
    "# Build changefeed options based on column_family_mode\n",
    "if column_family_mode == \"multi_cf\":\n",
    "    # Include split_column_families for multi-family mode\n",
    "    changefeed_options = \"\"\"\n",
    "    format='parquet',\n",
    "    updated,\n",
    "    resolved='10s',\n",
    "    split_column_families\n",
    "\"\"\"\n",
    "else:\n",
    "    # Standard options for single-family mode\n",
    "    changefeed_options = \"\"\"\n",
    "    format='parquet',\n",
    "    updated,\n",
    "    resolved='10s'\n",
    "\"\"\"\n",
    "\n",
    "# Create changefeed SQL\n",
    "create_changefeed_sql = f\"\"\"\n",
    "CREATE CHANGEFEED FOR TABLE {source_table}\n",
    "INTO '{changefeed_path}'\n",
    "WITH {changefeed_options}\n",
    "\"\"\"\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        # Check for existing changefeed with THIS specific destination path\n",
    "        # (checks for source table AND full path to ensure uniqueness)\n",
    "        path_pattern = f\"%{source_table}%{source_catalog}/{source_schema}/{source_table}/{target_table}%\"\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "            SELECT job_id, status, description\n",
    "            FROM [SHOW CHANGEFEED JOBS] \n",
    "            WHERE description LIKE %s\n",
    "            AND status IN ('running', 'paused')\n",
    "            LIMIT 1\n",
    "        \"\"\", (path_pattern,))\n",
    "        \n",
    "        existing = cur.fetchone()\n",
    "        \n",
    "        if existing:\n",
    "            job_id, status, description = existing\n",
    "            print(f\"‚úÖ Changefeed already exists for this source ‚Üí target mapping\")\n",
    "            print(f\"   Job ID: {job_id}\")\n",
    "            print(f\"   Status: {status}\")\n",
    "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
    "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
    "            if column_family_mode == \"multi_cf\":\n",
    "                print(f\"   Expected: Column family fragments\")\n",
    "            print(f\"\")\n",
    "            print(f\"üí° Tip: Run Cell 10 to generate UPDATE/DELETE events\")\n",
    "            print(f\"   Then check Cell 11 to verify new files appear\")\n",
    "        else:\n",
    "            # Create new changefeed\n",
    "            cur.execute(create_changefeed_sql)\n",
    "            result = cur.fetchone()\n",
    "            job_id = result[0]\n",
    "            \n",
    "            print(f\"‚úÖ Changefeed created\")\n",
    "            print(f\"   Job ID: {job_id}\")\n",
    "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
    "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
    "            print(f\"   Format: Parquet\")\n",
    "            if column_family_mode == \"multi_cf\":\n",
    "                print(f\"   Split column families: TRUE (fragments will be generated)\")\n",
    "            else:\n",
    "                print(f\"   Split column families: FALSE (single file per event)\")\n",
    "            print(f\"   Destination: Azure Blob Storage\")\n",
    "            print(f\"\")\n",
    "            \n",
    "            # Wait for files to appear using helper function\n",
    "            wait_for_changefeed_files(\n",
    "                storage_account_name, storage_account_key, container_name,\n",
    "                source_catalog, source_schema, source_table, target_table,\n",
    "                max_wait=300, check_interval=5\n",
    "            )\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_workload",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Capture baseline file count BEFORE generating CDC events\n",
    "print(\"üìä Capturing baseline file count...\")\n",
    "result_before = check_azure_files(\n",
    "    storage_account_name, storage_account_key, container_name,\n",
    "    source_catalog, source_schema, source_table, target_table,\n",
    "    verbose=False\n",
    ")\n",
    "files_before = len(result_before['data_files'])\n",
    "print(f\"   Current files: {files_before}\")\n",
    "print()\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    # Get current table state using helper function\n",
    "    stats_before = get_table_stats(conn, source_table)\n",
    "    min_key = stats_before['min_key']\n",
    "    max_key = stats_before['max_key']\n",
    "    count_before = stats_before['count']\n",
    "    \n",
    "    print(f\"üìä Current table state:\")\n",
    "    print(f\"   Min key: {min_key}, Max key: {max_key}, Total rows: {count_before}\")\n",
    "    print()\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        \n",
    "        # 1. INSERT: Add new rows starting from max_key + 1 (using generate_series)\n",
    "        print(f\"‚ûï Running {insert_count} INSERTs (keys {max_key + 1} to {max_key + insert_count})...\")\n",
    "        insert_sql = f\"\"\"\n",
    "        INSERT INTO {source_table} \n",
    "        (ycsb_key, field0, field1, field2, field3, field4, field5, field6, field7, field8, field9)\n",
    "        SELECT \n",
    "            i AS ycsb_key,\n",
    "            'inserted_value_' || i || '_0' AS field0,\n",
    "            'inserted_value_' || i || '_1' AS field1,\n",
    "            'inserted_value_' || i || '_2' AS field2,\n",
    "            'inserted_value_' || i || '_3' AS field3,\n",
    "            'inserted_value_' || i || '_4' AS field4,\n",
    "            'inserted_value_' || i || '_5' AS field5,\n",
    "            'inserted_value_' || i || '_6' AS field6,\n",
    "            'inserted_value_' || i || '_7' AS field7,\n",
    "            'inserted_value_' || i || '_8' AS field8,\n",
    "            'inserted_value_' || i || '_9' AS field9\n",
    "        FROM generate_series(%s, %s) AS i\n",
    "        \"\"\"\n",
    "        cur.execute(insert_sql, (max_key + 1, max_key + insert_count))\n",
    "        \n",
    "        # 2. UPDATE: Update existing rows starting from min_key (single UPDATE statement)\n",
    "        print(f\"üìù Running {update_count} UPDATEs (keys {min_key} to {min_key + update_count - 1})...\")\n",
    "        timestamp = int(time.time())\n",
    "        cur.execute(f\"\"\"\n",
    "            UPDATE {source_table}\n",
    "            SET field0 = %s\n",
    "            WHERE ycsb_key >= %s AND ycsb_key < %s\n",
    "        \"\"\", (f\"updated_at_{timestamp}\", min_key, min_key + update_count))\n",
    "        \n",
    "        # 3. DELETE: Delete oldest rows starting from min_key (single DELETE)\n",
    "        delete_max = min_key + delete_count - 1\n",
    "        print(f\"üóëÔ∏è  Running {delete_count} DELETEs (keys {min_key} to {delete_max})...\")\n",
    "        cur.execute(f\"\"\"\n",
    "            DELETE FROM {source_table}\n",
    "            WHERE ycsb_key >= %s AND ycsb_key <= %s\n",
    "        \"\"\", (min_key, delete_max))\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    # Get final table state using helper function\n",
    "    stats_after = get_table_stats(conn, source_table)\n",
    "    min_key_after = stats_after['min_key']\n",
    "    max_key_after = stats_after['max_key']\n",
    "    count_after = stats_after['count']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Workload complete\")\n",
    "    print(f\"   Inserts: {insert_count}\")\n",
    "    print(f\"   Updates: {update_count}\")\n",
    "    print(f\"   Deletes: {delete_count}\")\n",
    "    print(f\"   Before: {count_before} rows (keys {min_key}-{max_key})\")\n",
    "    print(f\"   After:  {count_after} rows (keys {min_key_after}-{max_key_after})\")\n",
    "    print(f\"   Net change: {count_after - count_before:+d} rows\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    # Wait for new CDC files to appear in Azure (positive confirmation)\n",
    "    print(f\"‚è≥ Waiting for new CDC files to appear in Azure...\")\n",
    "    print(f\"   Baseline: {files_before} files\")\n",
    "    print()\n",
    "    \n",
    "    # Poll for new files (max 90 seconds)\n",
    "    max_wait = 90\n",
    "    check_interval = 10\n",
    "    elapsed = 0\n",
    "    \n",
    "    while elapsed < max_wait:\n",
    "        result = check_azure_files(\n",
    "            storage_account_name, storage_account_key, container_name,\n",
    "            source_catalog, source_schema, source_table, target_table,\n",
    "            verbose=False\n",
    "        )\n",
    "        files_now = len(result['data_files'])\n",
    "        \n",
    "        if files_now > files_before:\n",
    "            print(f\"‚úÖ New CDC files appeared after {elapsed} seconds!\")\n",
    "            print(f\"   Baseline (before workload): {files_before} files\")\n",
    "            print(f\"   Current (after workload): {files_now} files\")\n",
    "            print(f\"   New files generated: {files_now - files_before}\")\n",
    "            break\n",
    "        \n",
    "        print(f\"   Checking... ({elapsed}s elapsed, baseline: {files_before} files)\", end='\\r')\n",
    "        time.sleep(check_interval)\n",
    "        elapsed += check_interval\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - files may still be flushing\")\n",
    "        print(f\"   Run Cell 11 to check manually\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper function from Cell 4 to check for files\n",
    "result = check_azure_files(\n",
    "    storage_account_name, storage_account_key, container_name,\n",
    "    source_catalog, source_schema, source_table, target_table,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Provide guidance\n",
    "if len(result['data_files']) == 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  No data files found yet.\")\n",
    "    print(f\"   üí° Possible reasons:\")\n",
    "    print(f\"   - Changefeed not created yet (run Cell 9)\")\n",
    "    print(f\"   - Path configuration mismatch (check Cell 1 variables)\")\n",
    "    print(f\"   - Azure credentials issue (check External Location)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Files are ready! Proceed to Cell 10 to read with Databricks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "read_cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and run CDC ingestion function based on both modes (from Cell 1)\n",
    "# Functions are defined in Cell 5\n",
    "\n",
    "print(f\"üî∑ CDC Configuration:\")\n",
    "print(f\"   Processing Mode: {cdc_mode}\")\n",
    "print(f\"   Column Family Mode: {column_family_mode}\")\n",
    "print()\n",
    "\n",
    "# Select function based on BOTH cdc_mode and column_family_mode\n",
    "if cdc_mode == \"append_only\" and column_family_mode == \"single_cf\":\n",
    "    print(f\"üìò Running: ingest_cdc_append_only_single_family()\")\n",
    "    print(f\"   - All CDC events will be stored as rows\")\n",
    "    print(f\"   - No column family merging needed\\n\")\n",
    "    \n",
    "    query = ingest_cdc_append_only_single_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        spark=spark\n",
    "    )\n",
    "\n",
    "elif cdc_mode == \"append_only\" and column_family_mode == \"multi_cf\":\n",
    "    print(f\"üìô Running: ingest_cdc_append_only_multi_family()\")\n",
    "    print(f\"   - All CDC events will be stored as rows\")\n",
    "    print(f\"   - Column family fragments will be merged\\n\")\n",
    "    \n",
    "    if not primary_key_columns:\n",
    "        raise ValueError(\"primary_key_columns required for multi_cf mode\")\n",
    "    \n",
    "    query = ingest_cdc_append_only_multi_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        primary_key_columns=primary_key_columns,\n",
    "        spark=spark\n",
    "    )\n",
    "\n",
    "elif cdc_mode == \"update_delete\" and column_family_mode == \"single_cf\":\n",
    "    print(f\"üìó Running: ingest_cdc_with_merge_single_family()\")\n",
    "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
    "    print(f\"   - No column family merging needed\\n\")\n",
    "    \n",
    "    if not primary_key_columns:\n",
    "        raise ValueError(\"primary_key_columns required for update_delete mode\")\n",
    "    \n",
    "    result = ingest_cdc_with_merge_single_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        primary_key_columns=primary_key_columns,\n",
    "        spark=spark\n",
    "    )\n",
    "    \n",
    "    query = result[\"query\"]\n",
    "\n",
    "elif cdc_mode == \"update_delete\" and column_family_mode == \"multi_cf\":\n",
    "    print(f\"üìï Running: ingest_cdc_with_merge_multi_family()\")\n",
    "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
    "    print(f\"   - Column family fragments will be merged\\n\")\n",
    "    \n",
    "    if not primary_key_columns:\n",
    "        raise ValueError(\"primary_key_columns required for update_delete + multi_cf mode\")\n",
    "    \n",
    "    result = ingest_cdc_with_merge_multi_family(\n",
    "        storage_account_name=storage_account_name,\n",
    "        container_name=container_name,\n",
    "        source_catalog=source_catalog,\n",
    "        source_schema=source_schema,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        primary_key_columns=primary_key_columns,\n",
    "        spark=spark\n",
    "    )\n",
    "    \n",
    "    query = result[\"query\"]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Invalid mode combination:\\n\"\n",
    "        f\"  cdc_mode='{cdc_mode}' (valid: 'append_only', 'update_delete')\\n\"\n",
    "        f\"  column_family_mode='{column_family_mode}' (valid: 'single_cf', 'multi_cf')\\n\"\n",
    "        f\"Change modes in Cell 1.\"\n",
    "    )\n",
    "\n",
    "# Wait for completion (if not already complete)\n",
    "if cdc_mode == \"append_only\":\n",
    "    query.awaitTermination()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"‚úÖ CDC INGESTION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"   Mode: {cdc_mode} + {column_family_mode}\")\n",
    "    print(f\"   Target: {target_catalog}.{target_schema}.{target_table}\")\n",
    "    print()\n",
    "    print(f\"üìä Query your data: SELECT * FROM {target_catalog}.{target_schema}.{target_table}\")\n",
    "else:\n",
    "    # update_delete mode already completed inside the function\n",
    "    print(f\"üìä Query your data: SELECT * FROM {target_catalog}.{target_schema}.{target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "\n",
    "# Get total count\n",
    "df = spark.read.table(target_table_fqn)\n",
    "total_count = df.count()\n",
    "\n",
    "print(\"üìä CDC Event Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows: {total_count}\")\n",
    "print(f\"CDC Processing Mode: {cdc_mode}\")\n",
    "print(f\"Column Family Mode: {column_family_mode}\")\n",
    "print()\n",
    "\n",
    "# Show operation breakdown (works for both modes now!)\n",
    "print(\"Rows by last CDC operation:\")\n",
    "ops_df = df.groupBy(\"_cdc_operation\").count().orderBy(\"_cdc_operation\")\n",
    "ops_df.show()\n",
    "\n",
    "print(\"\\nüìã Sample rows (showing first 5):\")\n",
    "df.select(\n",
    "    \"ycsb_key\", \n",
    "    \"field0\", \n",
    "    \"_cdc_operation\", \n",
    "    \"_cdc_timestamp\"\n",
    ").orderBy(\"_cdc_timestamp\").show(5, truncate=False)\n",
    "\n",
    "if cdc_mode == \"append_only\":\n",
    "    print(\"\\n‚úÖ CDC data successfully loaded (append_only mode)\")\n",
    "    print(\"   üìä All CDC events stored as rows\")\n",
    "    print(\"   üìä _cdc_operation shows: DELETE, UPSERT for each event\")\n",
    "    print(\"   üìä Row count = all events (including DELETEs and multiple UPDATEs)\")\n",
    "elif cdc_mode == \"update_delete\":\n",
    "    print(\"\\n‚úÖ CDC data successfully loaded (update_delete mode)\")\n",
    "    print(\"   üìä MERGE operations applied: DELETEs removed, UPDATEs applied, INSERTs added\")\n",
    "    print(\"   üìä _cdc_operation shows: UPSERT (last operation on each row)\")\n",
    "    print(\"   üìä Row count = current state (deduplicated)\")\n",
    "\n",
    "print(\"\\nüí° Key Takeaway:\")\n",
    "print(\"   - Using pathGlobFilter to exclude .RESOLVED files avoids DECIMAL errors\")\n",
    "print(\"   - _cdc_operation is preserved in both modes for monitoring\")\n",
    "print(\"\\nüìç Next: Run Cell 14 to verify source and target tables are in sync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Verifying source and target tables are in sync...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get source table stats (CockroachDB)\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    source_stats = get_table_stats(conn, source_table)\n",
    "    source_sum = get_column_sum(conn, source_table, 'ycsb_key')\n",
    "    \n",
    "    # Get target table stats (Databricks Delta)\n",
    "    target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "    target_df = spark.read.table(target_table_fqn)\n",
    "    \n",
    "    # Calculate target stats using Spark\n",
    "    from pyspark.sql import functions as F\n",
    "    target_stats_df = target_df.agg(\n",
    "        F.min(\"ycsb_key\").alias(\"min_key\"),\n",
    "        F.max(\"ycsb_key\").alias(\"max_key\"),\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    target_stats = {\n",
    "        'min_key': target_stats_df['min_key'],\n",
    "        'max_key': target_stats_df['max_key'],\n",
    "        'count': target_stats_df['count']\n",
    "    }\n",
    "    \n",
    "    # Calculate sum using Spark helper function\n",
    "    target_sum = get_column_sum_spark(target_df, 'ycsb_key')\n",
    "    \n",
    "    # Display comparison\n",
    "    print(f\"\\nüìä Source Table (CockroachDB): {source_catalog}.{source_schema}.{source_table}\")\n",
    "    print(f\"   Min key: {source_stats['min_key']}\")\n",
    "    print(f\"   Max key: {source_stats['max_key']}\")\n",
    "    print(f\"   Count:   {source_stats['count']}\")\n",
    "    print(f\"   Sum (ycsb_key): {source_sum}\")\n",
    "    \n",
    "    print(f\"\\nüìä Target Table (Databricks Delta): {target_table_fqn}\")\n",
    "    print(f\"   Min key: {target_stats['min_key']}\")\n",
    "    print(f\"   Max key: {target_stats['max_key']}\")\n",
    "    print(f\"   Count:   {target_stats['count']}\")\n",
    "    print(f\"   Sum (ycsb_key): {target_sum}\")\n",
    "    \n",
    "    # Verify all numeric columns (YCSB schema: field0-9)\n",
    "    print(\"\\nüìä Column Sums Comparison (All Fields):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    columns_to_verify = ['ycsb_key', 'field0', 'field1', 'field2', 'field3',\n",
    "                         'field4', 'field5', 'field6', 'field7', 'field8', 'field9']\n",
    "    \n",
    "    all_columns_match = True\n",
    "    for col in columns_to_verify:\n",
    "        try:\n",
    "            source_col_sum = get_column_sum(conn, source_table, col)\n",
    "            target_col_sum = get_column_sum_spark(target_df, col)\n",
    "            col_matches = source_col_sum == target_col_sum\n",
    "            match_icon = \"‚úÖ\" if col_matches else \"‚ùå\"\n",
    "            \n",
    "            # Format with commas for readability\n",
    "            source_str = f\"{source_col_sum:,}\" if source_col_sum else \"NULL\"\n",
    "            target_str = f\"{target_col_sum:,}\" if target_col_sum else \"NULL\"\n",
    "            \n",
    "            print(f\"{match_icon} {col:12s}: Source={source_str:>20s} | Target={target_str:>20s}\")\n",
    "            \n",
    "            if not col_matches:\n",
    "                all_columns_match = False\n",
    "                diff = (target_col_sum or 0) - (source_col_sum or 0)\n",
    "                print(f\"   ‚ö†Ô∏è  Difference: {diff:+,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  {col:12s}: Error calculating sum - {e}\")\n",
    "            all_columns_match = False\n",
    "    \n",
    "    if all_columns_match:\n",
    "        print(\"\\n‚úÖ All column sums match!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Some column sums do not match - check data integrity\")\n",
    "    \n",
    "finally:\n",
    "    # Always close the connection, even if there's an error\n",
    "    conn.close()\n",
    "\n",
    "# Check CDC sync status (mode-aware verification)\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"Mode: {cdc_mode.upper()}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare keys and counts\n",
    "min_key_matches = source_stats['min_key'] == target_stats['min_key']\n",
    "max_key_matches = source_stats['max_key'] == target_stats['max_key']\n",
    "count_matches = source_stats['count'] == target_stats['count']\n",
    "sum_matches = source_sum == target_sum\n",
    "\n",
    "if cdc_mode == \"append_only\":\n",
    "    # In append_only mode, max_key is the key indicator of sync\n",
    "    # Min key and count are expected to differ due to deletes/updates not being applied\n",
    "    \n",
    "    if max_key_matches:\n",
    "        print(\"‚úÖ CDC PIPELINE IS WORKING!\")\n",
    "        print(f\"   Max key matches: {source_stats['max_key']}\")\n",
    "        print(f\"   Sum matches: {source_sum}\")\n",
    "        print()\n",
    "        \n",
    "        if not min_key_matches or not count_matches:\n",
    "            print(\"üìã Append-Only Mode (Expected Differences):\")\n",
    "            if not min_key_matches:\n",
    "                print(f\"   ‚ÑπÔ∏è  Min key differs (Source={source_stats['min_key']}, Target={target_stats['min_key']})\")\n",
    "                print(f\"      ‚Üí This is EXPECTED: DELETE events are captured but not applied\")\n",
    "            if not count_matches:\n",
    "                print(f\"   ‚ÑπÔ∏è  Row count differs (Source={source_stats['count']}, Target={target_stats['count']})\")\n",
    "                print(f\"      ‚Üí This is EXPECTED: All CDC events (INSERT/UPDATE/DELETE) are stored as rows\")\n",
    "            print()\n",
    "            print(\"üí° To apply deletes and updates:\")\n",
    "            print(\"   - Change cdc_mode='update_delete' in Cell 1\")\n",
    "            print(\"   - Or manually deduplicate using SQL window functions\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  MAX KEY MISMATCH - CDC may be lagging\")\n",
    "        print(\"\\n   Key Statistics:\")\n",
    "        print(f\"   - Source max: {source_stats['max_key']}\")\n",
    "        print(f\"   - Target max: {target_stats['max_key']}\")\n",
    "        print(f\"   - Difference: {source_stats['max_key'] - target_stats['max_key']:+d}\")\n",
    "        \n",
    "        print(\"\\n   üí° Possible reasons:\")\n",
    "        print(\"   - Auto Loader hasn't picked up all files yet (re-run Cell 12)\")\n",
    "        print(\"   - MERGE logic issue (check Cell 12 output)\")\n",
    "        print(\"   - Run Example 4 in Debug Section (Cell 30) for full diagnosis\")\n",
    "\n",
    "elif cdc_mode == \"update_delete\":\n",
    "    # In update_delete mode, ALL stats should match exactly\n",
    "    # DELETE operations are applied, UPDATE operations modify existing rows\n",
    "    \n",
    "    if min_key_matches and max_key_matches and count_matches and sum_matches and all_columns_match:\n",
    "        print(\"‚úÖ CDC PIPELINE IS WORKING PERFECTLY! All statistics and column sums match.\")\n",
    "        print(\"   All statistics match:\")\n",
    "        print(f\"   ‚úÖ Min key: {source_stats['min_key']}\")\n",
    "        print(f\"   ‚úÖ Max key: {source_stats['max_key']}\")\n",
    "        print(f\"   ‚úÖ Count:   {source_stats['count']}\")\n",
    "        print(f\"   ‚úÖ Sum:     {source_sum}\")\n",
    "        print()\n",
    "        print(\"üìã Update-Delete Mode:\")\n",
    "        print(\"   ‚úÖ DELETE events are applied (rows removed)\")\n",
    "        print(\"   ‚úÖ UPDATE events are applied (rows modified)\")\n",
    "        print(\"   ‚úÖ INSERT events are applied (rows added)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  SYNC MISMATCH - Tables are out of sync\")\n",
    "        print(\"\\n   Key Statistics:\")\n",
    "        if not min_key_matches:\n",
    "            print(f\"   ‚ùå Min key: Source={source_stats['min_key']}, Target={target_stats['min_key']}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Min key: {source_stats['min_key']}\")\n",
    "        \n",
    "        if not max_key_matches:\n",
    "            print(f\"   ‚ùå Max key: Source={source_stats['max_key']}, Target={target_stats['max_key']}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Max key: {source_stats['max_key']}\")\n",
    "        \n",
    "        if not count_matches:\n",
    "            print(f\"   ‚ùå Count: Source={source_stats['count']}, Target={target_stats['count']}\")\n",
    "            print(f\"      Difference: {target_stats['count'] - source_stats['count']:+d} rows\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Count: {source_stats['count']}\")\n",
    "        \n",
    "        if not sum_matches:\n",
    "            print(f\"   ‚ùå Sum: Source={source_sum}, Target={target_sum}\")\n",
    "            print(f\"      Difference: {target_sum - source_sum:+d}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Sum: {source_sum}\")\n",
    "        \n",
    "        \n",
    "        if not all_columns_match:\n",
    "            print(f\"   ‚ùå Column sums: Some column sums do not match (see above)\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Column sums: All match\")\n",
    "        print(\"\\n   üí° Possible reasons:\")\n",
    "        print(\"   - Auto Loader hasn't picked up all files yet (re-run Cell 12)\")\n",
    "        print(\"   - MERGE logic issue (check Cell 12 output for errors)\")\n",
    "        print(\"   - DELETE rows stored as data (run Cell 16 to fix)\")\n",
    "        print(\"   - Run diagnostic cell (Cell 15) to inspect target table\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Unknown mode: {cdc_mode}\")\n",
    "    print(\"   Cannot verify sync status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8f6b5",
   "metadata": {},
   "source": [
    "## Optional: Cleanup\n",
    "\n",
    "Run the cells below if you want to clean up the test resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7250ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è  SAFETY STOP: Cleanup Section\n",
    "# This cell prevents accidental cleanup when running \"Run All\"\n",
    "# \n",
    "# To cleanup resources, manually run each cell below INDIVIDUALLY:\n",
    "#   - Cell 16: Cancel changefeed\n",
    "#   - Cell 17: Drop CockroachDB source table  \n",
    "#   - Cell 18: Drop Databricks target table & checkpoint\n",
    "#   - Cell 19: Clear Azure changefeed data (optional - use for complete reset)\n",
    "\n",
    "raise RuntimeError(\n",
    "    \"\\n\"\n",
    "    \"‚ö†Ô∏è  CLEANUP SAFETY STOP\\n\"\n",
    "    \"\\n\"\n",
    "    \"The cells below will DELETE your resources.\\n\"\n",
    "    \"Do NOT run all cells - run each cleanup cell individually.\\n\"\n",
    "    \"\\n\"\n",
    "    \"üí° TIP: If Cell 13 shows sync issues due to old data,\\n\"\n",
    "    \"   run Cell 19 to clear Azure changefeed data completely.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f8e97da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Changefeed 1145428509383163905 cancelled\n",
      "   Source: defaultdb.public.usertable_update_delete_multi_cf\n",
      "   Target path: .../usertable_update_delete_multi_cf/usertable_update_delete_multi_cf/\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP CELL 1: CANCEL CHANGEFEED\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        # Find changefeed job for THIS specific source ‚Üí target mapping\n",
    "        # (matches the same path pattern used in Cell 7)\n",
    "        path_pattern = f\"%{source_table}%{source_catalog}/{source_schema}/{source_table}/{target_table}%\"\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "            SELECT job_id \n",
    "            FROM [SHOW CHANGEFEED JOBS] \n",
    "            WHERE description LIKE %s\n",
    "            AND status IN ('running', 'paused')\n",
    "            LIMIT 1\n",
    "        \"\"\", (path_pattern,))\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        if result:\n",
    "            job_id = result[0]\n",
    "            cur.execute(f\"CANCEL JOB {job_id}\")\n",
    "            print(f\"‚úÖ Changefeed {job_id} cancelled\")\n",
    "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
    "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è  No active changefeed found for this source ‚Üí target mapping\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f35e74ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table 'usertable_update_delete_multi_cf' dropped from CockroachDB\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP CELL 2: DROP SOURCE TABLE (CockroachDB)\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f\"DROP TABLE IF EXISTS {source_table} CASCADE\")\n",
    "        conn.commit()\n",
    "    print(f\"‚úÖ Table '{source_table}' dropped from CockroachDB\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "988ed2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Deleting Azure changefeed data...\n",
      "================================================================================\n",
      "Container: changefeed-events\n",
      "Path: parquet/defaultdb/public/usertable_update_delete_multi_cf/usertable_update_delete_multi_cf/\n",
      "\n",
      "üîç Scanning for files...\n",
      "‚úÖ Found 761 items to delete\n",
      "   üìÑ Data files: 51\n",
      "   üïê Resolved files: 708\n",
      "   üìÅ Directories: 2\n",
      "\n",
      "üîÑ Deleting 761 items...\n",
      "‚úÖ Deleted 759 items from Azure                    \n",
      "\n",
      "================================================================================\n",
      "‚úÖ Cleanup complete!\n",
      "\n",
      "üí° Next steps:\n",
      "   1. Drop the Databricks target table (Cell 17)\n",
      "   2. Re-run from Cell 6 (Snapshot) to start fresh\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP CELL 19: CLEAR AZURE CHANGEFEED DATA (Optional)\n",
    "# ‚ö†Ô∏è  WARNING: This will DELETE all changefeed data in Azure for this table!\n",
    "#\n",
    "# Use this when:\n",
    "# - You want to start completely fresh\n",
    "# - Old data from previous runs is causing sync issues\n",
    "# - You changed the table schema (e.g., VARCHAR ‚Üí INT)\n",
    "#\n",
    "# Uses Azure SDK (same as Cell 11 for checking files)\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Build Azure path (must match Cell 7 changefeed path)\n",
    "changefeed_path = f\"parquet/{source_catalog}/{source_schema}/{source_table}/{target_table}/\"\n",
    "\n",
    "print(f\"üóëÔ∏è  Deleting Azure changefeed data...\")\n",
    "print(f\"=\" * 80)\n",
    "print(f\"Container: {container_name}\")\n",
    "print(f\"Path: {changefeed_path}\")\n",
    "print()\n",
    "\n",
    "# Connect to Azure (same as Cell 9)\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service.get_container_client(container_name)\n",
    "\n",
    "# List all blobs with this prefix\n",
    "print(f\"üîç Scanning for files...\")\n",
    "blobs = list(container_client.list_blobs(name_starts_with=changefeed_path))\n",
    "\n",
    "if not blobs:\n",
    "    print(f\"‚ÑπÔ∏è  No files found at: {changefeed_path}\")\n",
    "    print(f\"   Files may have already been deleted, or path is incorrect\")\n",
    "    print()\n",
    "    print(f\"üí° To check what's in the container, run Cell 9\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(blobs)} items to delete\")\n",
    "    \n",
    "    # Show sample items\n",
    "    data_files = [b for b in blobs if b.size > 0 and '.parquet' in b.name]\n",
    "    resolved_files = [b for b in blobs if '.RESOLVED' in b.name]\n",
    "    directories = [b for b in blobs if b.size == 0]\n",
    "    \n",
    "    print(f\"   üìÑ Data files: {len(data_files)}\")\n",
    "    print(f\"   üïê Resolved files: {len(resolved_files)}\")\n",
    "    print(f\"   üìÅ Directories: {len(directories)}\")\n",
    "    print()\n",
    "    \n",
    "    # Delete all blobs with this prefix\n",
    "    # Note: Azure SDK doesn't have recursive delete - we list all blobs and delete each one\n",
    "    print(f\"üîÑ Deleting {len(blobs)} items...\")\n",
    "    deleted = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for blob in blobs:\n",
    "        try:\n",
    "            container_client.delete_blob(blob.name)\n",
    "            deleted += 1\n",
    "            if deleted % 50 == 0:\n",
    "                print(f\"   Deleted {deleted}/{len(blobs)} items...\", end='\\r')\n",
    "        except Exception as e:\n",
    "            # Some errors are expected (e.g., directories already removed)\n",
    "            error_str = str(e)\n",
    "            if \"DirectoryIsNotEmpty\" not in error_str and \"BlobNotFound\" not in error_str:\n",
    "                failed += 1\n",
    "                print(f\"\\n   ‚ö†Ô∏è  Failed: {blob.name[:60]}... - {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Deleted {deleted} items from Azure                    \")\n",
    "    if failed > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Failed to delete {failed} items\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"‚úÖ Cleanup complete!\")\n",
    "    print()\n",
    "    print(f\"üí° Next steps:\")\n",
    "    print(f\"   1. Drop the Databricks target table (Cell 17)\")\n",
    "    print(f\"   2. Re-run from Cell 6 (Snapshot) to start fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "033d5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Delta table 'robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf' dropped\n",
      "‚úÖ Checkpoint '/checkpoints/robert_lee_cockroachdb_usertable_update_delete_multi_cf' removed\n",
      "\n",
      "‚úÖ Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP CELL 3: DROP TARGET TABLE & CHECKPOINT (Databricks)\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}\"  # Must match Cell 10\n",
    "\n",
    "# Drop Delta table\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
    "print(f\"‚úÖ Delta table '{target_table_fqn}' dropped\")\n",
    "\n",
    "# Remove checkpoint\n",
    "try:\n",
    "    dbutils.fs.rm(checkpoint_path, True)\n",
    "    print(f\"‚úÖ Checkpoint '{checkpoint_path}' removed\")\n",
    "except:\n",
    "    print(f\"‚ÑπÔ∏è  Checkpoint not found (may have been already removed)\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a53c9621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Dropping staging table: robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf_staging_cf\n",
      "üóëÔ∏è  Dropping target table: robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf\n",
      "üóëÔ∏è  Clearing checkpoint: /checkpoints/robert_lee_cockroachdb_usertable_update_delete_multi_cf_merge_cf\n",
      "   ‚úÖ Checkpoint cleared\n",
      "\n",
      "‚úÖ Cleanup complete! Ready for fresh start.\n",
      "   Next: Re-run Cell 12 (ingestion)\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP CELL 4: Complete cleanup for fresh start\n",
    "\n",
    "# 1. Drop staging table\n",
    "staging_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}_staging_cf\"\n",
    "print(f\"üóëÔ∏è  Dropping staging table: {staging_table_fqn}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {staging_table_fqn}\")\n",
    "\n",
    "# 2. Drop target table (if not already done)\n",
    "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
    "print(f\"üóëÔ∏è  Dropping target table: {target_table_fqn}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
    "\n",
    "# 3. Clear checkpoint location\n",
    "checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}_merge_cf\"\n",
    "print(f\"üóëÔ∏è  Clearing checkpoint: {checkpoint_path}\")\n",
    "try:\n",
    "    dbutils.fs.rm(checkpoint_path, recurse=True)\n",
    "    print(f\"   ‚úÖ Checkpoint cleared\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ÑπÔ∏è  Checkpoint may not exist: {e}\")\n",
    "\n",
    "# 4. Verify cleanup\n",
    "print(f\"\\n‚úÖ Cleanup complete! Ready for fresh start.\")\n",
    "print(f\"   Next: Re-run Cell 12 (ingestion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76a28f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Creating schema: robert_lee.robert_lee_cockroachdb\n",
      "‚úÖ Schema created\n",
      "‚úÖ Verified: Schema robert_lee_cockroachdb exists\n"
     ]
    }
   ],
   "source": [
    "# Recreate the schema\n",
    "print(f\"üìÅ Creating schema: {target_catalog}.{target_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n",
    "print(f\"‚úÖ Schema created\")\n",
    "\n",
    "# Verify schema exists\n",
    "schemas = spark.sql(f\"SHOW SCHEMAS IN {target_catalog}\").collect()\n",
    "schema_names = [row['databaseName'] for row in schemas]\n",
    "if target_schema in schema_names:\n",
    "    print(f\"‚úÖ Verified: Schema {target_schema} exists\")\n",
    "else:\n",
    "    print(f\"‚ùå Schema {target_schema} not found. Available schemas: {schema_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bdda3d",
   "metadata": {},
   "source": [
    "# Debug Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c46a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Debug functions available:\n",
      "   ‚Ä¢ quick_check_missing_keys() - Quick check for specific keys\n",
      "   ‚Ä¢ inspect_target_table() - Comprehensive target table analysis\n",
      "   ‚Ä¢ detailed_missing_keys_investigation() - Detailed key investigation\n",
      "\n",
      "See the following cells for usage examples.\n"
     ]
    }
   ],
   "source": [
    "# Debug helper functions are available in cockroachdb_debug.py\n",
    "# Import them in Cell 15, then use the examples below:\n",
    "\n",
    "print(\"üí° Debug functions available:\")\n",
    "print(\"   ‚Ä¢ quick_check_missing_keys() - Quick check for specific keys\")\n",
    "print(\"   ‚Ä¢ inspect_target_table() - Comprehensive target table analysis\")\n",
    "print(\"   ‚Ä¢ detailed_missing_keys_investigation() - Detailed key investigation\")\n",
    "print(\"\\nSee the following cells for usage examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a71eb847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Quick Debug: Checking missing keys...\n",
      "================================================================================\n",
      "Keys to check: [17, 18, 19]\n",
      "\n",
      "üìä CockroachDB (usertable_update_delete_multi_cf):\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "{'S': 'ERROR', 'V': 'ERROR', 'C': '42P01', 'F': 'errors.go', 'L': '233', 'R': 'NewUndefinedRelationError', 'M': 'relation \"usertable_update_delete_multi_cf\" does not exist'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/lakeflow-community-connectors/.venv_311_dogfood/lib/python3.11/site-packages/pg8000/legacy.py:256\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, operation, args, stream)\u001b[39m\n\u001b[32m    255\u001b[39m     statement, vals = convert_paramstyle(\u001b[38;5;28mself\u001b[39m.paramstyle, operation, args)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28mself\u001b[39m._context = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_c\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_unnamed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_oids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m rows = [] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._context.rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._context.rows\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/lakeflow-community-connectors/.venv_311_dogfood/lib/python3.11/site-packages/pg8000/core.py:704\u001b[39m, in \u001b[36mCoreConnection.execute_unnamed\u001b[39m\u001b[34m(self, statement, vals, oids, stream)\u001b[39m\n\u001b[32m    703\u001b[39m _flush(\u001b[38;5;28mself\u001b[39m._sock)\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;28mself\u001b[39m.send_DESCRIBE_STATEMENT(NULL_BYTE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/lakeflow-community-connectors/.venv_311_dogfood/lib/python3.11/site-packages/pg8000/core.py:844\u001b[39m, in \u001b[36mCoreConnection.handle_messages\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m context.error\n",
      "\u001b[31mDatabaseError\u001b[39m: {'S': 'ERROR', 'V': 'ERROR', 'C': '42P01', 'F': 'errors.go', 'L': '233', 'R': 'NewUndefinedRelationError', 'M': 'relation \"usertable_update_delete_multi_cf\" does not exist'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m conn = get_cockroachdb_connection()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mquick_check_missing_keys\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_table\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_catalog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_catalog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_table\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m19\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ‚Üê Update based on Cell 14 output\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     16\u001b[39m     conn.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/lakeflow-community-connectors/sources/cockroachdb/docs/cockroachdb_debug.py:896\u001b[39m, in \u001b[36mquick_check_missing_keys\u001b[39m\u001b[34m(conn, spark, source_table, target_catalog, target_schema, target_table, missing_keys)\u001b[39m\n\u001b[32m    894\u001b[39m cursor = conn.cursor()\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m missing_keys:\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msource_table\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m WHERE ycsb_key = %s\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    897\u001b[39m     result = cursor.fetchone()\n\u001b[32m    898\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m‚úÖ EXISTS\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mresult\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m‚ùå NOT FOUND (deleted)\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/lakeflow-community-connectors/.venv_311_dogfood/lib/python3.11/site-packages/pg8000/legacy.py:283\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, operation, args, stream)\u001b[39m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    281\u001b[39m         \u001b[38;5;28mcls\u001b[39m = ProgrammingError\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(msg)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(msg)\n",
      "\u001b[31mProgrammingError\u001b[39m: {'S': 'ERROR', 'V': 'ERROR', 'C': '42P01', 'F': 'errors.go', 'L': '233', 'R': 'NewUndefinedRelationError', 'M': 'relation \"usertable_update_delete_multi_cf\" does not exist'}"
     ]
    }
   ],
   "source": [
    "# Example 1: Quick check for specific missing keys\n",
    "from cockroachdb_debug import quick_check_missing_keys\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    quick_check_missing_keys(\n",
    "        conn=conn,\n",
    "        spark=spark,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        missing_keys=[17, 18, 19]  # ‚Üê Update based on Cell 14 output\n",
    "    )\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70044870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Comprehensive target table analysis\n",
    "from cockroachdb_debug import inspect_target_table\n",
    "\n",
    "inspect_target_table(\n",
    "    spark=spark,\n",
    "    target_catalog=target_catalog,\n",
    "    target_schema=target_schema,\n",
    "    target_table=target_table\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Detailed investigation of specific missing keys\n",
    "from cockroachdb_debug import detailed_missing_keys_investigation\n",
    "\n",
    "conn = get_cockroachdb_connection()\n",
    "try:\n",
    "    detailed_missing_keys_investigation(\n",
    "        conn=conn,\n",
    "        spark=spark,\n",
    "        source_table=source_table,\n",
    "        target_catalog=target_catalog,\n",
    "        target_schema=target_schema,\n",
    "        target_table=target_table,\n",
    "        missing_keys=[17, 18, 19]  # ‚Üê Update based on inspect_target_table output\n",
    "    )\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Full diagnosis using config (Recommended for Cell 14 sync issues)\n",
    "import importlib,cockroachdb_debug\n",
    "importlib.reload(cockroachdb_debug)\n",
    "from cockroachdb_debug import run_full_diagnosis_from_config\n",
    "\n",
    "# Define mismatched columns from Cell 14 output\n",
    "mismatched_columns = ['field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9']\n",
    "\n",
    "# Run full diagnosis using config from Cell 3\n",
    "# This will:\n",
    "#   - Refresh target DataFrame\n",
    "#   - Establish CockroachDB connection\n",
    "#   - Run comprehensive diagnosis\n",
    "#   - Check staging table, Azure files, and row-by-row comparison\n",
    "#   - Provide detailed troubleshooting recommendations\n",
    "run_full_diagnosis_from_config(\n",
    "    spark=spark,\n",
    "    config=config,\n",
    "    mismatched_columns=mismatched_columns  # Set to None if no mismatches\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_311_dogfood (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
