{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Stream CockroachDB CDC to Databricks (Azure)\n",
        "\n",
        "This notebook demonstrates how to stream CockroachDB changefeeds to Databricks using Azure Blob Storage.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- CockroachDB cluster (Cloud or self-hosted)\n",
        "- Azure Storage Account with hierarchical namespace enabled\n",
        "- Databricks workspace with Unity Catalog\n",
        "- Unity Catalog External Location configured for your storage account\n",
        "\n",
        "**Note:** This notebook uses the **YCSB (Yahoo! Cloud Serving Benchmark)** schema as the default table structure, with `ycsb_key` as the primary key and `field0-9` columns. The default schema name is `public`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c81271",
      "metadata": {},
      "source": [
        "## CDC Mode Selection\n",
        "\n",
        "This notebook supports **4 CDC ingestion modes** by combining two independent settings:\n",
        "\n",
        "### 1. CDC Processing Mode (`cdc_mode`)\n",
        "How CDC events are processed in the target table:\n",
        "\n",
        "- **`append_only`**: Store all CDC events as rows (audit log)\n",
        "  - **Behavior**: All events (INSERT/UPDATE/DELETE) are appended as new rows\n",
        "  - **Use case**: History tracking, time-series analysis, audit logs\n",
        "  - **Storage**: Higher (keeps all historical events)\n",
        "\n",
        "- **`update_delete`**: Apply MERGE logic (current state replication)\n",
        "  - **Behavior**: DELETE removes rows, UPDATE modifies rows in-place\n",
        "  - **Use case**: Current state synchronization, production replication\n",
        "  - **Storage**: Lower (only latest state per key)\n",
        "\n",
        "### 2. Column Family Mode (`column_family_mode`)\n",
        "Table structure and changefeed configuration:\n",
        "\n",
        "- **`single_cf`**: Standard table (1 column family, default)\n",
        "  - **Changefeed**: `split_column_families=false`\n",
        "  - **Files**: 1 Parquet file per CDC event\n",
        "  - **Use case**: Most tables, simpler configuration, better performance\n",
        "\n",
        "- **`multi_cf`**: Multiple column families (for wide tables)\n",
        "  - **Changefeed**: `split_column_families=true`\n",
        "  - **Files**: Multiple Parquet files per CDC event (fragments need merging)\n",
        "  - **Use case**: Wide tables (50+ columns), selective column access patterns\n",
        "\n",
        "### Function Selection Matrix\n",
        "\n",
        "The notebook automatically selects the appropriate ingestion function based on your configuration:\n",
        "\n",
        "| CDC Mode | Column Family Mode | Function Called |\n",
        "|----------|-------------------|-----------------|\n",
        "| `append_only` | `single_cf` | `ingest_cdc_append_only_single_family()` |\n",
        "| `append_only` | `multi_cf` | `ingest_cdc_append_only_multi_family()` |\n",
        "| `update_delete` | `single_cf` | `ingest_cdc_with_merge_single_family()` |\n",
        "| `update_delete` | `multi_cf` | `ingest_cdc_with_merge_multi_family()` |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "config",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration loaded from: ../.env/cockroachdb_cdc_tutorial_config_update_delete_multi_cf.json\n",
            "‚úÖ Configuration loaded\n",
            "   Data Source: uc_external_volume\n",
            "   UC Volume: robert_lee.robert_lee_cockroachdb.cockroachdb_cdc_1768934658\n",
            "   CDC Processing Mode: update_delete\n",
            "   Column Family Mode: multi_cf\n",
            "   Primary Keys: ['ycsb_key']\n",
            "   Target Table: usertable_update_delete_multi_cf\n",
            "   CDC Workload: 10 snapshot ‚Üí +10 INSERTs, ~9 UPDATEs, -8 DELETEs\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from urllib.parse import quote\n",
        "\n",
        "# Configuration file config.cdc_config.path (adjust as needed)\n",
        "#config_file = \"../.env/cockroachdb_cdc_tutorial_config_append_single_cf.json\"\n",
        "\n",
        "#config_file = \"../.env/cockroachdb_cdc_tutorial_config_append_multi_cf.json\"\n",
        "\n",
        "config_file = \"../.env/cockroachdb_cdc_tutorial_config_update_delete_multi_cf.json\"\n",
        "\n",
        "#config_file = \"../.env/cockroachdb_cdc_tutorial_config_update_delete_single_cf.json\"\n",
        "\n",
        "\n",
        "import importlib\n",
        "import cockroachdb_config\n",
        "importlib.reload(cockroachdb_config)\n",
        "from cockroachdb_config import load_config, process_config\n",
        "\n",
        "# Try to load from file, fallback to embedded config\n",
        "config = load_config(config_file)\n",
        "\n",
        "# Embedded configuration (fallback)\n",
        "if config is None:\n",
        "    config = {\n",
        "      \"cockroachdb\": {\n",
        "        \"host\": \"replace_me\",\n",
        "        \"port\": 26257,\n",
        "        \"user\": \"replace_me\",\n",
        "        \"password\": \"replace_me\",\n",
        "        \"database\": \"defaultdb\"\n",
        "      },\n",
        "      \"cockroachdb_source\": {\n",
        "        \"catalog\": \"defaultdb\",\n",
        "        \"schema\": \"public\",\n",
        "        \"table_name\": \"usertable\",\n",
        "        \"_schema_note\": \"Default schema is 'public'. Table uses YCSB structure (ycsb_key, field0-9)\",\n",
        "      },\n",
        "      \"azure_storage\": {\n",
        "        \"account_name\": \"replace_me\",\n",
        "        \"account_key\": \"replace_me\",\n",
        "        \"config.azure_storage.container_name\": \"changefeed-events\"\n",
        "      },\n",
        "      \"databricks_target\": {\n",
        "        \"catalog\": \"main\",\n",
        "        \"schema\": \"replace_me\",\n",
        "        \"table_name\": \"usertable\",\n",
        "      },\n",
        "      \"cdc_config\": {\n",
        "        \"mode\": \"append_only\",\n",
        "        \"config.cdc_config.column_family_mode\": \"multi_cf\",\n",
        "        \"config.cdc_config.primary_key_columns\": [\"ycsb_key\"],\n",
        "        \"auto_suffix_mode_family\": True,\n",
        "      },\n",
        "      \"workload_config\": {\n",
        "        \"config.workload_config.snapshot_count\": 10,\n",
        "        \"config.workload_config.insert_count\": 10,\n",
        "        \"config.workload_config.update_count\": 9,\n",
        "        \"config.workload_config.delete_count\": 8,\n",
        "      }\n",
        "    }\n",
        "config=process_config(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "install",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "%pip install pg8000 azure-storage-blob --quiet\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "connect",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Connected to CockroachDB\n",
            "   Version: CockroachDB CCL v25.4.4 (x86_64-pc-linux-gnu, buil...\n",
            "‚úÖ Connection function ready for use\n"
          ]
        }
      ],
      "source": [
        "# Import CockroachDB connection utilities\n",
        "import importlib\n",
        "import cockroachdb_conn\n",
        "importlib.reload(cockroachdb_conn)\n",
        "from cockroachdb_conn import get_cockroachdb_connection\n",
        "\n",
        "# Test connection to CockroachDB\n",
        "# The function automatically tests the connection (test=True by default)\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=True  # Automatically tests connection and prints version (default)\n",
        ")\n",
        "conn.close()\n",
        "\n",
        "print(\"‚úÖ Connection function ready for use\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "64c40c9d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper functions loaded (CockroachDB & Azure)\n",
            "‚úÖ YCSB utility functions imported from cockroachdb_ycsb.py\n"
          ]
        }
      ],
      "source": [
        "# Import storage utilities (works with both Azure and UC Volume)\n",
        "import importlib\n",
        "import cockroachdb_storage\n",
        "importlib.reload(cockroachdb_storage)\n",
        "from cockroachdb_storage import check_files, wait_for_files\n",
        "\n",
        "# Import YCSB utility functions\n",
        "import cockroachdb_ycsb\n",
        "importlib.reload(cockroachdb_ycsb)\n",
        "from cockroachdb_ycsb import (\n",
        "    get_table_stats,\n",
        "    get_table_stats_spark,\n",
        "    get_column_sum,\n",
        "    get_column_sum_spark,\n",
        "    deduplicate_to_latest,\n",
        "    get_column_sum_spark_deduplicated\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded (CockroachDB & Azure)\")\n",
        "print(\"‚úÖ YCSB utility functions imported from cockroachdb_ycsb.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "create_table",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Table 'usertable_update_delete_multi_cf' created (or already exists)\n",
            "   Column Family Mode: multi_cf\n",
            "   Column families: 3 column families (frequently_read, medium_read, rarely_read)\n"
          ]
        }
      ],
      "source": [
        "# Create table using cockroachdb_ycsb.py\n",
        "# Import YCSB functions\n",
        "import importlib, cockroachdb_ycsb\n",
        "importlib.reload(cockroachdb_ycsb)\n",
        "from cockroachdb_ycsb import create_ycsb_table\n",
        "\n",
        "# Create table\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=False  # Skip test, connection already validated\n",
        ")\n",
        "try:\n",
        "    create_ycsb_table(\n",
        "        conn=conn,\n",
        "        table_name=config.tables.source_table_name,\n",
        "        column_family_mode=config.cdc_config.column_family_mode\n",
        "    )\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "insert_data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è  Table already contains data - skipping snapshot insert\n",
            "   Current key range: 216 to 279\n",
            "   Tip: If you want to re-run the snapshot, drop the table first\n"
          ]
        }
      ],
      "source": [
        "# Insert snapshot data with NULL testing using cockroachdb_ycsb.py\n",
        "from cockroachdb_ycsb import insert_ycsb_snapshot_with_random_nulls\n",
        "\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=False  # Skip test, connection already validated\n",
        ")\n",
        "try:\n",
        "    insert_ycsb_snapshot_with_random_nulls(\n",
        "        conn=conn,\n",
        "        table_name=config.tables.source_table_name,\n",
        "        snapshot_count=config.workload_config.snapshot_count,\n",
        "        null_probability=0.3,  # 30% chance of NULL in snapshot\n",
        "        columns_to_randomize=['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9'],  # ALL fields\n",
        "        seed=42,  # Reproducible random NULLs\n",
        "        force_all_null_row=True  # Row 0 will have all randomized columns as NULL (edge case testing)\n",
        "    )\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "create_changefeed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "connection is closed\n"
          ]
        }
      ],
      "source": [
        "from cockroachdb_sql import create_changefeed_from_config\n",
        "\n",
        "try:\n",
        "    result = create_changefeed_from_config(conn, config, spark)\n",
        "    \n",
        "    if result['created']:\n",
        "        print(f\"New changefeed: Job {result['job_id']}\")\n",
        "    else:\n",
        "        print(f\"Using existing: {result['existing_count']} found\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "run_workload",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Capturing baseline file count...\n",
            "   Data files: 84\n",
            "   Resolved files: 1449\n",
            "\n",
            "üìä Current table state:\n",
            "   Min key: 216, Max key: 279, Total rows: 64\n",
            "\n",
            "‚ûï Running 10 INSERTs (keys 280 to 289)...\n",
            "üìù Running 9 UPDATEs with random NULLs...\n",
            "   NULL probability: 50.0%\n",
            "   Columns to randomize: field0, field1, field2, field3, field4, field5, field6, field7, field8, field9\n",
            "   ‚ö†Ô∏è  First updated row (key 216) will have ALL randomized columns as NULL\n",
            "üóëÔ∏è  Running 8 DELETEs (keys 216 to 223)...\n",
            "\n",
            "‚úÖ Workload complete\n",
            "   Inserts: 10\n",
            "   Updates: 9 (with random NULLs)\n",
            "   Deletes: 8\n",
            "   Before: 64 rows (keys 216-279)\n",
            "   After:  66 rows (keys 224-289)\n",
            "   Net change: +2 rows\n",
            "\n",
            "‚è≥ Waiting for new CDC files to appear in Unity Catalog Volume...\n",
            "   Baseline: 84 data files, 1449 resolved files\n",
            "\n",
            "‚úÖ New CDC files appeared after 0 seconds!\n",
            "   Data files: 84 ‚Üí 84 (+0)\n",
            "   Resolved files: 1449 ‚Üí 1450 (+1)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Capture baseline file count BEFORE generating CDC events\n",
        "print(\"üìä Capturing baseline file count...\")\n",
        "result_before = check_files(\n",
        "    config=config,\n",
        "    spark=spark,\n",
        "    verbose=False\n",
        ")\n",
        "files_before = len(result_before['data_files'])\n",
        "resolved_before = len(result_before['resolved_files'])\n",
        "print(f\"   Data files: {files_before}\")\n",
        "print(f\"   Resolved files: {resolved_before}\")\n",
        "print()\n",
        "\n",
        "# Run workload with NULL testing using cockroachdb_ycsb.py\n",
        "from cockroachdb_ycsb import run_ycsb_workload_with_random_nulls\n",
        "\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=False  # Skip test, connection already validated\n",
        ")\n",
        "try:\n",
        "    run_ycsb_workload_with_random_nulls(\n",
        "        conn=conn,\n",
        "        table_name=config.tables.source_table_name,\n",
        "        insert_count=config.workload_config.insert_count,\n",
        "        update_count=config.workload_config.update_count,\n",
        "        delete_count=config.workload_config.delete_count,\n",
        "        null_probability=0.5,  # 50% chance of NULL in UPDATEs\n",
        "        columns_to_randomize=['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9'],  # ALL fields\n",
        "        seed=42,  # Reproducible random NULLs\n",
        "        force_all_null_update=True  # First UPDATE will have all NULLs (edge case testing)\n",
        "    )\n",
        "finally:\n",
        "    conn.close()\n",
        "\n",
        "# Wait for new CDC files to appear in Azure (positive confirmation)\n",
        "print(f\"\")\n",
        "storage_label = \"Unity Catalog Volume\" if config.data_source == \"uc_external_volume\" else \"Azure\"\n",
        "print(f\"‚è≥ Waiting for new CDC files to appear in {storage_label}...\")\n",
        "print(f\"   Baseline: {files_before} data files, {resolved_before} resolved files\")\n",
        "print()\n",
        "\n",
        "# Poll for new files (max 90 seconds)\n",
        "max_wait = 90\n",
        "check_interval = 10\n",
        "elapsed = 0\n",
        "\n",
        "while elapsed < max_wait:\n",
        "    result = check_files(\n",
        "        config=config,\n",
        "        spark=spark,\n",
        "        verbose=False\n",
        "    )\n",
        "    files_now = len(result['data_files'])\n",
        "    resolved_now = len(result['resolved_files'])\n",
        "    \n",
        "    if files_now > files_before or resolved_now > resolved_before:\n",
        "        print(f\"‚úÖ New CDC files appeared after {elapsed} seconds!\")\n",
        "        print(f\"   Data files: {files_before} ‚Üí {files_now} (+{files_now - files_before})\")\n",
        "        print(f\"   Resolved files: {resolved_before} ‚Üí {resolved_now} (+{resolved_now - resolved_before})\")\n",
        "        break\n",
        "    \n",
        "    print(f\"   Checking... ({elapsed}s elapsed, baseline: {files_before} data, {resolved_before} resolved)\", end='\\r')\n",
        "    time.sleep(check_interval)\n",
        "    elapsed += check_interval\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - files may still be flushing\")\n",
        "    print(f\"   Run Cell 11 to check manually\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "check_files",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üöÄ Using Spark for fast parallel file listing...\n",
            "   ‚è≥ Spark: Reading directory structure...\n",
            "   ‚è≥ Spark: Collecting file paths...\n",
            "   ‚úÖ Spark: Found 1534 total paths\n",
            "üìÅ Files in Unity Catalog Volume:\n",
            "   Path: /Volumes/robert_lee/robert_lee_cockroachdb/cockroachdb_cdc_1768934658/parquet/defaultdb/public/usertable_update_delete_multi_cf/usertable_update_delete_multi_cf/\n",
            "   üìÑ Data files: 84\n",
            "   üïê Resolved files: 1450\n",
            "   üìä Total: 1534\n",
            "\n",
            "   Example data file:\n",
            "   202602022023010000000000000000001-04678f99961a1b57-2-15-0000000b-usertable_update_delete_multi_cf+rarely_read-1.parquet\n",
            "\n",
            "‚úÖ Files are ready! Proceed to Cell 10 to read with Databricks.\n"
          ]
        }
      ],
      "source": [
        "# Use the unified storage function to check for files (works with both Azure and UC Volume)\n",
        "result = check_files(\n",
        "    config=config,\n",
        "    spark=spark,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Provide guidance\n",
        "if len(result['data_files']) == 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  No data files found yet.\")\n",
        "    print(f\"   üí° Possible reasons:\")\n",
        "    print(f\"   - Changefeed not created yet (run Cell 9)\")\n",
        "    print(f\"   - Path configuration mismatch (check Cell 1 variables)\")\n",
        "    print(f\"   - Azure credentials issue (check External Location)\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Files are ready! Proceed to Cell 10 to read with Databricks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "read_cdc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî∑ CDC Configuration:\n",
            "   Processing Mode: update_delete\n",
            "   Column Family Mode: multi_cf\n",
            "   Data Source: uc_external_volume\n",
            "\n",
            "üìï Running: ingest_cdc_with_merge_multi_family()\n",
            "   - MERGE logic applied (UPDATE/DELETE processed)\n",
            "   - Column family fragments will be merged\n",
            "\n",
            "üìñ Ingesting CDC events\n",
            "================================================================================\n",
            "Mode: MERGE with Column Families\n",
            "Source: defaultdb.public.usertable_update_delete_multi_cf (CockroachDB)\n",
            "Target: robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf (Databricks Delta)\n",
            "Source path: /Volumes/robert_lee/robert_lee_cockroachdb/cockroachdb_cdc_1768934658/parquet/defaultdb/public/usertable_update_delete_multi_cf/usertable_update_delete_multi_cf/ (all dates, recursively)\n",
            "File filter: *usertable_update_delete_multi_cf*.parquet\n",
            "   ‚úÖ Includes: Data files\n",
            "   ‚ùå Excludes: .RESOLVED, _metadata/, _SUCCESS, etc.\n",
            "\n",
            "Primary keys: ['ycsb_key']\n",
            "\n",
            "üîí RESOLVED Watermarking: ENABLED\n",
            "   This GUARANTEES all column family fragments are complete before processing\n",
            "   üîç Scanning for .RESOLVED files in Unity Catalog Volume...\n",
            "   üìÇ Volume path: /Volumes/robert_lee/robert_lee_cockroachdb/cockroachdb_cdc_1768934658\n",
            "   ‚è≥ Listing files (this may take 5-10 seconds)...\n",
            "   ‚úÖ File listing completed in 2.2s\n",
            "   üìÅ Found 1450 .RESOLVED file(s)\n",
            "   üìÑ Example RESOLVED file: 202601292259386834839740000000000.RESOLVED\n",
            "   ‚úÖ Found 1450 .RESOLVED file(s)\n",
            "   ‚úÖ Latest RESOLVED watermark: 1770266019000000000 nanos\n",
            "      (2026-02-04 22:33:39 UTC)\n",
            "   üí° Only events with timestamp ‚â§ 1770266019000000000 will be processed\n",
            "      This GUARANTEES all column family fragments have arrived\n",
            "\n",
            "   ‚úÖ RESOLVED watermark will be applied to CDC events\n",
            "\n",
            "‚úÖ Schema inferred from data files\n",
            "\n",
            "   üîí Applying RESOLVED watermark filter: timestamp ‚â§ 1770266019000000000\n",
            "   ‚úÖ RESOLVED watermark applied - only complete data will be processed\n",
            "‚úÖ CDC transformations applied (streaming compatible)\n",
            "   ‚ÑπÔ∏è  Column family merge will happen in Stage 2 (batch mode)\n",
            "\n",
            "üî∑ STAGE 1: Streaming to staging table (no aggregations)\n",
            "   Staging: robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf_staging_cf\n",
            "\n",
            "‚è≥ Streaming CDC events to staging table...\n",
            "‚úÖ Stream completed\n",
            "\n",
            "   üîß Merging column family fragments (SCD Type 1 mode)...\n",
            "      Step 1: Merge fragments within same CDC event\n",
            "      Step 2: Deduplicate to keep latest state per key\n",
            "\n",
            "\n",
            "üîç Column Family Merge (Batch Mode)\n",
            "   Primary key columns: ['ycsb_key']\n",
            "   Metadata columns: 17 columns\n",
            "   Data columns: 10 columns\n",
            "     ['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9']\n",
            "\n",
            "üìä Fragmentation Detection:\n",
            "   Total rows: 1,566\n",
            "   Unique events (PK + timestamp + operation): 523\n",
            "   Duplication ratio: 3.0x\n",
            "\n",
            "üîß Column family fragmentation detected!\n",
            "   Merging 1,566 fragments into 523 distinct CDC events...\n",
            "\n",
            "‚úÖ Merge transformation applied!\n",
            "   Batch DataFrame merged\n",
            "   ‚úÖ Fragments merged: 1043\n",
            "\n",
            "üî∑ STAGE 2: Applying MERGE logic (batch operation)\n",
            "   Source: robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf_staging_cf\n",
            "   Target: robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf\n",
            "\n",
            "   üìä Raw staging events: 523\n",
            "   üîÑ Deduplicating by primary keys: ['ycsb_key']...\n",
            "   ‚úÖ Deduplicated: 280 unique events (243 duplicates removed)\n",
            "   üîÑ Executing MERGE...\n",
            "      Join: target.ycsb_key = source.ycsb_key\n",
            "      ‚ÑπÔ∏è  _cdc_operation will be preserved for monitoring\n",
            "   ‚úÖ MERGE complete: processed 280 events\n",
            "\n",
            "================================================================================\n",
            "‚úÖ CDC INGESTION COMPLETE (MERGE + COLUMN FAMILIES)\n",
            "================================================================================\n",
            "üìä Raw events: 1566\n",
            "üìä After deduplication: 280 unique events\n",
            "üìä Staging table: robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf_staging_cf\n",
            "üìä Target table:  robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf\n",
            "\n",
            "üí° TIP: Staging table can be dropped after successful MERGE:\n",
            "   spark.sql('DROP TABLE IF EXISTS robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf_staging_cf')\n",
            "üìä Query your data: SELECT * FROM robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf\n"
          ]
        }
      ],
      "source": [
        "# Import CDC ingestion functions from cockroachdb_autoload.py\n",
        "import importlib, cockroachdb_autoload\n",
        "importlib.reload(cockroachdb_autoload)\n",
        "from cockroachdb_autoload import (\n",
        "    ingest_cdc_append_only_single_family,\n",
        "    ingest_cdc_append_only_multi_family,\n",
        "    ingest_cdc_with_merge_single_family,\n",
        "    ingest_cdc_with_merge_multi_family\n",
        ")\n",
        "\n",
        "print(f\"üî∑ CDC Configuration:\")\n",
        "print(f\"   Processing Mode: {config.cdc_config.mode}\")\n",
        "print(f\"   Column Family Mode: {config.cdc_config.column_family_mode}\")\n",
        "print(f\"   Data Source: {config.data_source}\")\n",
        "print()\n",
        "\n",
        "# Select function based on BOTH config.cdc_config.mode and config.cdc_config.column_family_mode\n",
        "if config.cdc_config.mode == \"append_only\" and config.cdc_config.column_family_mode == \"single_cf\":\n",
        "    print(f\"üìò Running: ingest_cdc_append_only_single_family()\")\n",
        "    print(f\"   - All CDC events will be stored as rows\")\n",
        "    print(f\"   - No column family merging needed\\n\")\n",
        "    \n",
        "    query = ingest_cdc_append_only_single_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "\n",
        "elif config.cdc_config.mode == \"append_only\" and config.cdc_config.column_family_mode == \"multi_cf\":\n",
        "    print(f\"üìô Running: ingest_cdc_append_only_multi_family()\")\n",
        "    print(f\"   - All CDC events will be stored as rows\")\n",
        "    print(f\"   - Column family fragments will be merged\\n\")\n",
        "    \n",
        "    if not config.cdc_config.primary_key_columns:\n",
        "        raise ValueError(\"config.cdc_config.primary_key_columns required for multi_cf mode\")\n",
        "    \n",
        "    query = ingest_cdc_append_only_multi_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "\n",
        "elif config.cdc_config.mode == \"update_delete\" and config.cdc_config.column_family_mode == \"single_cf\":\n",
        "    print(f\"üìó Running: ingest_cdc_with_merge_single_family()\")\n",
        "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
        "    print(f\"   - No column family merging needed\\n\")\n",
        "    \n",
        "    if not config.cdc_config.primary_key_columns:\n",
        "        raise ValueError(\"config.cdc_config.primary_key_columns required for update_delete mode\")\n",
        "    \n",
        "    result = ingest_cdc_with_merge_single_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "    \n",
        "    query = result[\"query\"]\n",
        "\n",
        "elif config.cdc_config.mode == \"update_delete\" and config.cdc_config.column_family_mode == \"multi_cf\":\n",
        "    print(f\"üìï Running: ingest_cdc_with_merge_multi_family()\")\n",
        "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
        "    print(f\"   - Column family fragments will be merged\\n\")\n",
        "    \n",
        "    if not config.cdc_config.primary_key_columns:\n",
        "        raise ValueError(\"config.cdc_config.primary_key_columns required for update_delete + multi_cf mode\")\n",
        "    \n",
        "    result = ingest_cdc_with_merge_multi_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "    \n",
        "    query = result[\"query\"]\n",
        "\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Invalid mode combination:\\n\"\n",
        "        f\"  cdc_mode='{config.cdc_config.mode}' (valid: 'append_only', 'update_delete')\\n\"\n",
        "        f\"  column_family_mode='{config.cdc_config.column_family_mode}' (valid: 'single_cf', 'multi_cf')\\n\"\n",
        "        f\"Change modes in Cell 1.\"\n",
        "    )\n",
        "\n",
        "# Wait for completion (if not already complete)\n",
        "if config.cdc_config.mode == \"append_only\":\n",
        "    query.awaitTermination()\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"‚úÖ CDC INGESTION COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"   Mode: {config.cdc_config.mode} + {config.cdc_config.column_family_mode}\")\n",
        "    print(f\"   Target: {config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\")\n",
        "    print()\n",
        "    print(f\"üìä Query your data: SELECT * FROM {config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\")\n",
        "else:\n",
        "    # update_delete mode already completed inside the function\n",
        "    print(f\"üìä Query your data: SELECT * FROM {config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5df3f327",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üîç CDC SYNC DIAGNOSIS CONFIGURATION\n",
            "================================================================================\n",
            "   Source: defaultdb.public.usertable_update_delete_multi_cf\n",
            "   Target: robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf\n",
            "   Staging: robert_lee.robert_lee_cockroachdb.usertable_update_delete_multi_cf_staging_cf\n",
            "   Storage: Unity Catalog Volume\n",
            "   Path: /Volumes/robert_lee/robert_lee_cockroachdb/cockroachdb_cdc_1768934658/parquet/de...\n",
            "\n",
            "üìä Refreshing target DataFrame...\n",
            "‚úÖ Target DataFrame refreshed: 64 rows\n",
            "\n",
            "\n",
            "================================================================================\n",
            "üìä CDC EVENT SUMMARY\n",
            "================================================================================\n",
            "Total rows: 64\n",
            "CDC Processing Mode: update_delete\n",
            "Column Family Mode: multi_cf\n",
            "\n",
            "Rows by last CDC operation:\n",
            "+--------------+-----+\n",
            "|_cdc_operation|count|\n",
            "+--------------+-----+\n",
            "|        UPSERT|   64|\n",
            "+--------------+-----+\n",
            "\n",
            "\n",
            "üìã Sample rows (showing first 5):\n",
            "+--------+--------------------+--------------+-------------------+\n",
            "|ycsb_key|field0              |_cdc_operation|_cdc_timestamp     |\n",
            "+--------+--------------------+--------------+-------------------+\n",
            "|218     |inserted_value_218_0|UPSERT        |2026-02-02 21:30:15|\n",
            "|219     |inserted_value_219_0|UPSERT        |2026-02-02 21:30:15|\n",
            "|217     |inserted_value_217_0|UPSERT        |2026-02-02 21:30:15|\n",
            "|221     |inserted_value_221_0|UPSERT        |2026-02-04 20:59:34|\n",
            "|220     |inserted_value_220_0|UPSERT        |2026-02-04 20:59:34|\n",
            "+--------+--------------------+--------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "üìä Mode: UPDATE_DELETE\n",
            "   ‚Ä¢ MERGE operations applied: DELETEs removed, UPDATEs applied, INSERTs added\n",
            "   ‚Ä¢ _cdc_operation shows: UPSERT (last operation on each row)\n",
            "   ‚Ä¢ Row count = current state (deduplicated)\n",
            "\n",
            "================================================================================\n",
            "üîç SOURCE vs TARGET VERIFICATION\n",
            "================================================================================\n",
            "\n",
            "üîå Establishing CockroachDB connection...\n",
            "‚úÖ Connection established\n",
            "\n",
            "\n",
            "üîå Connection closed\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Connection' object has no attribute 'commit'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m importlib.reload(cockroachdb_debug)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcockroachdb_debug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_full_diagnosis_from_config\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mrun_full_diagnosis_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/crdb_to_dbx/crdb_to_dbx/cockroachdb_debug.py:1495\u001b[39m, in \u001b[36mrun_full_diagnosis_from_config\u001b[39m\u001b[34m(spark, config, conn, mismatched_columns)\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;66;03m# Get source table stats\u001b[39;00m\n\u001b[32m   1494\u001b[39m source_table_fqn = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_catalog\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_schema\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1495\u001b[39m source_stats = \u001b[43mget_table_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_table_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1496\u001b[39m source_sum = get_column_sum(conn, source_table_fqn, \u001b[33m'\u001b[39m\u001b[33mycsb_key\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# Get raw target stats first (before any processing)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/crdb_to_dbx/crdb_to_dbx/cockroachdb_ycsb.py:108\u001b[39m, in \u001b[36mget_table_stats\u001b[39m\u001b[34m(conn, table_name)\u001b[39m\n\u001b[32m    105\u001b[39m min_key, max_key, count = result[\u001b[32m0\u001b[39m]\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Commit to close the read transaction - prevents conflicts with subsequent write transactions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommit\u001b[49m()\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    111\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmin_key\u001b[39m\u001b[33m'\u001b[39m: min_key,\n\u001b[32m    112\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_key\u001b[39m\u001b[33m'\u001b[39m: max_key,\n\u001b[32m    113\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m: count,\n\u001b[32m    114\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mis_empty\u001b[39m\u001b[33m'\u001b[39m: min_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m }\n",
            "\u001b[31mAttributeError\u001b[39m: 'Connection' object has no attribute 'commit'"
          ]
        }
      ],
      "source": [
        "# ALL-IN-ONE CDC DIAGNOSIS\n",
        "\n",
        "# What this does:\n",
        "#   1. CDC Event Summary (replaces Cell 13)\n",
        "#      - Shows total rows, operation breakdown, sample data\n",
        "#   \n",
        "#   2. Source vs Target Verification (replaces Cell 14)\n",
        "#      - Connects to CockroachDB source\n",
        "#      - Auto-deduplicates target for append_only mode\n",
        "#      - Compares column sums\n",
        "#      - Detects mismatches\n",
        "#   \n",
        "#   3. Detailed Diagnosis (automatic if issues found)\n",
        "#      - Column family sync analysis\n",
        "#      - CDC event distribution\n",
        "#      - Row-by-row comparison\n",
        "#      - Troubleshooting recommendations\n",
        "#\n",
        "# Smart behavior:\n",
        "#   ‚úÖ If everything matches ‚Üí Shows \"Perfect sync!\" and exits\n",
        "#   ‚ö†Ô∏è  If mismatches found ‚Üí Automatically runs detailed diagnosis\n",
        "#\n",
        "# No external dependencies - just run this!\n",
        "# ============================================================================\n",
        "\n",
        "import importlib,cockroachdb_ycsb,cockroachdb_debug, cockroachdb_conn\n",
        "importlib.reload(cockroachdb_conn)  # Reload first (cockroachdb_debug depends on it)\n",
        "importlib.reload(cockroachdb_ycsb)  # Reload first (cockroachdb_ycsb depends on it)\n",
        "importlib.reload(cockroachdb_debug)\n",
        "from cockroachdb_debug import run_full_diagnosis_from_config\n",
        "\n",
        "run_full_diagnosis_from_config(spark=spark, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a8f6b5",
      "metadata": {},
      "source": [
        "## Optional: Cleanup\n",
        "\n",
        "Run the cells below if you want to clean up the test resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7250ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è  SAFETY STOP: Cleanup Section\n",
        "# This cell prevents accidental cleanup when running \"Run All\"\n",
        "# \n",
        "# To cleanup resources, manually run each cell below INDIVIDUALLY:\n",
        "#   - Cell 16: Cancel changefeed\n",
        "#   - Cell 17: Drop CockroachDB source table  \n",
        "#   - Cell 18: Drop Databricks target table & checkpoint\n",
        "#   - Cell 19: Clear Azure changefeed data (optional - use for complete reset)\n",
        "\n",
        "raise RuntimeError(\n",
        "    \"\\n\"\n",
        "    \"‚ö†Ô∏è  CLEANUP SAFETY STOP\\n\"\n",
        "    \"\\n\"\n",
        "    \"The cells below will DELETE your resources.\\n\"\n",
        "    \"Do NOT run all cells - run each cleanup cell individually.\\n\"\n",
        "    \"\\n\"\n",
        "    \"üí° TIP: If Cell 13 shows sync issues due to old data,\\n\"\n",
        "    \"   run Cell 19 to clear Azure changefeed data completely.\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8e97da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 1: CANCEL CHANGEFEED(S)\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=False  # Skip test, connection already validated\n",
        ")\n",
        "try:\n",
        "    with conn.cursor() as cur:\n",
        "        # Find ALL changefeed jobs by matching the sink_uri\n",
        "        # (matches the same pattern used in Cell 9)\n",
        "        # Note: We cancel ALL matches to handle duplicate scenarios\n",
        "        sink_uri_pattern = f\"%{config.azure_storage.container_name}/{config.cdc_config.path}%\"\n",
        "        \n",
        "        cur.execute(\"\"\"\n",
        "            SELECT job_id, sink_uri\n",
        "            FROM [SHOW CHANGEFEED JOBS] \n",
        "            WHERE sink_uri LIKE %s\n",
        "            AND status IN ('running', 'paused')\n",
        "        \"\"\", (sink_uri_pattern,))\n",
        "        \n",
        "        changefeeds = cur.fetchall()\n",
        "        if changefeeds:\n",
        "            print(f\"üóëÔ∏è  Cancelling {len(changefeeds)} changefeed(s)...\")\n",
        "            for job_id, sink_uri in changefeeds:\n",
        "                cur.execute(f\"CANCEL JOB {job_id}\")\n",
        "                print(f\"   ‚úÖ Cancelled Job ID: {job_id}\")\n",
        "                print(f\"      Sink URI: {sink_uri[:80]}...\")\n",
        "            if len(changefeeds) > 1:\n",
        "                print(f\"\\n‚ö†Ô∏è  Cancelled {len(changefeeds)} changefeeds (duplicates detected!)\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  No active changefeeds found for this source ‚Üí target mapping\")\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35e74ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 2: DROP SOURCE TABLE (CockroachDB)\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=False  # Skip test, connection already validated\n",
        ")\n",
        "try:\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(f\"DROP TABLE IF EXISTS {config.tables.source_table_name} CASCADE\")\n",
        "        conn.commit()\n",
        "    print(f\"‚úÖ Table '{config.tables.source_table_name}' dropped from CockroachDB\")\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988ed2d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 19: CLEAR AZURE CHANGEFEED DATA (Optional)\n",
        "# ‚ö†Ô∏è  WARNING: This will DELETE all changefeed data in Azure for this table!\n",
        "#\n",
        "# Use this when:\n",
        "# - You want to start completely fresh\n",
        "# - Old data from previous runs is causing sync issues\n",
        "# - You changed the table schema (e.g., VARCHAR ‚Üí INT)\n",
        "#\n",
        "# Uses Azure SDK (same as Cell 11 for checking files)\n",
        "\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "# Use config.cdc_config.path from config (must match Cell 9 changefeed config.cdc_config.path)\n",
        "changefeed_path = f\"{config.cdc_config.path}/\"\n",
        "\n",
        "print(f\"üóëÔ∏è  Deleting Azure changefeed data...\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Container: {config.azure_storage.container_name}\")\n",
        "print(f\"Path: {changefeed_path}\")\n",
        "print()\n",
        "\n",
        "# Connect to Azure (same as Cell 9)\n",
        "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={config.azure_storage.account_name};AccountKey={config.azure_storage.account_key};EndpointSuffix=core.windows.net\"\n",
        "blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
        "container_client = blob_service.get_container_client(config.azure_storage.container_name)\n",
        "\n",
        "# List all blobs with this prefix\n",
        "print(f\"üîç Scanning for files...\")\n",
        "blobs = list(container_client.list_blobs(name_starts_with=changefeed_path))\n",
        "\n",
        "if not blobs:\n",
        "    print(f\"‚ÑπÔ∏è  No files found at: {changefeed_path}\")\n",
        "    print(f\"   Files may have already been deleted, or config.cdc_config.path is incorrect\")\n",
        "    print()\n",
        "    print(f\"üí° To check what's in the container, run Cell 9\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found {len(blobs)} items to delete\")\n",
        "    \n",
        "    # Show sample items\n",
        "    data_files = [b for b in blobs if b.size > 0 and '.parquet' in b.name]\n",
        "    resolved_files = [b for b in blobs if '.RESOLVED' in b.name]\n",
        "    directories = [b for b in blobs if b.size == 0]\n",
        "    \n",
        "    print(f\"   üìÑ Data files: {len(data_files)}\")\n",
        "    print(f\"   üïê Resolved files: {len(resolved_files)}\")\n",
        "    print(f\"   üìÅ Directories: {len(directories)}\")\n",
        "    print()\n",
        "    \n",
        "    # Delete all blobs with this prefix\n",
        "    # Note: Azure SDK doesn't have recursive delete - we list all blobs and delete each one\n",
        "    print(f\"üîÑ Deleting {len(blobs)} items...\")\n",
        "    deleted = 0\n",
        "    failed = 0\n",
        "    \n",
        "    for blob in blobs:\n",
        "        try:\n",
        "            container_client.delete_blob(blob.name)\n",
        "            deleted += 1\n",
        "            if deleted % 50 == 0:\n",
        "                print(f\"   Deleted {deleted}/{len(blobs)} items...\", end='\\r')\n",
        "        except Exception as e:\n",
        "            # Some errors are expected (e.g., directories already removed)\n",
        "            error_str = str(e)\n",
        "            if \"DirectoryIsNotEmpty\" not in error_str and \"BlobNotFound\" not in error_str:\n",
        "                failed += 1\n",
        "                print(f\"\\n   ‚ö†Ô∏è  Failed: {blob.name[:60]}... - {e}\")\n",
        "    \n",
        "    print(f\"‚úÖ Deleted {deleted} items from Azure                    \")\n",
        "    if failed > 0:\n",
        "        print(f\"   ‚ö†Ô∏è  Failed to delete {failed} items\")\n",
        "    \n",
        "    print()\n",
        "    print(f\"=\" * 80)\n",
        "    print(f\"‚úÖ Cleanup complete!\")\n",
        "    print()\n",
        "    print(f\"üí° Next steps:\")\n",
        "    print(f\"   1. Drop the Databricks target table (Cell 17)\")\n",
        "    print(f\"   2. Re-run from Cell 6 (Snapshot) to start fresh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033d5054",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 3: DROP TARGET TABLE & CHECKPOINT (Databricks)\n",
        "# Checkpoint lives on target schema; directory name = table name (same as ingestion Cell 10).\n",
        "from cockroachdb_autoload import _build_paths\n",
        "_, checkpoint_path, target_table_fqn = _build_paths(config, spark=spark)\n",
        "\n",
        "# Drop Delta table\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
        "print(f\"‚úÖ Delta table '{target_table_fqn}' dropped\")\n",
        "\n",
        "# Remove checkpoint\n",
        "try:\n",
        "    dbutils.fs.rm(checkpoint_path, True)\n",
        "    print(f\"‚úÖ Checkpoint '{checkpoint_path}' removed\")\n",
        "except:\n",
        "    print(f\"‚ÑπÔ∏è  Checkpoint not found (may have been already removed)\")\n",
        "\n",
        "print(\"\\n‚úÖ Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53c9621",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 4: Complete cleanup for fresh start\n",
        "\n",
        "# 1. Drop staging table\n",
        "staging_table_fqn = f\"{config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}_staging_cf\"\n",
        "print(f\"üóëÔ∏è  Dropping staging table: {staging_table_fqn}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {staging_table_fqn}\")\n",
        "\n",
        "# 2. Drop target table (if not already done)\n",
        "target_table_fqn = f\"{config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\"\n",
        "print(f\"üóëÔ∏è  Dropping target table: {target_table_fqn}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
        "\n",
        "# 3. Clear checkpoint location (target schema, directory = table name + _merge_cf)\n",
        "from cockroachdb_autoload import _build_paths\n",
        "_, checkpoint_path, _ = _build_paths(config, mode_suffix=\"_merge_cf\", spark=spark)\n",
        "print(f\"üóëÔ∏è  Clearing checkpoint: {checkpoint_path}\")\n",
        "try:\n",
        "    dbutils.fs.rm(checkpoint_path, recurse=True)\n",
        "    print(f\"   ‚úÖ Checkpoint cleared\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ÑπÔ∏è  Checkpoint may not exist: {e}\")\n",
        "\n",
        "# 4. Verify cleanup\n",
        "print(f\"\\n‚úÖ Cleanup complete! Ready for fresh start.\")\n",
        "print(f\"   Next: Re-run Cell 12 (ingestion)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a28f2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the schema\n",
        "print(f\"üìÅ Creating schema: {config.tables.destination_catalog}.{config.tables.destination_schema}\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {config.tables.destination_catalog}.{config.tables.destination_schema}\")\n",
        "print(f\"‚úÖ Schema created\")\n",
        "\n",
        "# Verify schema exists\n",
        "schemas = spark.sql(f\"SHOW SCHEMAS IN {config.tables.destination_catalog}\").collect()\n",
        "schema_names = [row['databaseName'] for row in schemas]\n",
        "if config.tables.destination_schema in schema_names:\n",
        "    print(f\"‚úÖ Verified: Schema {config.tables.destination_schema} exists\")\n",
        "else:\n",
        "    print(f\"‚ùå Schema {config.tables.destination_schema} not found. Available schemas: {schema_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83bdda3d",
      "metadata": {},
      "source": [
        "# Debug Codes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_3.11.3 (3.11.13)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
