{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Stream CockroachDB CDC to Databricks (Azure)\n",
        "\n",
        "This notebook demonstrates how to stream CockroachDB changefeeds to Databricks using Azure Blob Storage.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- CockroachDB cluster (Cloud or self-hosted)\n",
        "- Azure Storage Account with hierarchical namespace enabled\n",
        "- Databricks workspace with Unity Catalog\n",
        "- Unity Catalog External Location configured for your storage account\n",
        "\n",
        "**Note:** This notebook uses the **YCSB (Yahoo! Cloud Serving Benchmark)** schema as the default table structure, with `ycsb_key` as the primary key and `field0-9` columns. The default schema name is `public`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c81271",
      "metadata": {},
      "source": [
        "## CDC Mode Selection\n",
        "\n",
        "This notebook supports **4 CDC ingestion modes** by combining two independent settings:\n",
        "\n",
        "### 1. CDC Processing Mode (`cdc_mode`)\n",
        "How CDC events are processed in the target table:\n",
        "\n",
        "- **`append_only`**: Store all CDC events as rows (audit log)\n",
        "  - **Behavior**: All events (INSERT/UPDATE/DELETE) are appended as new rows\n",
        "  - **Use case**: History tracking, time-series analysis, audit logs\n",
        "  - **Storage**: Higher (keeps all historical events)\n",
        "\n",
        "- **`update_delete`**: Apply MERGE logic (current state replication)\n",
        "  - **Behavior**: DELETE removes rows, UPDATE modifies rows in-place\n",
        "  - **Use case**: Current state synchronization, production replication\n",
        "  - **Storage**: Lower (only latest state per key)\n",
        "\n",
        "### 2. Column Family Mode (`column_family_mode`)\n",
        "Table structure and changefeed configuration:\n",
        "\n",
        "- **`single_cf`**: Standard table (1 column family, default)\n",
        "  - **Changefeed**: `split_column_families=false`\n",
        "  - **Files**: 1 Parquet file per CDC event\n",
        "  - **Use case**: Most tables, simpler configuration, better performance\n",
        "\n",
        "- **`multi_cf`**: Multiple column families (for wide tables)\n",
        "  - **Changefeed**: `split_column_families=true`\n",
        "  - **Files**: Multiple Parquet files per CDC event (fragments need merging)\n",
        "  - **Use case**: Wide tables (50+ columns), selective column access patterns\n",
        "\n",
        "### Function Selection Matrix\n",
        "\n",
        "The notebook automatically selects the appropriate ingestion function based on your configuration:\n",
        "\n",
        "| CDC Mode | Column Family Mode | Function Called |\n",
        "|----------|-------------------|-----------------|\n",
        "| `append_only` | `single_cf` | `ingest_cdc_append_only_single_family()` |\n",
        "| `append_only` | `multi_cf` | `ingest_cdc_append_only_multi_family()` |\n",
        "| `update_delete` | `single_cf` | `ingest_cdc_with_merge_single_family()` |\n",
        "| `update_delete` | `multi_cf` | `ingest_cdc_with_merge_multi_family()` |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9f6d2d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "config",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration loaded from: ../.env/cockroachdb_cdc_tutorial_config_update_delete_single_cf.json\n",
            "‚úÖ Configuration loaded\n",
            "   Data Source: uc_external_volume\n",
            "   UC Volume: robert_lee.robert_lee_cockroachdb.cockroachdb_cdc_1768934658\n",
            "   CDC Processing Mode: update_delete\n",
            "   Column Family Mode: single_cf\n",
            "   Primary Keys: []\n",
            "   Target Table: usertable_update_delete_single_cf\n",
            "   CDC Workload: 10 snapshot ‚Üí +10 INSERTs, ~9 UPDATEs, -8 DELETEs\n",
            "   Primary keys from schema: ['ycsb_key']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from urllib.parse import quote\n",
        "\n",
        "# Configuration file config.cdc_config.path (adjust as needed)\n",
        "#config_file = \"../.env/cockroachdb_cdc_tutorial_config_append_single_cf.json\"\n",
        "\n",
        "#config_file = \"../.env/cockroachdb_cdc_tutorial_config_append_multi_cf.json\"\n",
        "\n",
        "#config_file = \"../.env/cockroachdb_cdc_tutorial_config_update_delete_multi_cf.json\"\n",
        "\n",
        "config_file = \"../.env/cockroachdb_cdc_tutorial_config_update_delete_single_cf.json\"\n",
        "\n",
        "\n",
        "import importlib\n",
        "import cockroachdb_config\n",
        "importlib.reload(cockroachdb_config)\n",
        "from cockroachdb_config import load_config, process_config, ensure_primary_key_from_schema\n",
        "\n",
        "# Try to load from file, fallback to embedded config\n",
        "config = load_config(config_file)\n",
        "\n",
        "# Embedded configuration (fallback)\n",
        "if config is None:\n",
        "    config = {\n",
        "      \"cockroachdb\": {\n",
        "        \"host\": \"replace_me\",\n",
        "        \"port\": 26257,\n",
        "        \"user\": \"replace_me\",\n",
        "        \"password\": \"replace_me\",\n",
        "        \"database\": \"defaultdb\"\n",
        "      },\n",
        "      \"cockroachdb_source\": {\n",
        "        \"catalog\": \"defaultdb\",\n",
        "        \"schema\": \"public\",\n",
        "        \"table_name\": \"usertable\",\n",
        "        \"_schema_note\": \"Default schema is 'public'. Table uses YCSB structure (ycsb_key, field0-9)\",\n",
        "      },\n",
        "      \"azure_storage\": {\n",
        "        \"account_name\": \"replace_me\",\n",
        "        \"account_key\": \"replace_me\",\n",
        "        \"config.azure_storage.container_name\": \"changefeed-events\"\n",
        "      },\n",
        "      \"databricks_target\": {\n",
        "        \"catalog\": \"main\",\n",
        "        \"schema\": \"replace_me\",\n",
        "        \"table_name\": \"usertable\",\n",
        "      },\n",
        "      \"cdc_config\": {\n",
        "        \"mode\": \"append_only\",\n",
        "        \"column_family_mode\": \"multi_cf\",\n",
        "        \"auto_suffix_mode_family\": True,\n",
        "      },\n",
        "    \"uc_external_volume\": {\n",
        "        \"volume_catalog\": \"robert_lee\",\n",
        "        \"volume_full_path\": \"robert_lee.robert_lee_cockroachdb.cockroachdb_cdc_1768934658\",\n",
        "        \"volume_id\": \"de84b515-ec65-4dbc-8a76-460328c6f1b1\",\n",
        "        \"volume_name\": \"cockroachdb_cdc_1768934658\",\n",
        "        \"volume_schema\": \"robert_lee_cockroachdb\"\n",
        "    },       \n",
        "      \"workload_config\": {\n",
        "        \"config.workload_config.snapshot_count\": 10,\n",
        "        \"config.workload_config.insert_count\": 10,\n",
        "        \"config.workload_config.update_count\": 9,\n",
        "        \"config.workload_config.delete_count\": 8,\n",
        "      }\n",
        "    }\n",
        "config = process_config(config)\n",
        "# Fill primary_key_columns from schema in storage if missing (requires spark for UC Volume)\n",
        "config = ensure_primary_key_from_schema(config, spark)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "install",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "%pip install pg8000 azure-storage-blob --quiet\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "connect",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Connected to CockroachDB\n",
            "   Version: CockroachDB CCL v25.4.4 (x86_64-pc-linux-gnu, buil...\n",
            "‚úÖ Connection function ready for use\n"
          ]
        }
      ],
      "source": [
        "# Import CockroachDB connection utilities\n",
        "import importlib\n",
        "import cockroachdb_conn\n",
        "importlib.reload(cockroachdb_conn)\n",
        "from cockroachdb_conn import get_cockroachdb_connection\n",
        "\n",
        "# Test connection to CockroachDB\n",
        "# The function automatically tests the connection (test=True by default)\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=True  # Automatically tests connection and prints version (default)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Connection function ready for use\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "64c40c9d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper functions loaded (CockroachDB & Azure)\n",
            "‚úÖ YCSB utility functions imported from cockroachdb_ycsb.py\n"
          ]
        }
      ],
      "source": [
        "# Import storage utilities (works with both Azure and UC Volume)\n",
        "import importlib\n",
        "import cockroachdb_storage\n",
        "importlib.reload(cockroachdb_storage)\n",
        "from cockroachdb_storage import check_files, wait_for_files\n",
        "\n",
        "# Import YCSB utility functions\n",
        "import cockroachdb_ycsb\n",
        "importlib.reload(cockroachdb_ycsb)\n",
        "from cockroachdb_ycsb import (\n",
        "    get_table_stats,\n",
        "    get_table_stats_spark,\n",
        "    get_column_sum,\n",
        "    get_column_sum_spark,\n",
        "    deduplicate_to_latest,\n",
        "    get_column_sum_spark_deduplicated\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded (CockroachDB & Azure)\")\n",
        "print(\"‚úÖ YCSB utility functions imported from cockroachdb_ycsb.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "create_table",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Table 'usertable_update_delete_single_cf' created (or already exists)\n",
            "   Column Family Mode: single_cf\n",
            "   Column families: 1 column family (default primary)\n"
          ]
        }
      ],
      "source": [
        "# Create table using cockroachdb_ycsb.py\n",
        "# Import YCSB functions\n",
        "import importlib, cockroachdb_ycsb\n",
        "importlib.reload(cockroachdb_ycsb)\n",
        "from cockroachdb_ycsb import create_ycsb_table\n",
        "\n",
        "# Create table\n",
        "try:\n",
        "    create_ycsb_table(\n",
        "        conn=conn,\n",
        "        table_name=config.tables.source_table_name,\n",
        "        column_family_mode=config.cdc_config.column_family_mode\n",
        "    )\n",
        "except:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "insert_data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è  Table already contains data - skipping snapshot insert\n",
            "   Current key range: 144 to 189\n",
            "   Tip: If you want to re-run the snapshot, drop the table first\n"
          ]
        }
      ],
      "source": [
        "from cockroachdb_ycsb import insert_ycsb_snapshot_with_random_nulls\n",
        "\n",
        "try:\n",
        "    insert_ycsb_snapshot_with_random_nulls(\n",
        "        conn=conn,\n",
        "        table_name=config.tables.source_table_name,\n",
        "        snapshot_count=config.workload_config.snapshot_count,\n",
        "        null_probability=0.3,  # 30% chance of NULL in snapshot\n",
        "        columns_to_randomize=['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9'],  # ALL fields\n",
        "        seed=42,  # Reproducible random NULLs\n",
        "        force_all_null_row=True  # Row 0 will have all randomized columns as NULL (edge case testing)\n",
        "    )\n",
        "except:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7b1c41ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 1 existing changefeed(s)\n",
            "   Job 1145690863609970690: running\n",
            "   Sink URI: azure://changefeed-events/parquet/defaultdb/public/usertable_update_delete_single_cf/usertable_update_delete_single_cf?AZURE_ACCOUNT_KEY=redacted&AZURE_ACCOUNT_NAME=cockroachcdc1768934658\n",
            "Using existing: 1 found\n"
          ]
        }
      ],
      "source": [
        "from cockroachdb_sql import create_changefeed_from_config\n",
        "\n",
        "try:\n",
        "    result = create_changefeed_from_config(conn, config, spark)\n",
        "    \n",
        "    if result['created']:\n",
        "        print(f\"New changefeed: Job {result['job_id']}\")\n",
        "    else:\n",
        "        print(f\"Using existing: {result['existing_count']} found\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7542f06",
      "metadata": {},
      "source": [
        "### Ensure schema in storage (run before first ingestion)\n",
        "\n",
        "Writes the table schema (including primary key) to Azure or UC Volume so the autoloader can resolve primary keys **without** CockroachDB credentials. Run this cell once after creating the changefeed; then ingestion can run in a backend that has no source DB access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ef77856d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Schema already in storage (primary keys: ['ycsb_key'])\n"
          ]
        }
      ],
      "source": [
        "# Ensure schema file exists in storage (Azure or UC Volume) before autoloader runs.\n",
        "# This allows ingestion to resolve primary keys from storage without source (CockroachDB) credentials.\n",
        "from cockroachdb_storage import ensure_schema_in_storage\n",
        "\n",
        "ensure_schema_in_storage(config, spark, conn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "run_workload",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Capturing baseline file count...\n",
            "   Data files: 19\n",
            "   Resolved files: 2365\n",
            "\n",
            "üìä Current table state:\n",
            "   Min key: 144, Max key: 189, Total rows: 46\n",
            "\n",
            "‚ûï Running 10 INSERTs (keys 190 to 199)...\n",
            "üìù Running 9 UPDATEs with random NULLs...\n",
            "   NULL probability: 50.0%\n",
            "   Columns to randomize: field0, field1, field2, field3, field4, field5, field6, field7, field8, field9\n",
            "   ‚ö†Ô∏è  First updated row (key 144) will have ALL randomized columns as NULL\n",
            "üóëÔ∏è  Running 8 DELETEs (keys 144 to 151)...\n",
            "\n",
            "‚úÖ Workload complete\n",
            "   Inserts: 10\n",
            "   Updates: 9 (with random NULLs)\n",
            "   Deletes: 8\n",
            "   Before: 46 rows (keys 144-189)\n",
            "   After:  48 rows (keys 152-199)\n",
            "   Net change: +2 rows\n",
            "\n",
            "‚è≥ Waiting for new CDC files to appear in Unity Catalog Volume...\n",
            "   Baseline: 19 data files, 2365 resolved files\n",
            "\n",
            "‚úÖ New CDC files appeared after 10 seconds!ata, 2365 resolved)\n",
            "   Data files: 19 ‚Üí 20 (+1)\n",
            "   Resolved files: 2365 ‚Üí 2366 (+1)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Capture baseline file count BEFORE generating CDC events\n",
        "print(\"üìä Capturing baseline file count...\")\n",
        "result_before = check_files(\n",
        "    config=config,\n",
        "    spark=spark,\n",
        "    verbose=False\n",
        ")\n",
        "files_before = len(result_before['data_files'])\n",
        "resolved_before = len(result_before['resolved_files'])\n",
        "print(f\"   Data files: {files_before}\")\n",
        "print(f\"   Resolved files: {resolved_before}\")\n",
        "print()\n",
        "\n",
        "# Run workload with NULL testing using cockroachdb_ycsb.py\n",
        "from cockroachdb_ycsb import run_ycsb_workload_with_random_nulls\n",
        "\n",
        "# Run workload - connection is managed by notebook, not closed here\n",
        "run_ycsb_workload_with_random_nulls(\n",
        "    conn=conn,\n",
        "    table_name=config.tables.source_table_name,\n",
        "    insert_count=config.workload_config.insert_count,\n",
        "    update_count=config.workload_config.update_count,\n",
        "    delete_count=config.workload_config.delete_count,\n",
        "    null_probability=0.5,  # 50% chance of NULL in UPDATEs\n",
        "    columns_to_randomize=['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9'],  # ALL fields\n",
        "    seed=42,  # Reproducible random NULLs\n",
        "    force_all_null_update=True  # First UPDATE will have all NULLs (edge case testing)\n",
        ")\n",
        "\n",
        "# Wait for new CDC files to appear in storage (positive confirmation)\n",
        "storage_label = \"Unity Catalog Volume\" if config.data_source == \"uc_external_volume\" else \"Azure\"\n",
        "print(f\"\")\n",
        "print(f\"‚è≥ Waiting for new CDC files to appear in {storage_label}...\")\n",
        "print(f\"   Baseline: {files_before} data files, {resolved_before} resolved files\")\n",
        "print()\n",
        "\n",
        "# Poll for new files (max 90 seconds)\n",
        "max_wait = 90\n",
        "check_interval = 10\n",
        "elapsed = 0\n",
        "\n",
        "while elapsed < max_wait:\n",
        "    result = check_files(\n",
        "        config=config,\n",
        "        spark=spark,\n",
        "        verbose=False\n",
        "    )\n",
        "    files_now = len(result['data_files'])\n",
        "    resolved_now = len(result['resolved_files'])\n",
        "    \n",
        "    if files_now > files_before or resolved_now > resolved_before:\n",
        "        print(f\"‚úÖ New CDC files appeared after {elapsed} seconds!\")\n",
        "        print(f\"   Data files: {files_before} ‚Üí {files_now} (+{files_now - files_before})\")\n",
        "        print(f\"   Resolved files: {resolved_before} ‚Üí {resolved_now} (+{resolved_now - resolved_before})\")\n",
        "        break\n",
        "    \n",
        "    print(f\"   Checking... ({elapsed}s elapsed, baseline: {files_before} data, {resolved_before} resolved)\", end='\\r')\n",
        "    time.sleep(check_interval)\n",
        "    elapsed += check_interval\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - files may still be flushing\")\n",
        "    print(f\"   Run Cell 11 to check manually\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "check_files",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üöÄ Using Spark for fast parallel file listing...\n",
            "   ‚è≥ Spark: Reading directory structure...\n",
            "   ‚è≥ Spark: Collecting file paths...\n",
            "   ‚úÖ Spark: Found 2386 total paths\n",
            "üìÅ Files in Unity Catalog Volume:\n",
            "   Path: /Volumes/robert_lee/robert_lee_cockroachdb/cockroachdb_cdc_1768934658/parquet/defaultdb/public/usertable_update_delete_single_cf/usertable_update_delete_single_cf/\n",
            "   üìÑ Data files: 20\n",
            "   üïê Resolved files: 2366\n",
            "   üìä Total: 2386\n",
            "\n",
            "   Example data file:\n",
            "   202602052126450000000000000000001-49e3e4ffda44edd1-2-3-00000000-usertable_update_delete_single_cf-1.parquet\n",
            "\n",
            "‚úÖ Files are ready! Proceed to Cell 10 to read with Databricks.\n"
          ]
        }
      ],
      "source": [
        "# Use the unified storage function to check for files (works with both Azure and UC Volume)\n",
        "result = check_files(\n",
        "    config=config,\n",
        "    spark=spark,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Provide guidance\n",
        "if len(result['data_files']) == 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  No data files found yet.\")\n",
        "    print(f\"   üí° Possible reasons:\")\n",
        "    print(f\"   - Changefeed not created yet (run Cell 9)\")\n",
        "    print(f\"   - Path configuration mismatch (check Cell 1 variables)\")\n",
        "    print(f\"   - Azure credentials issue (check External Location)\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Files are ready! Proceed to Cell 10 to read with Databricks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "read_cdc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî∑ CDC Configuration:\n",
            "   Processing Mode: update_delete\n",
            "   Column Family Mode: single_cf\n",
            "   Data Source: uc_external_volume\n",
            "\n",
            "üìó Running: ingest_cdc_with_merge_single_family()\n",
            "   - MERGE logic applied (UPDATE/DELETE processed)\n",
            "   - No column family merging needed\n",
            "\n",
            "   ‚úÖ Checkpoint volume ensured: robert_lee.robert_lee_cockroachdb.checkpoints\n",
            "üìñ Ingesting CDC events\n",
            "================================================================================\n",
            "Mode: MERGE (Apply UPDATE/DELETE)\n",
            "Source: defaultdb.public.usertable_update_delete_single_cf (CockroachDB)\n",
            "Target: robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf (Databricks Delta)\n",
            "Source path: /Volumes/robert_lee/robert_lee_cockroachdb/cockroachdb_cdc_1768934658/parquet/defaultdb/public/usertable_update_delete_single_cf/usertable_update_delete_single_cf/ (all dates, recursively)\n",
            "File filter: *usertable_update_delete_single_cf*.parquet\n",
            "   ‚úÖ Includes: Data files\n",
            "   ‚ùå Excludes: .RESOLVED, _metadata/, _SUCCESS, etc.\n",
            "\n",
            "Primary keys: ['ycsb_key']\n",
            "\n",
            "üîí RESOLVED Watermarking: ENABLED (optional for single-CF tables)\n",
            "   üîç Scanning for .RESOLVED files in Unity Catalog Volume...\n",
            "   üìÇ Volume path: /Volumes/robert_lee/robert_lee_cockroachdb/cockroachdb_cdc_1768934658\n",
            "   ‚è≥ Listing files (this may take 5-10 seconds)...\n",
            "   ‚úÖ File listing completed in 2.0s\n",
            "   üìÅ Found 2366 .RESOLVED file(s)\n",
            "   üìÑ Example RESOLVED file: 202601291725043527142210000000000.RESOLVED\n",
            "   ‚úÖ Found 2366 .RESOLVED file(s)\n",
            "   ‚úÖ Latest RESOLVED watermark (HLC): 1770352588000000000.0000000000\n",
            "      (2026-02-05 22:36:28 UTC)\n",
            "   üí° Only events with __crdb__updated ‚â§ 1770352588000000000.0000000000 will be processed\n",
            "\n",
            "‚úÖ Schema inferred from data files\n",
            "   (Filtering matches cockroachdb.py production code)\n",
            "\n",
            "   üîí Applying RESOLVED watermark filter: __crdb__updated ‚â§ 1770352588000000000.0000000000\n",
            "   ‚úÖ RESOLVED watermark applied - only complete data will be processed\n",
            "‚úÖ CDC transformations applied (streaming compatible)\n",
            "   ‚ÑπÔ∏è  Deduplication will happen in Stage 2 (batch mode)\n",
            "\n",
            "üî∑ STAGE 1: Streaming to staging table (no Python UDFs)\n",
            "   Staging: robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf_staging\n",
            "\n",
            "‚è≥ Streaming CDC events to staging table...\n",
            "‚úÖ Stream completed\n",
            "\n",
            "üî∑ STAGE 2: Applying MERGE logic (batch operation)\n",
            "   Source: robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf_staging\n",
            "   Target: robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf\n",
            "\n",
            "   üìä Raw staging events: 371\n",
            "   üîÑ Deduplicating by primary keys: ['ycsb_key']...\n",
            "   ‚úÖ Deduplicated: 200 unique events (171 duplicates removed)\n",
            "   üîÑ Executing MERGE...\n",
            "      Join: target.ycsb_key = source.ycsb_key\n",
            "      Order: whenMatchedDelete first (so DELETEs remove rows before any update), then whenMatchedUpdate, then whenNotMatchedInsert\n",
            "      Update when: UPSERT and source.__crdb__updated > target.__crdb__updated\n",
            "   ‚úÖ MERGE complete: processed 200 events\n",
            "\n",
            "================================================================================\n",
            "‚úÖ CDC INGESTION COMPLETE (TWO-STAGE MERGE)\n",
            "================================================================================\n",
            "üìä Raw events: 371\n",
            "üìä After deduplication: 200 unique events\n",
            "üìä Staging table: robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf_staging\n",
            "üìä Target table:  robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf\n",
            "\n",
            "üìã Target table includes:\n",
            "   - All data columns from source\n",
            "   - _cdc_operation: UPSERT (for monitoring)\n",
            "   - __crdb__updated: HLC timestamp (display as-is, full HLC)\n",
            "\n",
            "üí° TIP: Staging table can be dropped after successful MERGE:\n",
            "   spark.sql('DROP TABLE IF EXISTS robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf_staging')\n",
            "üìä Query your data: SELECT * FROM robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf\n"
          ]
        }
      ],
      "source": [
        "# Import CDC ingestion functions from cockroachdb_autoload.py\n",
        "import importlib, cockroachdb_autoload\n",
        "importlib.reload(cockroachdb_autoload)\n",
        "from cockroachdb_autoload import (\n",
        "    ingest_cdc_append_only_single_family,\n",
        "    ingest_cdc_append_only_multi_family,\n",
        "    ingest_cdc_with_merge_single_family,\n",
        "    ingest_cdc_with_merge_multi_family\n",
        ")\n",
        "\n",
        "print(f\"üî∑ CDC Configuration:\")\n",
        "print(f\"   Processing Mode: {config.cdc_config.mode}\")\n",
        "print(f\"   Column Family Mode: {config.cdc_config.column_family_mode}\")\n",
        "print(f\"   Data Source: {config.data_source}\")\n",
        "print()\n",
        "\n",
        "# Select function based on BOTH config.cdc_config.mode and config.cdc_config.column_family_mode\n",
        "if config.cdc_config.mode == \"append_only\" and config.cdc_config.column_family_mode == \"single_cf\":\n",
        "    print(f\"üìò Running: ingest_cdc_append_only_single_family()\")\n",
        "    print(f\"   - All CDC events will be stored as rows\")\n",
        "    print(f\"   - No column family merging needed\\n\")\n",
        "    \n",
        "    query = ingest_cdc_append_only_single_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "\n",
        "elif config.cdc_config.mode == \"append_only\" and config.cdc_config.column_family_mode == \"multi_cf\":\n",
        "    print(f\"üìô Running: ingest_cdc_append_only_multi_family()\")\n",
        "    print(f\"   - All CDC events will be stored as rows\")\n",
        "    print(f\"   - Column family fragments will be merged\\n\")\n",
        "    \n",
        "    query = ingest_cdc_append_only_multi_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "\n",
        "elif config.cdc_config.mode == \"update_delete\" and config.cdc_config.column_family_mode == \"single_cf\":\n",
        "    print(f\"üìó Running: ingest_cdc_with_merge_single_family()\")\n",
        "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
        "    print(f\"   - No column family merging needed\\n\")\n",
        "    \n",
        "    result = ingest_cdc_with_merge_single_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "    \n",
        "    query = result[\"query\"]\n",
        "\n",
        "elif config.cdc_config.mode == \"update_delete\" and config.cdc_config.column_family_mode == \"multi_cf\":\n",
        "    print(f\"üìï Running: ingest_cdc_with_merge_multi_family()\")\n",
        "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
        "    print(f\"   - Column family fragments will be merged\\n\")\n",
        "    \n",
        "    result = ingest_cdc_with_merge_multi_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "    \n",
        "    query = result[\"query\"]\n",
        "\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Invalid mode combination:\\n\"\n",
        "        f\"  cdc_mode='{config.cdc_config.mode}' (valid: 'append_only', 'update_delete')\\n\"\n",
        "        f\"  column_family_mode='{config.cdc_config.column_family_mode}' (valid: 'single_cf', 'multi_cf')\\n\"\n",
        "        f\"Change modes in Cell 1.\"\n",
        "    )\n",
        "\n",
        "# Wait for completion (if not already complete)\n",
        "if config.cdc_config.mode == \"append_only\":\n",
        "    query.awaitTermination()\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"‚úÖ CDC INGESTION COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"   Mode: {config.cdc_config.mode} + {config.cdc_config.column_family_mode}\")\n",
        "    print(f\"   Target: {config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\")\n",
        "    print()\n",
        "    print(f\"üìä Query your data: SELECT * FROM {config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\")\n",
        "else:\n",
        "    # update_delete mode already completed inside the function\n",
        "    print(f\"üìä Query your data: SELECT * FROM {config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5df3f327",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üîç CDC SYNC DIAGNOSIS CONFIGURATION\n",
            "================================================================================\n",
            "   Source: defaultdb.public.usertable_update_delete_single_cf\n",
            "   Target: robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf\n",
            "   Staging: robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf_staging_cf\n",
            "   Storage: Unity Catalog Volume\n",
            "   Path: /Volumes/robert_lee/robert_lee_cockroachdb/cockroachdb_cdc_1768934658/parquet/de...\n",
            "\n",
            "üìä Refreshing target DataFrame...\n",
            "‚úÖ Target DataFrame refreshed: 48 rows\n",
            "\n",
            "\n",
            "================================================================================\n",
            "üìä CDC EVENT SUMMARY\n",
            "================================================================================\n",
            "Total rows: 48\n",
            "CDC Processing Mode: update_delete\n",
            "Column Family Mode: single_cf\n",
            "\n",
            "Rows by last CDC operation:\n",
            "+--------------+-----+\n",
            "|_cdc_operation|count|\n",
            "+--------------+-----+\n",
            "|        UPSERT|   48|\n",
            "+--------------+-----+\n",
            "\n",
            "\n",
            "üìã Sample rows (showing first 5):\n",
            "+--------+--------------------+--------------+------------------------------+\n",
            "|ycsb_key|field0              |_cdc_operation|__crdb__updated               |\n",
            "+--------+--------------------+--------------+------------------------------+\n",
            "|191     |inserted_value_191_0|UPSERT        |1770330980832463153.0000000002|\n",
            "|192     |inserted_value_192_0|UPSERT        |1770330980832463153.0000000002|\n",
            "|152     |NULL                |UPSERT        |1770330980832463153.0000000002|\n",
            "|190     |inserted_value_190_0|UPSERT        |1770330980832463153.0000000002|\n",
            "|193     |inserted_value_193_0|UPSERT        |1770330980832463153.0000000002|\n",
            "+--------+--------------------+--------------+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "üìä Mode: UPDATE_DELETE\n",
            "   ‚Ä¢ MERGE operations applied: DELETEs removed, UPDATEs applied, INSERTs added\n",
            "   ‚Ä¢ _cdc_operation shows: UPSERT (last operation on each row)\n",
            "   ‚Ä¢ Row count = current state (deduplicated)\n",
            "\n",
            "================================================================================\n",
            "üîç SOURCE vs TARGET VERIFICATION\n",
            "================================================================================\n",
            "\n",
            "üîå Using provided CockroachDB connection...\n",
            "‚úÖ Connection ready\n",
            "\n",
            "üìä Source Table (CockroachDB): defaultdb.public.usertable_update_delete_single_cf\n",
            "   Min key: 152\n",
            "   Max key: 199\n",
            "   Count:   48\n",
            "   Sum (ycsb_key): 8424\n",
            "\n",
            "üìä Target Table (Databricks Delta): robert_lee.robert_lee_cockroachdb.usertable_update_delete_single_cf\n",
            "   Min key: 152\n",
            "   Max key: 199\n",
            "   Count:   48\n",
            "   Sum (ycsb_key): 8424\n",
            "\n",
            "üìä Column Sums Comparison (All Fields):\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚úÖ ycsb_key    : Source=               8,424 | Target=               8,424\n",
            "‚úÖ field0      : Source=              82,720 | Target=              82,720\n",
            "‚úÖ field1      : Source=      17,703,392,568 | Target=      17,703,392,568\n",
            "‚úÖ field2      : Source=      17,703,392,616 | Target=      17,703,392,616\n",
            "‚úÖ field3      : Source=              82,861 | Target=              82,861\n",
            "‚úÖ field4      : Source=      17,703,392,712 | Target=      17,703,392,712\n",
            "‚úÖ field5      : Source=              82,955 | Target=              82,955\n",
            "‚úÖ field6      : Source=      17,703,392,808 | Target=      17,703,392,808\n",
            "‚úÖ field7      : Source=              83,049 | Target=              83,049\n",
            "‚úÖ field8      : Source=              83,096 | Target=              83,096\n",
            "‚úÖ field9      : Source=              83,143 | Target=              83,143\n",
            "\n",
            "================================================================================\n",
            "‚úÖ ALL COLUMNS MATCH! Data is in sync.\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Perfect sync! No need for detailed diagnosis.\n",
            "   All statistics and column sums match.\n",
            "   Source rows: 48, Target rows: 48\n",
            "\n",
            "üîå Connection kept open (managed by caller)\n"
          ]
        }
      ],
      "source": [
        "# ALL-IN-ONE CDC DIAGNOSIS\n",
        "\n",
        "# What this does:\n",
        "#   1. CDC Event Summary (replaces Cell 13)\n",
        "#      - Shows total rows, operation breakdown, sample data\n",
        "#   \n",
        "#   2. Source vs Target Verification (replaces Cell 14)\n",
        "#      - Connects to CockroachDB source\n",
        "#      - Auto-deduplicates target for append_only mode\n",
        "#      - Compares column sums\n",
        "#      - Detects mismatches\n",
        "#   \n",
        "#   3. Detailed Diagnosis (automatic if issues found)\n",
        "#      - Column family sync analysis\n",
        "#      - CDC event distribution\n",
        "#      - Row-by-row comparison\n",
        "#      - Troubleshooting recommendations\n",
        "#\n",
        "# Smart behavior:\n",
        "#   ‚úÖ If everything matches ‚Üí Shows \"Perfect sync!\" and exits\n",
        "#   ‚ö†Ô∏è  If mismatches found ‚Üí Automatically runs detailed diagnosis\n",
        "#\n",
        "# No external dependencies - just run this!\n",
        "# ============================================================================\n",
        "\n",
        "import importlib,cockroachdb_ycsb,cockroachdb_debug, cockroachdb_conn\n",
        "importlib.reload(cockroachdb_conn)  # Reload first (cockroachdb_debug depends on it)\n",
        "importlib.reload(cockroachdb_ycsb)  # Reload first (cockroachdb_ycsb depends on it)\n",
        "importlib.reload(cockroachdb_debug)\n",
        "from cockroachdb_debug import run_full_diagnosis_from_config\n",
        "\n",
        "run_full_diagnosis_from_config(conn=conn, spark=spark, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a8f6b5",
      "metadata": {},
      "source": [
        "## Optional: Cleanup\n",
        "\n",
        "Run the cells below if you want to clean up the test resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3c7250ce",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "\n‚ö†Ô∏è  CLEANUP SAFETY STOP\n\nThe cells below will DELETE your resources.\nDo NOT run all cells - run each cleanup cell individually.\n\nüí° TIP: If Cell 13 shows sync issues due to old data,\n   run Cell 19 to clear Azure changefeed data completely.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ‚ö†Ô∏è  SAFETY STOP: Cleanup Section\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This cell prevents accidental cleanup when running \"Run All\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#   - Cell 18: Drop Databricks target table & checkpoint\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#   - Cell 19: Clear Azure changefeed data (optional - use for complete reset)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è  CLEANUP SAFETY STOP\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe cells below will DELETE your resources.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDo NOT run all cells - run each cleanup cell individually.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33müí° TIP: If Cell 13 shows sync issues due to old data,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m   run Cell 19 to clear Azure changefeed data completely.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m )\n",
            "\u001b[31mRuntimeError\u001b[39m: \n‚ö†Ô∏è  CLEANUP SAFETY STOP\n\nThe cells below will DELETE your resources.\nDo NOT run all cells - run each cleanup cell individually.\n\nüí° TIP: If Cell 13 shows sync issues due to old data,\n   run Cell 19 to clear Azure changefeed data completely.\n"
          ]
        }
      ],
      "source": [
        "# ‚ö†Ô∏è  SAFETY STOP: Cleanup Section\n",
        "# This cell prevents accidental cleanup when running \"Run All\"\n",
        "# \n",
        "# To cleanup resources, manually run each cell below INDIVIDUALLY:\n",
        "#   - Cell 16: Cancel changefeed\n",
        "#   - Cell 17: Drop CockroachDB source table  \n",
        "#   - Cell 18: Drop Databricks target table & checkpoint\n",
        "#   - Cell 19: Clear Azure changefeed data (optional - use for complete reset)\n",
        "\n",
        "raise RuntimeError(\n",
        "    \"\\n\"\n",
        "    \"‚ö†Ô∏è  CLEANUP SAFETY STOP\\n\"\n",
        "    \"\\n\"\n",
        "    \"The cells below will DELETE your resources.\\n\"\n",
        "    \"Do NOT run all cells - run each cleanup cell individually.\\n\"\n",
        "    \"\\n\"\n",
        "    \"üí° TIP: If Cell 13 shows sync issues due to old data,\\n\"\n",
        "    \"   run Cell 19 to clear Azure changefeed data completely.\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c68ac58",
      "metadata": {},
      "outputs": [],
      "source": [
        "if conn is None:\n",
        "    conn = get_cockroachdb_connection(\n",
        "        cockroachdb_host=config.cockroachdb.host,\n",
        "        cockroachdb_port=config.cockroachdb.port,\n",
        "        cockroachdb_user=config.cockroachdb.user,\n",
        "        cockroachdb_password=config.cockroachdb.password,\n",
        "        cockroachdb_database=config.cockroachdb.database,\n",
        "        test=False  # Skip test, connection already validated\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8e97da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 1: CANCEL CHANGEFEED(S)\n",
        "from cockroachdb_sql import cancel_changefeeds\n",
        "\n",
        "try:\n",
        "    result = cancel_changefeeds(conn, config)\n",
        "except:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35e74ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 2: DROP SOURCE TABLE (CockroachDB)\n",
        "from cockroachdb_sql import drop_table\n",
        "\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=False  # Skip test, connection already validated\n",
        ")\n",
        "try:\n",
        "    drop_table(conn, config.tables.source_table_name)\n",
        "except:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6924b8f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "importlib.reload(cockroachdb_azure)\n",
        "from cockroachdb_azure import delete_changefeed_files\n",
        "\n",
        "result = delete_changefeed_files(\n",
        "    storage_account_name=config.azure_storage.account_name,\n",
        "    storage_account_key=config.azure_storage.account_key,\n",
        "    container_name=config.azure_storage.container_name,\n",
        "    changefeed_path=config.cdc_config.path  # Uses path from config\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033d5054",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 3: DROP TARGET TABLE & CHECKPOINT (Databricks)\n",
        "# Checkpoint lives on target schema; directory name = table name (same as ingestion Cell 10).\n",
        "from cockroachdb_autoload import _build_paths\n",
        "_, checkpoint_path, target_table_fqn = _build_paths(config, spark=spark)\n",
        "\n",
        "# Drop Delta table\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
        "print(f\"‚úÖ Delta table '{target_table_fqn}' dropped\")\n",
        "\n",
        "# Remove checkpoint\n",
        "try:\n",
        "    dbutils.fs.rm(checkpoint_path, True)\n",
        "    print(f\"‚úÖ Checkpoint '{checkpoint_path}' removed\")\n",
        "except:\n",
        "    print(f\"‚ÑπÔ∏è  Checkpoint not found (may have been already removed)\")\n",
        "\n",
        "print(\"\\n‚úÖ Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53c9621",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 4: Complete cleanup for fresh start\n",
        "\n",
        "# 1. Drop staging table\n",
        "staging_table_fqn = f\"{config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}_staging_cf\"\n",
        "print(f\"üóëÔ∏è  Dropping staging table: {staging_table_fqn}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {staging_table_fqn}\")\n",
        "\n",
        "# 2. Drop target table (if not already done)\n",
        "target_table_fqn = f\"{config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\"\n",
        "print(f\"üóëÔ∏è  Dropping target table: {target_table_fqn}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
        "\n",
        "# 3. Clear checkpoint location (target schema, directory = table name + _merge_cf)\n",
        "from cockroachdb_autoload import _build_paths\n",
        "_, checkpoint_path, _ = _build_paths(config, mode_suffix=\"_merge_cf\", spark=spark)\n",
        "print(f\"üóëÔ∏è  Clearing checkpoint: {checkpoint_path}\")\n",
        "try:\n",
        "    dbutils.fs.rm(checkpoint_path, recurse=True)\n",
        "    print(f\"   ‚úÖ Checkpoint cleared\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ÑπÔ∏è  Checkpoint may not exist: {e}\")\n",
        "\n",
        "# 4. Verify cleanup\n",
        "print(f\"\\n‚úÖ Cleanup complete! Ready for fresh start.\")\n",
        "print(f\"   Next: Re-run Cell 12 (ingestion)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a28f2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the schema\n",
        "print(f\"üìÅ Creating schema: {config.tables.destination_catalog}.{config.tables.destination_schema}\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {config.tables.destination_catalog}.{config.tables.destination_schema}\")\n",
        "print(f\"‚úÖ Schema created\")\n",
        "\n",
        "# Verify schema exists\n",
        "schemas = spark.sql(f\"SHOW SCHEMAS IN {config.tables.destination_catalog}\").collect()\n",
        "schema_names = [row['databaseName'] for row in schemas]\n",
        "if config.tables.destination_schema in schema_names:\n",
        "    print(f\"‚úÖ Verified: Schema {config.tables.destination_schema} exists\")\n",
        "else:\n",
        "    print(f\"‚ùå Schema {config.tables.destination_schema} not found. Available schemas: {schema_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83bdda3d",
      "metadata": {},
      "source": [
        "# Debug Codes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_3.11.3 (3.11.13)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
