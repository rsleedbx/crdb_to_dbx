{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Stream CockroachDB CDC to Databricks (Azure)\n",
        "\n",
        "This notebook demonstrates how to stream CockroachDB changefeeds to Databricks using Azure Blob Storage.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- CockroachDB cluster (Cloud or self-hosted)\n",
        "- Azure Storage Account with hierarchical namespace enabled\n",
        "- Databricks workspace with Unity Catalog\n",
        "- Unity Catalog External Location configured for your storage account\n",
        "\n",
        "**Note:** This notebook uses the **YCSB (Yahoo! Cloud Serving Benchmark)** schema as the default table structure, with `ycsb_key` as the primary key and `field0-9` columns. The default schema name is `public`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c81271",
      "metadata": {},
      "source": [
        "## CDC Mode Selection\n",
        "\n",
        "This notebook supports **4 CDC ingestion modes** by combining two independent settings:\n",
        "\n",
        "### 1. CDC Processing Mode (`cdc_mode`)\n",
        "How CDC events are processed in the target table:\n",
        "\n",
        "- **`append_only`**: Store all CDC events as rows (audit log)\n",
        "  - **Behavior**: All events (INSERT/UPDATE/DELETE) are appended as new rows\n",
        "  - **Use case**: History tracking, time-series analysis, audit logs\n",
        "  - **Storage**: Higher (keeps all historical events)\n",
        "\n",
        "- **`update_delete`**: Apply MERGE logic (current state replication)\n",
        "  - **Behavior**: DELETE removes rows, UPDATE modifies rows in-place\n",
        "  - **Use case**: Current state synchronization, production replication\n",
        "  - **Storage**: Lower (only latest state per key)\n",
        "\n",
        "### 2. Column Family Mode (`column_family_mode`)\n",
        "Table structure and changefeed configuration:\n",
        "\n",
        "- **`single_cf`**: Standard table (1 column family, default)\n",
        "  - **Changefeed**: `split_column_families=false`\n",
        "  - **Files**: 1 Parquet file per CDC event\n",
        "  - **Use case**: Most tables, simpler configuration, better performance\n",
        "\n",
        "- **`multi_cf`**: Multiple column families (for wide tables)\n",
        "  - **Changefeed**: `split_column_families=true`\n",
        "  - **Files**: Multiple Parquet files per CDC event (fragments need merging)\n",
        "  - **Use case**: Wide tables (50+ columns), selective column access patterns\n",
        "\n",
        "### Function Selection Matrix\n",
        "\n",
        "The notebook automatically selects the appropriate ingestion function based on your configuration:\n",
        "\n",
        "| CDC Mode | Column Family Mode | Function Called |\n",
        "|----------|-------------------|-----------------|\n",
        "| `append_only` | `single_cf` | `ingest_cdc_append_only_single_family()` |\n",
        "| `append_only` | `multi_cf` | `ingest_cdc_append_only_multi_family()` |\n",
        "| `update_delete` | `single_cf` | `ingest_cdc_with_merge_single_family()` |\n",
        "| `update_delete` | `multi_cf` | `ingest_cdc_with_merge_multi_family()` |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6d2d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from urllib.parse import quote\n",
        "\n",
        "# Configuration file config.cdc_config.path (adjust as needed)\n",
        "#config_file = \"../.env/cockroachdb_cdc_tutorial_config_append_single_cf.json\"\n",
        "\n",
        "#config_file = \"../.env/cockroachdb_cdc_tutorial_config_append_multi_cf.json\"\n",
        "\n",
        "#config_file = \"../.env/cockroachdb_cdc_tutorial_config_update_delete_multi_cf.json\"\n",
        "\n",
        "config_file = \"../.env/cockroachdb_cdc_tutorial_config_update_delete_single_cf.json\"\n",
        "\n",
        "\n",
        "import importlib\n",
        "import cockroachdb_config\n",
        "importlib.reload(cockroachdb_config)\n",
        "from cockroachdb_config import load_config, process_config\n",
        "\n",
        "# Try to load from file, fallback to embedded config\n",
        "config = load_config(config_file)\n",
        "\n",
        "# Embedded configuration (fallback)\n",
        "if config is None:\n",
        "    config = {\n",
        "      \"cockroachdb\": {\n",
        "        \"host\": \"replace_me\",\n",
        "        \"port\": 26257,\n",
        "        \"user\": \"replace_me\",\n",
        "        \"password\": \"replace_me\",\n",
        "        \"database\": \"defaultdb\"\n",
        "      },\n",
        "      \"cockroachdb_source\": {\n",
        "        \"catalog\": \"defaultdb\",\n",
        "        \"schema\": \"public\",\n",
        "        \"table_name\": \"usertable\",\n",
        "        \"_schema_note\": \"Default schema is 'public'. Table uses YCSB structure (ycsb_key, field0-9)\",\n",
        "      },\n",
        "      \"azure_storage\": {\n",
        "        \"account_name\": \"replace_me\",\n",
        "        \"account_key\": \"replace_me\",\n",
        "        \"config.azure_storage.container_name\": \"changefeed-events\"\n",
        "      },\n",
        "      \"databricks_target\": {\n",
        "        \"catalog\": \"main\",\n",
        "        \"schema\": \"replace_me\",\n",
        "        \"table_name\": \"usertable\",\n",
        "      },\n",
        "      \"cdc_config\": {\n",
        "        \"mode\": \"append_only\",\n",
        "        \"config.cdc_config.column_family_mode\": \"multi_cf\",\n",
        "        \"config.cdc_config.primary_key_columns\": [\"ycsb_key\"],\n",
        "        \"auto_suffix_mode_family\": True,\n",
        "      },\n",
        "    \"uc_external_volume\": {\n",
        "        \"volume_catalog\": \"robert_lee\",\n",
        "        \"volume_full_path\": \"robert_lee.robert_lee_cockroachdb.cockroachdb_cdc_1768934658\",\n",
        "        \"volume_id\": \"de84b515-ec65-4dbc-8a76-460328c6f1b1\",\n",
        "        \"volume_name\": \"cockroachdb_cdc_1768934658\",\n",
        "        \"volume_schema\": \"robert_lee_cockroachdb\"\n",
        "    },       \n",
        "      \"workload_config\": {\n",
        "        \"config.workload_config.snapshot_count\": 10,\n",
        "        \"config.workload_config.insert_count\": 10,\n",
        "        \"config.workload_config.update_count\": 9,\n",
        "        \"config.workload_config.delete_count\": 8,\n",
        "      }\n",
        "    }\n",
        "config=process_config(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pg8000 azure-storage-blob --quiet\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "connect",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import CockroachDB connection utilities\n",
        "import importlib\n",
        "import cockroachdb_conn\n",
        "importlib.reload(cockroachdb_conn)\n",
        "from cockroachdb_conn import get_cockroachdb_connection\n",
        "\n",
        "# Test connection to CockroachDB\n",
        "# The function automatically tests the connection (test=True by default)\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=True  # Automatically tests connection and prints version (default)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Connection function ready for use\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c40c9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import storage utilities (works with both Azure and UC Volume)\n",
        "import importlib\n",
        "import cockroachdb_storage\n",
        "importlib.reload(cockroachdb_storage)\n",
        "from cockroachdb_storage import check_files, wait_for_files\n",
        "\n",
        "# Import YCSB utility functions\n",
        "import cockroachdb_ycsb\n",
        "importlib.reload(cockroachdb_ycsb)\n",
        "from cockroachdb_ycsb import (\n",
        "    get_table_stats,\n",
        "    get_table_stats_spark,\n",
        "    get_column_sum,\n",
        "    get_column_sum_spark,\n",
        "    deduplicate_to_latest,\n",
        "    get_column_sum_spark_deduplicated\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded (CockroachDB & Azure)\")\n",
        "print(\"‚úÖ YCSB utility functions imported from cockroachdb_ycsb.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_table",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create table using cockroachdb_ycsb.py\n",
        "# Import YCSB functions\n",
        "import importlib, cockroachdb_ycsb\n",
        "importlib.reload(cockroachdb_ycsb)\n",
        "from cockroachdb_ycsb import create_ycsb_table\n",
        "\n",
        "# Create table\n",
        "try:\n",
        "    create_ycsb_table(\n",
        "        conn=conn,\n",
        "        table_name=config.tables.source_table_name,\n",
        "        column_family_mode=config.cdc_config.column_family_mode\n",
        "    )\n",
        "except:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "insert_data",
      "metadata": {},
      "outputs": [],
      "source": [
        "from cockroachdb_ycsb import insert_ycsb_snapshot_with_random_nulls\n",
        "\n",
        "try:\n",
        "    insert_ycsb_snapshot_with_random_nulls(\n",
        "        conn=conn,\n",
        "        table_name=config.tables.source_table_name,\n",
        "        snapshot_count=config.workload_config.snapshot_count,\n",
        "        null_probability=0.3,  # 30% chance of NULL in snapshot\n",
        "        columns_to_randomize=['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9'],  # ALL fields\n",
        "        seed=42,  # Reproducible random NULLs\n",
        "        force_all_null_row=True  # Row 0 will have all randomized columns as NULL (edge case testing)\n",
        "    )\n",
        "except:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1c41ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "from cockroachdb_sql import create_changefeed_from_config\n",
        "\n",
        "try:\n",
        "    result = create_changefeed_from_config(conn, config, spark)\n",
        "    \n",
        "    if result['created']:\n",
        "        print(f\"New changefeed: Job {result['job_id']}\")\n",
        "    else:\n",
        "        print(f\"Using existing: {result['existing_count']} found\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_workload",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Capture baseline file count BEFORE generating CDC events\n",
        "print(\"üìä Capturing baseline file count...\")\n",
        "result_before = check_files(\n",
        "    config=config,\n",
        "    spark=spark,\n",
        "    verbose=False\n",
        ")\n",
        "files_before = len(result_before['data_files'])\n",
        "resolved_before = len(result_before['resolved_files'])\n",
        "print(f\"   Data files: {files_before}\")\n",
        "print(f\"   Resolved files: {resolved_before}\")\n",
        "print()\n",
        "\n",
        "# Run workload with NULL testing using cockroachdb_ycsb.py\n",
        "from cockroachdb_ycsb import run_ycsb_workload_with_random_nulls\n",
        "\n",
        "# Run workload - connection is managed by notebook, not closed here\n",
        "run_ycsb_workload_with_random_nulls(\n",
        "    conn=conn,\n",
        "    table_name=config.tables.source_table_name,\n",
        "    insert_count=config.workload_config.insert_count,\n",
        "    update_count=config.workload_config.update_count,\n",
        "    delete_count=config.workload_config.delete_count,\n",
        "    null_probability=0.5,  # 50% chance of NULL in UPDATEs\n",
        "    columns_to_randomize=['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9'],  # ALL fields\n",
        "    seed=42,  # Reproducible random NULLs\n",
        "    force_all_null_update=True  # First UPDATE will have all NULLs (edge case testing)\n",
        ")\n",
        "\n",
        "# Wait for new CDC files to appear in storage (positive confirmation)\n",
        "storage_label = \"Unity Catalog Volume\" if config.data_source == \"uc_external_volume\" else \"Azure\"\n",
        "print(f\"\")\n",
        "print(f\"‚è≥ Waiting for new CDC files to appear in {storage_label}...\")\n",
        "print(f\"   Baseline: {files_before} data files, {resolved_before} resolved files\")\n",
        "print()\n",
        "\n",
        "# Poll for new files (max 90 seconds)\n",
        "max_wait = 90\n",
        "check_interval = 10\n",
        "elapsed = 0\n",
        "\n",
        "while elapsed < max_wait:\n",
        "    result = check_files(\n",
        "        config=config,\n",
        "        spark=spark,\n",
        "        verbose=False\n",
        "    )\n",
        "    files_now = len(result['data_files'])\n",
        "    resolved_now = len(result['resolved_files'])\n",
        "    \n",
        "    if files_now > files_before or resolved_now > resolved_before:\n",
        "        print(f\"‚úÖ New CDC files appeared after {elapsed} seconds!\")\n",
        "        print(f\"   Data files: {files_before} ‚Üí {files_now} (+{files_now - files_before})\")\n",
        "        print(f\"   Resolved files: {resolved_before} ‚Üí {resolved_now} (+{resolved_now - resolved_before})\")\n",
        "        break\n",
        "    \n",
        "    print(f\"   Checking... ({elapsed}s elapsed, baseline: {files_before} data, {resolved_before} resolved)\", end='\\r')\n",
        "    time.sleep(check_interval)\n",
        "    elapsed += check_interval\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - files may still be flushing\")\n",
        "    print(f\"   Run Cell 11 to check manually\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check_files",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the unified storage function to check for files (works with both Azure and UC Volume)\n",
        "result = check_files(\n",
        "    config=config,\n",
        "    spark=spark,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Provide guidance\n",
        "if len(result['data_files']) == 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  No data files found yet.\")\n",
        "    print(f\"   üí° Possible reasons:\")\n",
        "    print(f\"   - Changefeed not created yet (run Cell 9)\")\n",
        "    print(f\"   - Path configuration mismatch (check Cell 1 variables)\")\n",
        "    print(f\"   - Azure credentials issue (check External Location)\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Files are ready! Proceed to Cell 10 to read with Databricks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "read_cdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import CDC ingestion functions from cockroachdb_autoload.py\n",
        "import importlib, cockroachdb_autoload\n",
        "importlib.reload(cockroachdb_autoload)\n",
        "from cockroachdb_autoload import (\n",
        "    ingest_cdc_append_only_single_family,\n",
        "    ingest_cdc_append_only_multi_family,\n",
        "    ingest_cdc_with_merge_single_family,\n",
        "    ingest_cdc_with_merge_multi_family\n",
        ")\n",
        "\n",
        "print(f\"üî∑ CDC Configuration:\")\n",
        "print(f\"   Processing Mode: {config.cdc_config.mode}\")\n",
        "print(f\"   Column Family Mode: {config.cdc_config.column_family_mode}\")\n",
        "print(f\"   Data Source: {config.data_source}\")\n",
        "print()\n",
        "\n",
        "# Select function based on BOTH config.cdc_config.mode and config.cdc_config.column_family_mode\n",
        "if config.cdc_config.mode == \"append_only\" and config.cdc_config.column_family_mode == \"single_cf\":\n",
        "    print(f\"üìò Running: ingest_cdc_append_only_single_family()\")\n",
        "    print(f\"   - All CDC events will be stored as rows\")\n",
        "    print(f\"   - No column family merging needed\\n\")\n",
        "    \n",
        "    query = ingest_cdc_append_only_single_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "\n",
        "elif config.cdc_config.mode == \"append_only\" and config.cdc_config.column_family_mode == \"multi_cf\":\n",
        "    print(f\"üìô Running: ingest_cdc_append_only_multi_family()\")\n",
        "    print(f\"   - All CDC events will be stored as rows\")\n",
        "    print(f\"   - Column family fragments will be merged\\n\")\n",
        "    \n",
        "    if not config.cdc_config.primary_key_columns:\n",
        "        raise ValueError(\"config.cdc_config.primary_key_columns required for multi_cf mode\")\n",
        "    \n",
        "    query = ingest_cdc_append_only_multi_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "\n",
        "elif config.cdc_config.mode == \"update_delete\" and config.cdc_config.column_family_mode == \"single_cf\":\n",
        "    print(f\"üìó Running: ingest_cdc_with_merge_single_family()\")\n",
        "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
        "    print(f\"   - No column family merging needed\\n\")\n",
        "    \n",
        "    if not config.cdc_config.primary_key_columns:\n",
        "        raise ValueError(\"config.cdc_config.primary_key_columns required for update_delete mode\")\n",
        "    \n",
        "    result = ingest_cdc_with_merge_single_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "    \n",
        "    query = result[\"query\"]\n",
        "\n",
        "elif config.cdc_config.mode == \"update_delete\" and config.cdc_config.column_family_mode == \"multi_cf\":\n",
        "    print(f\"üìï Running: ingest_cdc_with_merge_multi_family()\")\n",
        "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
        "    print(f\"   - Column family fragments will be merged\\n\")\n",
        "    \n",
        "    if not config.cdc_config.primary_key_columns:\n",
        "        raise ValueError(\"config.cdc_config.primary_key_columns required for update_delete + multi_cf mode\")\n",
        "    \n",
        "    result = ingest_cdc_with_merge_multi_family(\n",
        "        config=config,\n",
        "        spark=spark\n",
        "    )\n",
        "    \n",
        "    query = result[\"query\"]\n",
        "\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Invalid mode combination:\\n\"\n",
        "        f\"  cdc_mode='{config.cdc_config.mode}' (valid: 'append_only', 'update_delete')\\n\"\n",
        "        f\"  column_family_mode='{config.cdc_config.column_family_mode}' (valid: 'single_cf', 'multi_cf')\\n\"\n",
        "        f\"Change modes in Cell 1.\"\n",
        "    )\n",
        "\n",
        "# Wait for completion (if not already complete)\n",
        "if config.cdc_config.mode == \"append_only\":\n",
        "    query.awaitTermination()\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"‚úÖ CDC INGESTION COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"   Mode: {config.cdc_config.mode} + {config.cdc_config.column_family_mode}\")\n",
        "    print(f\"   Target: {config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\")\n",
        "    print()\n",
        "    print(f\"üìä Query your data: SELECT * FROM {config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\")\n",
        "else:\n",
        "    # update_delete mode already completed inside the function\n",
        "    print(f\"üìä Query your data: SELECT * FROM {config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df3f327",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALL-IN-ONE CDC DIAGNOSIS\n",
        "\n",
        "# What this does:\n",
        "#   1. CDC Event Summary (replaces Cell 13)\n",
        "#      - Shows total rows, operation breakdown, sample data\n",
        "#   \n",
        "#   2. Source vs Target Verification (replaces Cell 14)\n",
        "#      - Connects to CockroachDB source\n",
        "#      - Auto-deduplicates target for append_only mode\n",
        "#      - Compares column sums\n",
        "#      - Detects mismatches\n",
        "#   \n",
        "#   3. Detailed Diagnosis (automatic if issues found)\n",
        "#      - Column family sync analysis\n",
        "#      - CDC event distribution\n",
        "#      - Row-by-row comparison\n",
        "#      - Troubleshooting recommendations\n",
        "#\n",
        "# Smart behavior:\n",
        "#   ‚úÖ If everything matches ‚Üí Shows \"Perfect sync!\" and exits\n",
        "#   ‚ö†Ô∏è  If mismatches found ‚Üí Automatically runs detailed diagnosis\n",
        "#\n",
        "# No external dependencies - just run this!\n",
        "# ============================================================================\n",
        "\n",
        "import importlib,cockroachdb_ycsb,cockroachdb_debug, cockroachdb_conn\n",
        "importlib.reload(cockroachdb_conn)  # Reload first (cockroachdb_debug depends on it)\n",
        "importlib.reload(cockroachdb_ycsb)  # Reload first (cockroachdb_ycsb depends on it)\n",
        "importlib.reload(cockroachdb_debug)\n",
        "from cockroachdb_debug import run_full_diagnosis_from_config\n",
        "\n",
        "run_full_diagnosis_from_config(conn=conn, spark=spark, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a8f6b5",
      "metadata": {},
      "source": [
        "## Optional: Cleanup\n",
        "\n",
        "Run the cells below if you want to clean up the test resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7250ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è  SAFETY STOP: Cleanup Section\n",
        "# This cell prevents accidental cleanup when running \"Run All\"\n",
        "# \n",
        "# To cleanup resources, manually run each cell below INDIVIDUALLY:\n",
        "#   - Cell 16: Cancel changefeed\n",
        "#   - Cell 17: Drop CockroachDB source table  \n",
        "#   - Cell 18: Drop Databricks target table & checkpoint\n",
        "#   - Cell 19: Clear Azure changefeed data (optional - use for complete reset)\n",
        "\n",
        "raise RuntimeError(\n",
        "    \"\\n\"\n",
        "    \"‚ö†Ô∏è  CLEANUP SAFETY STOP\\n\"\n",
        "    \"\\n\"\n",
        "    \"The cells below will DELETE your resources.\\n\"\n",
        "    \"Do NOT run all cells - run each cleanup cell individually.\\n\"\n",
        "    \"\\n\"\n",
        "    \"üí° TIP: If Cell 13 shows sync issues due to old data,\\n\"\n",
        "    \"   run Cell 19 to clear Azure changefeed data completely.\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c68ac58",
      "metadata": {},
      "outputs": [],
      "source": [
        "if conn is None:\n",
        "    conn = get_cockroachdb_connection(\n",
        "        cockroachdb_host=config.cockroachdb.host,\n",
        "        cockroachdb_port=config.cockroachdb.port,\n",
        "        cockroachdb_user=config.cockroachdb.user,\n",
        "        cockroachdb_password=config.cockroachdb.password,\n",
        "        cockroachdb_database=config.cockroachdb.database,\n",
        "        test=False  # Skip test, connection already validated\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8e97da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 1: CANCEL CHANGEFEED(S)\n",
        "from cockroachdb_sql import cancel_changefeeds\n",
        "\n",
        "try:\n",
        "    result = cancel_changefeeds(conn, config)\n",
        "except:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35e74ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 2: DROP SOURCE TABLE (CockroachDB)\n",
        "from cockroachdb_sql import drop_table\n",
        "\n",
        "conn = get_cockroachdb_connection(\n",
        "    cockroachdb_host=config.cockroachdb.host,\n",
        "    cockroachdb_port=config.cockroachdb.port,\n",
        "    cockroachdb_user=config.cockroachdb.user,\n",
        "    cockroachdb_password=config.cockroachdb.password,\n",
        "    cockroachdb_database=config.cockroachdb.database,\n",
        "    test=False  # Skip test, connection already validated\n",
        ")\n",
        "try:\n",
        "    drop_table(conn, config.tables.source_table_name)\n",
        "except:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6924b8f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "importlib.reload(cockroachdb_azure)\n",
        "from cockroachdb_azure import delete_changefeed_files\n",
        "\n",
        "result = delete_changefeed_files(\n",
        "    storage_account_name=config.azure_storage.account_name,\n",
        "    storage_account_key=config.azure_storage.account_key,\n",
        "    container_name=config.azure_storage.container_name,\n",
        "    changefeed_path=config.cdc_config.path  # Uses path from config\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033d5054",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 3: DROP TARGET TABLE & CHECKPOINT (Databricks)\n",
        "# Checkpoint lives on target schema; directory name = table name (same as ingestion Cell 10).\n",
        "from cockroachdb_autoload import _build_paths\n",
        "_, checkpoint_path, target_table_fqn = _build_paths(config, spark=spark)\n",
        "\n",
        "# Drop Delta table\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
        "print(f\"‚úÖ Delta table '{target_table_fqn}' dropped\")\n",
        "\n",
        "# Remove checkpoint\n",
        "try:\n",
        "    dbutils.fs.rm(checkpoint_path, True)\n",
        "    print(f\"‚úÖ Checkpoint '{checkpoint_path}' removed\")\n",
        "except:\n",
        "    print(f\"‚ÑπÔ∏è  Checkpoint not found (may have been already removed)\")\n",
        "\n",
        "print(\"\\n‚úÖ Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53c9621",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 4: Complete cleanup for fresh start\n",
        "\n",
        "# 1. Drop staging table\n",
        "staging_table_fqn = f\"{config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}_staging_cf\"\n",
        "print(f\"üóëÔ∏è  Dropping staging table: {staging_table_fqn}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {staging_table_fqn}\")\n",
        "\n",
        "# 2. Drop target table (if not already done)\n",
        "target_table_fqn = f\"{config.tables.destination_catalog}.{config.tables.destination_schema}.{config.tables.destination_table_name}\"\n",
        "print(f\"üóëÔ∏è  Dropping target table: {target_table_fqn}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
        "\n",
        "# 3. Clear checkpoint location (target schema, directory = table name + _merge_cf)\n",
        "from cockroachdb_autoload import _build_paths\n",
        "_, checkpoint_path, _ = _build_paths(config, mode_suffix=\"_merge_cf\", spark=spark)\n",
        "print(f\"üóëÔ∏è  Clearing checkpoint: {checkpoint_path}\")\n",
        "try:\n",
        "    dbutils.fs.rm(checkpoint_path, recurse=True)\n",
        "    print(f\"   ‚úÖ Checkpoint cleared\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ÑπÔ∏è  Checkpoint may not exist: {e}\")\n",
        "\n",
        "# 4. Verify cleanup\n",
        "print(f\"\\n‚úÖ Cleanup complete! Ready for fresh start.\")\n",
        "print(f\"   Next: Re-run Cell 12 (ingestion)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a28f2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the schema\n",
        "print(f\"üìÅ Creating schema: {config.tables.destination_catalog}.{config.tables.destination_schema}\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {config.tables.destination_catalog}.{config.tables.destination_schema}\")\n",
        "print(f\"‚úÖ Schema created\")\n",
        "\n",
        "# Verify schema exists\n",
        "schemas = spark.sql(f\"SHOW SCHEMAS IN {config.tables.destination_catalog}\").collect()\n",
        "schema_names = [row['databaseName'] for row in schemas]\n",
        "if config.tables.destination_schema in schema_names:\n",
        "    print(f\"‚úÖ Verified: Schema {config.tables.destination_schema} exists\")\n",
        "else:\n",
        "    print(f\"‚ùå Schema {config.tables.destination_schema} not found. Available schemas: {schema_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83bdda3d",
      "metadata": {},
      "source": [
        "# Debug Codes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_3.11.3 (3.11.13)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
