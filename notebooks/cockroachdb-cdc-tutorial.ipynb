{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Stream CockroachDB CDC to Databricks (Azure)\n",
        "\n",
        "This notebook demonstrates how to stream CockroachDB changefeeds to Databricks using Azure Blob Storage.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- CockroachDB cluster (Cloud or self-hosted)\n",
        "- Azure Storage Account with hierarchical namespace enabled\n",
        "- Databricks workspace with Unity Catalog\n",
        "- Unity Catalog External Location configured for your storage account\n",
        "\n",
        "**Note:** This notebook uses the **YCSB (Yahoo! Cloud Serving Benchmark)** schema as the default table structure, with `ycsb_key` as the primary key and `field0-9` columns. The default schema name is `public`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c81271",
      "metadata": {},
      "source": [
        "## CDC Mode Selection\n",
        "\n",
        "This notebook supports **4 CDC ingestion modes** by combining two independent settings:\n",
        "\n",
        "### 1. CDC Processing Mode (`cdc_mode`)\n",
        "How CDC events are processed in the target table:\n",
        "\n",
        "- **`append_only`**: Store all CDC events as rows (audit log)\n",
        "  - **Behavior**: All events (INSERT/UPDATE/DELETE) are appended as new rows\n",
        "  - **Use case**: History tracking, time-series analysis, audit logs\n",
        "  - **Storage**: Higher (keeps all historical events)\n",
        "\n",
        "- **`update_delete`**: Apply MERGE logic (current state replication)\n",
        "  - **Behavior**: DELETE removes rows, UPDATE modifies rows in-place\n",
        "  - **Use case**: Current state synchronization, production replication\n",
        "  - **Storage**: Lower (only latest state per key)\n",
        "\n",
        "### 2. Column Family Mode (`column_family_mode`)\n",
        "Table structure and changefeed configuration:\n",
        "\n",
        "- **`single_cf`**: Standard table (1 column family, default)\n",
        "  - **Changefeed**: `split_column_families=false`\n",
        "  - **Files**: 1 Parquet file per CDC event\n",
        "  - **Use case**: Most tables, simpler configuration, better performance\n",
        "\n",
        "- **`multi_cf`**: Multiple column families (for wide tables)\n",
        "  - **Changefeed**: `split_column_families=true`\n",
        "  - **Files**: Multiple Parquet files per CDC event (fragments need merging)\n",
        "  - **Use case**: Wide tables (50+ columns), selective column access patterns\n",
        "\n",
        "### Function Selection Matrix\n",
        "\n",
        "The notebook automatically selects the appropriate ingestion function based on your configuration:\n",
        "\n",
        "| CDC Mode | Column Family Mode | Function Called |\n",
        "|----------|-------------------|-----------------|\n",
        "| `append_only` | `single_cf` | `ingest_cdc_append_only_single_family()` |\n",
        "| `append_only` | `multi_cf` | `ingest_cdc_append_only_multi_family()` |\n",
        "| `update_delete` | `single_cf` | `ingest_cdc_with_merge_single_family()` |\n",
        "| `update_delete` | `multi_cf` | `ingest_cdc_with_merge_multi_family()` |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from urllib.parse import quote\n",
        "\n",
        "# Configuration file path (adjust as needed)\n",
        "config_file = \"cockroachdb_cdc_tutorial_config_append_single_cf.json\"\n",
        "\n",
        "#config_file = \"cockroachdb_cdc_tutorial_config_append_multi_cf.json\"\n",
        "\n",
        "#config_file = \"cockroachdb_cdc_tutorial_config_update_delete_multi_cf.json\"\n",
        "\n",
        "#config_file = \"cockroachdb_cdc_tutorial_config_update_delete_single_cf.json\"\n",
        "\n",
        "\n",
        "# Try to load from file, fallback to embedded config\n",
        "try:\n",
        "    with open(config_file, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    print(f\"‚úÖ Configuration loaded from: {config_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ÑπÔ∏è  Using embedded configuration (config file error: {e})\")\n",
        "    config = None\n",
        "\n",
        "# Embedded configuration (fallback)\n",
        "if config is None:\n",
        "    config = {\n",
        "      \"cockroachdb\": {\n",
        "        \"host\": \"replace_me\",\n",
        "        \"port\": 26257,\n",
        "        \"user\": \"replace_me\",\n",
        "        \"password\": \"replace_me\",\n",
        "        \"database\": \"defaultdb\"\n",
        "      },\n",
        "      \"cockroachdb_source\": {\n",
        "        \"catalog\": \"defaultdb\",\n",
        "        \"schema\": \"public\",\n",
        "        \"table_name\": \"usertable\",\n",
        "        \"_schema_note\": \"Default schema is 'public'. Table uses YCSB structure (ycsb_key, field0-9)\",\n",
        "      },\n",
        "      \"azure_storage\": {\n",
        "        \"account_name\": \"replace_me\",\n",
        "        \"account_key\": \"replace_me\",\n",
        "        \"container_name\": \"changefeed-events\"\n",
        "      },\n",
        "      \"databricks_target\": {\n",
        "        \"catalog\": \"main\",\n",
        "        \"schema\": \"replace_me\",\n",
        "        \"table_name\": \"usertable\",\n",
        "      },\n",
        "      \"cdc_config\": {\n",
        "        \"mode\": \"append_only\",\n",
        "        \"column_family_mode\": \"multi_cf\",\n",
        "        \"primary_key_columns\": [\"ycsb_key\"],\n",
        "        \"auto_suffix_mode_family\": True,\n",
        "      },\n",
        "      \"workload_config\": {\n",
        "        \"snapshot_count\": 10,\n",
        "        \"insert_count\": 10,\n",
        "        \"update_count\": 9,\n",
        "        \"delete_count\": 8,\n",
        "      }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c870de83",
      "metadata": {},
      "outputs": [],
      "source": [
        "from urllib.parse import quote\n",
        "\n",
        "# Extract configuration values\n",
        "cockroachdb_host = config[\"cockroachdb\"][\"host\"]\n",
        "cockroachdb_port = config[\"cockroachdb\"][\"port\"]\n",
        "cockroachdb_user = config[\"cockroachdb\"][\"user\"]\n",
        "cockroachdb_password = config[\"cockroachdb\"][\"password\"]\n",
        "cockroachdb_database = config[\"cockroachdb\"][\"database\"]\n",
        "\n",
        "source_catalog = config[\"cockroachdb_source\"][\"catalog\"]\n",
        "source_schema = config[\"cockroachdb_source\"][\"schema\"]\n",
        "source_table = config[\"cockroachdb_source\"][\"table_name\"]\n",
        "\n",
        "storage_account_name = config[\"azure_storage\"][\"account_name\"]\n",
        "storage_account_key = config[\"azure_storage\"][\"account_key\"]\n",
        "storage_account_key_encoded = quote(storage_account_key, safe='')\n",
        "container_name = config[\"azure_storage\"][\"container_name\"]\n",
        "\n",
        "target_catalog = config[\"databricks_target\"][\"catalog\"]\n",
        "target_schema = config[\"databricks_target\"][\"schema\"]\n",
        "target_table = config[\"databricks_target\"][\"table_name\"]\n",
        "\n",
        "cdc_mode = config[\"cdc_config\"][\"mode\"]\n",
        "column_family_mode = config[\"cdc_config\"][\"column_family_mode\"]\n",
        "primary_key_columns = config[\"cdc_config\"][\"primary_key_columns\"]\n",
        "\n",
        "snapshot_count = config[\"workload_config\"][\"snapshot_count\"]\n",
        "insert_count = config[\"workload_config\"][\"insert_count\"]\n",
        "update_count = config[\"workload_config\"][\"update_count\"]\n",
        "delete_count = config[\"workload_config\"][\"delete_count\"]\n",
        "\n",
        "# Auto-suffix table names with mode and column family if enabled\n",
        "auto_suffix = config[\"cdc_config\"].get(\"auto_suffix_mode_family\", False)\n",
        "if auto_suffix:\n",
        "    suffix = f\"_{cdc_mode}_{column_family_mode}\"\n",
        "    \n",
        "    # Add suffix to source_table if not already present\n",
        "    if not source_table.endswith(suffix):\n",
        "        source_table = f\"{source_table}{suffix}\"\n",
        "    \n",
        "    # Add suffix to target_table if not already present\n",
        "    if not target_table.endswith(suffix):\n",
        "        target_table = f\"{target_table}{suffix}\"\n",
        "\n",
        "    # Update config dict with suffixed table names\n",
        "    config[\"cockroachdb_source\"][\"table_name\"] = source_table\n",
        "    config[\"databricks_target\"][\"table_name\"] = target_table\n",
        "\n",
        "# Extract format for reuse (default: parquet)\n",
        "cdc_format = config[\"cdc_config\"].get(\"format\", \"parquet\")\n",
        "\n",
        "# set the path in azure\n",
        "path = f\"{cdc_format}/{source_catalog}/{source_schema}/{source_table}/{target_table}\"\n",
        "config[\"cdc_config\"][\"path\"] = path\n",
        "\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"   CDC Processing Mode: {cdc_mode}\")\n",
        "print(f\"   Column Family Mode: {column_family_mode}\")\n",
        "print(f\"   Primary Keys: {primary_key_columns}\")\n",
        "print(f\"   Target Table: {target_table}\")\n",
        "print(f\"   CDC Workload: {snapshot_count} snapshot ‚Üí +{insert_count} INSERTs, ~{update_count} UPDATEs, -{delete_count} DELETEs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pg8000 azure-storage-blob --quiet\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "connect",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import CockroachDB connection utilities\n",
        "import importlib\n",
        "import cockroachdb_conn\n",
        "importlib.reload(cockroachdb_conn)\n",
        "from cockroachdb_conn import get_cockroachdb_connection as _get_connection\n",
        "\n",
        "# Wrapper function that uses config variables from Cell 3\n",
        "def get_cockroachdb_connection():\n",
        "    \"\"\"Create connection to CockroachDB using config from Cell 3\"\"\"\n",
        "    return _get_connection(\n",
        "        cockroachdb_host=cockroachdb_host,\n",
        "        cockroachdb_port=cockroachdb_port,\n",
        "        cockroachdb_user=cockroachdb_user,\n",
        "        cockroachdb_password=cockroachdb_password,\n",
        "        cockroachdb_database=cockroachdb_database\n",
        "    )\n",
        "\n",
        "# Test connection\n",
        "try:\n",
        "    conn = get_cockroachdb_connection()\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(\"SELECT version()\")\n",
        "        version = cur.fetchone()[0]\n",
        "    conn.close()\n",
        "    \n",
        "    print(\"‚úÖ Connected to CockroachDB\")\n",
        "    print(f\"   Version: {version[:50]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c40c9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Azure utilities\n",
        "import importlib\n",
        "import cockroachdb_azure\n",
        "importlib.reload(cockroachdb_azure)\n",
        "from cockroachdb_azure import check_azure_files, wait_for_changefeed_files\n",
        "\n",
        "# Import YCSB utility functions\n",
        "import cockroachdb_ycsb\n",
        "importlib.reload(cockroachdb_ycsb)\n",
        "from cockroachdb_ycsb import (\n",
        "    get_table_stats,\n",
        "    get_table_stats_spark,\n",
        "    get_column_sum,\n",
        "    get_column_sum_spark,\n",
        "    deduplicate_to_latest,\n",
        "    get_column_sum_spark_deduplicated\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded (CockroachDB & Azure)\")\n",
        "print(\"‚úÖ YCSB utility functions imported from cockroachdb_ycsb.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_table",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create table using cockroachdb_ycsb.py\n",
        "# Import YCSB functions\n",
        "import importlib, cockroachdb_ycsb\n",
        "importlib.reload(cockroachdb_ycsb)\n",
        "from cockroachdb_ycsb import create_ycsb_table\n",
        "\n",
        "# Create table\n",
        "conn = get_cockroachdb_connection()\n",
        "try:\n",
        "    create_ycsb_table(\n",
        "        conn=conn,\n",
        "        table_name=source_table,\n",
        "        column_family_mode=column_family_mode\n",
        "    )\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "insert_data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert snapshot data with NULL testing using cockroachdb_ycsb.py\n",
        "from cockroachdb_ycsb import insert_ycsb_snapshot_with_random_nulls\n",
        "\n",
        "conn = get_cockroachdb_connection()\n",
        "try:\n",
        "    insert_ycsb_snapshot_with_random_nulls(\n",
        "        conn=conn,\n",
        "        table_name=source_table,\n",
        "        snapshot_count=snapshot_count,\n",
        "        null_probability=0.3,  # 30% chance of NULL in snapshot\n",
        "        columns_to_randomize=['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9'],  # ALL fields\n",
        "        seed=42,  # Reproducible random NULLs\n",
        "        force_all_null_row=True  # Row 0 will have all randomized columns as NULL (edge case testing)\n",
        "    )\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_changefeed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Azure Blob Storage URI with table-specific path\n",
        "# Note: For Azure, path goes in URI (not as path_prefix query parameter like S3)\n",
        "changefeed_path = f\"azure://{container_name}/{path}?AZURE_ACCOUNT_NAME={storage_account_name}&AZURE_ACCOUNT_KEY={storage_account_key_encoded}\"\n",
        "\n",
        "# Build changefeed options based on column_family_mode\n",
        "if column_family_mode == \"multi_cf\":\n",
        "    # Include split_column_families for multi-family mode\n",
        "    changefeed_options = \"\"\"\n",
        "    format='parquet',\n",
        "    updated,\n",
        "    resolved='10s',\n",
        "    split_column_families\n",
        "\"\"\"\n",
        "else:\n",
        "    # Standard options for single-family mode\n",
        "    changefeed_options = \"\"\"\n",
        "    format='parquet',\n",
        "    updated,\n",
        "    resolved='10s'\n",
        "\"\"\"\n",
        "\n",
        "# Create changefeed SQL\n",
        "create_changefeed_sql = f\"\"\"\n",
        "CREATE CHANGEFEED FOR TABLE {source_table}\n",
        "INTO '{changefeed_path}'\n",
        "WITH {changefeed_options}\n",
        "\"\"\"\n",
        "\n",
        "conn = get_cockroachdb_connection()\n",
        "try:\n",
        "    with conn.cursor() as cur:\n",
        "        # Check for existing changefeeds by matching the sink_uri\n",
        "        # This is more exact than parsing the description field\n",
        "        # Match pattern: azure://{container}/{path}?...\n",
        "        # Note: We check for ALL matches (no LIMIT) to detect duplicates\n",
        "        sink_uri_pattern = f\"%{container_name}/{path}%\"\n",
        "        \n",
        "        cur.execute(\"\"\"\n",
        "            SELECT job_id, status, sink_uri\n",
        "            FROM [SHOW CHANGEFEED JOBS] \n",
        "            WHERE sink_uri LIKE %s\n",
        "            AND status IN ('running', 'paused')\n",
        "        \"\"\", (sink_uri_pattern,))\n",
        "        \n",
        "        existing_changefeeds = cur.fetchall()\n",
        "        \n",
        "        if existing_changefeeds:\n",
        "            print(f\"‚úÖ Changefeed(s) already exist for this source ‚Üí target mapping\")\n",
        "            print(f\"   Found {len(existing_changefeeds)} changefeed(s):\")\n",
        "            for job_id, status, sink_uri in existing_changefeeds:\n",
        "                print(f\"   ‚Ä¢ Job ID: {job_id}, Status: {status}\")\n",
        "                print(f\"     Sink URI: {sink_uri[:80]}...\")  # Show first 80 chars (redacted credentials)\n",
        "            if len(existing_changefeeds) > 1:\n",
        "                print(f\"\\n‚ö†Ô∏è  WARNING: Multiple changefeeds detected for same destination!\")\n",
        "                print(f\"   This may cause duplicate data. Consider running Cell 17 to clean up.\")\n",
        "            if column_family_mode == \"multi_cf\":\n",
        "                print(f\"\\n   Expected: Column family fragments\")\n",
        "            print(f\"\\nüí° Tip: Run Cell 10 to generate UPDATE/DELETE events\")\n",
        "            print(f\"   Then check Cell 11 to verify new files appear\")\n",
        "        else:\n",
        "            # Create new changefeed\n",
        "            cur.execute(create_changefeed_sql)\n",
        "            result = cur.fetchone()\n",
        "            job_id = result[0]\n",
        "            \n",
        "            print(f\"‚úÖ Changefeed created\")\n",
        "            print(f\"   Job ID: {job_id}\")\n",
        "            print(f\"   Source: {source_catalog}.{source_schema}.{source_table}\")\n",
        "            print(f\"   Target path: .../{source_table}/{target_table}/\")\n",
        "            print(f\"   Format: Parquet\")\n",
        "            if column_family_mode == \"multi_cf\":\n",
        "                print(f\"   Split column families: TRUE (fragments will be generated)\")\n",
        "            else:\n",
        "                print(f\"   Split column families: FALSE (single file per event)\")\n",
        "            print(f\"   Destination: Azure Blob Storage\")\n",
        "            print(f\"\")\n",
        "            \n",
        "            # Wait for files to appear using helper function\n",
        "            wait_for_changefeed_files(\n",
        "                storage_account_name, storage_account_key, container_name,\n",
        "                source_catalog, source_schema, source_table, target_table,\n",
        "                max_wait=300, check_interval=5,\n",
        "                format=cdc_format\n",
        "            )\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_workload",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Capture baseline file count BEFORE generating CDC events\n",
        "print(\"üìä Capturing baseline file count...\")\n",
        "result_before = check_azure_files(\n",
        "    storage_account_name, storage_account_key, container_name,\n",
        "    source_catalog, source_schema, source_table, target_table,\n",
        "    verbose=False,\n",
        "    format=cdc_format\n",
        ")\n",
        "files_before = len(result_before['data_files'])\n",
        "print(f\"   Current files: {files_before}\")\n",
        "print()\n",
        "\n",
        "# Run workload with NULL testing using cockroachdb_ycsb.py\n",
        "from cockroachdb_ycsb import run_ycsb_workload_with_random_nulls\n",
        "\n",
        "conn = get_cockroachdb_connection()\n",
        "try:\n",
        "    run_ycsb_workload_with_random_nulls(\n",
        "        conn=conn,\n",
        "        table_name=source_table,\n",
        "        insert_count=insert_count,\n",
        "        update_count=update_count,\n",
        "        delete_count=delete_count,\n",
        "        null_probability=0.5,  # 50% chance of NULL in UPDATEs\n",
        "        columns_to_randomize=['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9'],  # ALL fields\n",
        "        seed=42,  # Reproducible random NULLs\n",
        "        force_all_null_update=True  # First UPDATE will have all NULLs (edge case testing)\n",
        "    )\n",
        "finally:\n",
        "    conn.close()\n",
        "\n",
        "# Wait for new CDC files to appear in Azure (positive confirmation)\n",
        "print(f\"\")\n",
        "print(f\"‚è≥ Waiting for new CDC files to appear in Azure...\")\n",
        "print(f\"   Baseline: {files_before} files\")\n",
        "print()\n",
        "\n",
        "# Poll for new files (max 90 seconds)\n",
        "max_wait = 90\n",
        "check_interval = 10\n",
        "elapsed = 0\n",
        "\n",
        "while elapsed < max_wait:\n",
        "    result = check_azure_files(\n",
        "        storage_account_name, storage_account_key, container_name,\n",
        "        source_catalog, source_schema, source_table, target_table,\n",
        "        verbose=False,\n",
        "        format=cdc_format\n",
        "    )\n",
        "    files_now = len(result['data_files'])\n",
        "    \n",
        "    if files_now > files_before:\n",
        "        print(f\"‚úÖ New CDC files appeared after {elapsed} seconds!\")\n",
        "        print(f\"   Baseline (before workload): {files_before} files\")\n",
        "        print(f\"   Current (after workload): {files_now} files\")\n",
        "        print(f\"   New files generated: {files_now - files_before}\")\n",
        "        break\n",
        "    \n",
        "    print(f\"   Checking... ({elapsed}s elapsed, baseline: {files_before} files)\", end='\\r')\n",
        "    time.sleep(check_interval)\n",
        "    elapsed += check_interval\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Timeout after {max_wait}s - files may still be flushing\")\n",
        "    print(f\"   Run Cell 11 to check manually\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check_files",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the helper function from Cell 4 to check for files\n",
        "result = check_azure_files(\n",
        "    storage_account_name, storage_account_key, container_name,\n",
        "    source_catalog, source_schema, source_table, target_table,\n",
        "    verbose=True,\n",
        "    format=cdc_format\n",
        ")\n",
        "\n",
        "# Provide guidance\n",
        "if len(result['data_files']) == 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  No data files found yet.\")\n",
        "    print(f\"   üí° Possible reasons:\")\n",
        "    print(f\"   - Changefeed not created yet (run Cell 9)\")\n",
        "    print(f\"   - Path configuration mismatch (check Cell 1 variables)\")\n",
        "    print(f\"   - Azure credentials issue (check External Location)\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Files are ready! Proceed to Cell 10 to read with Databricks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "read_cdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import CDC ingestion functions from cockroachdb_autoload.py\n",
        "import importlib, cockroachdb_autoload\n",
        "importlib.reload(cockroachdb_autoload)\n",
        "from cockroachdb_autoload import (\n",
        "    ingest_cdc_append_only_single_family,\n",
        "    ingest_cdc_append_only_multi_family,\n",
        "    ingest_cdc_with_merge_single_family,\n",
        "    ingest_cdc_with_merge_multi_family\n",
        ")\n",
        "\n",
        "print(f\"üî∑ CDC Configuration:\")\n",
        "print(f\"   Processing Mode: {cdc_mode}\")\n",
        "print(f\"   Column Family Mode: {column_family_mode}\")\n",
        "print()\n",
        "\n",
        "# Select function based on BOTH cdc_mode and column_family_mode\n",
        "if cdc_mode == \"append_only\" and column_family_mode == \"single_cf\":\n",
        "    print(f\"üìò Running: ingest_cdc_append_only_single_family()\")\n",
        "    print(f\"   - All CDC events will be stored as rows\")\n",
        "    print(f\"   - No column family merging needed\\n\")\n",
        "    \n",
        "    query = ingest_cdc_append_only_single_family(\n",
        "        storage_account_name=storage_account_name,\n",
        "        container_name=container_name,\n",
        "        source_catalog=source_catalog,\n",
        "        source_schema=source_schema,\n",
        "        source_table=source_table,\n",
        "        target_catalog=target_catalog,\n",
        "        target_schema=target_schema,\n",
        "        target_table=target_table,\n",
        "        spark=spark\n",
        "    )\n",
        "\n",
        "elif cdc_mode == \"append_only\" and column_family_mode == \"multi_cf\":\n",
        "    print(f\"üìô Running: ingest_cdc_append_only_multi_family()\")\n",
        "    print(f\"   - All CDC events will be stored as rows\")\n",
        "    print(f\"   - Column family fragments will be merged\\n\")\n",
        "    \n",
        "    if not primary_key_columns:\n",
        "        raise ValueError(\"primary_key_columns required for multi_cf mode\")\n",
        "    \n",
        "    query = ingest_cdc_append_only_multi_family(\n",
        "        storage_account_name=storage_account_name,\n",
        "        container_name=container_name,\n",
        "        source_catalog=source_catalog,\n",
        "        source_schema=source_schema,\n",
        "        source_table=source_table,\n",
        "        target_catalog=target_catalog,\n",
        "        target_schema=target_schema,\n",
        "        target_table=target_table,\n",
        "        primary_key_columns=primary_key_columns,\n",
        "        spark=spark\n",
        "    )\n",
        "\n",
        "elif cdc_mode == \"update_delete\" and column_family_mode == \"single_cf\":\n",
        "    print(f\"üìó Running: ingest_cdc_with_merge_single_family()\")\n",
        "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
        "    print(f\"   - No column family merging needed\\n\")\n",
        "    \n",
        "    if not primary_key_columns:\n",
        "        raise ValueError(\"primary_key_columns required for update_delete mode\")\n",
        "    \n",
        "    result = ingest_cdc_with_merge_single_family(\n",
        "        storage_account_name=storage_account_name,\n",
        "        container_name=container_name,\n",
        "        source_catalog=source_catalog,\n",
        "        source_schema=source_schema,\n",
        "        source_table=source_table,\n",
        "        target_catalog=target_catalog,\n",
        "        target_schema=target_schema,\n",
        "        target_table=target_table,\n",
        "        primary_key_columns=primary_key_columns,\n",
        "        spark=spark\n",
        "    )\n",
        "    \n",
        "    query = result[\"query\"]\n",
        "\n",
        "elif cdc_mode == \"update_delete\" and column_family_mode == \"multi_cf\":\n",
        "    print(f\"üìï Running: ingest_cdc_with_merge_multi_family()\")\n",
        "    print(f\"   - MERGE logic applied (UPDATE/DELETE processed)\")\n",
        "    print(f\"   - Column family fragments will be merged\\n\")\n",
        "    \n",
        "    if not primary_key_columns:\n",
        "        raise ValueError(\"primary_key_columns required for update_delete + multi_cf mode\")\n",
        "    \n",
        "    result = ingest_cdc_with_merge_multi_family(\n",
        "        storage_account_name=storage_account_name,\n",
        "        container_name=container_name,\n",
        "        source_catalog=source_catalog,\n",
        "        source_schema=source_schema,\n",
        "        source_table=source_table,\n",
        "        target_catalog=target_catalog,\n",
        "        target_schema=target_schema,\n",
        "        target_table=target_table,\n",
        "        primary_key_columns=primary_key_columns,\n",
        "        spark=spark\n",
        "    )\n",
        "    \n",
        "    query = result[\"query\"]\n",
        "\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Invalid mode combination:\\n\"\n",
        "        f\"  cdc_mode='{cdc_mode}' (valid: 'append_only', 'update_delete')\\n\"\n",
        "        f\"  column_family_mode='{column_family_mode}' (valid: 'single_cf', 'multi_cf')\\n\"\n",
        "        f\"Change modes in Cell 1.\"\n",
        "    )\n",
        "\n",
        "# Wait for completion (if not already complete)\n",
        "if cdc_mode == \"append_only\":\n",
        "    query.awaitTermination()\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"‚úÖ CDC INGESTION COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"   Mode: {cdc_mode} + {column_family_mode}\")\n",
        "    print(f\"   Target: {target_catalog}.{target_schema}.{target_table}\")\n",
        "    print()\n",
        "    print(f\"üìä Query your data: SELECT * FROM {target_catalog}.{target_schema}.{target_table}\")\n",
        "else:\n",
        "    # update_delete mode already completed inside the function\n",
        "    print(f\"üìä Query your data: SELECT * FROM {target_catalog}.{target_schema}.{target_table}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df3f327",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALL-IN-ONE CDC DIAGNOSIS\n",
        "\n",
        "# What this does:\n",
        "#   1. CDC Event Summary (replaces Cell 13)\n",
        "#      - Shows total rows, operation breakdown, sample data\n",
        "#   \n",
        "#   2. Source vs Target Verification (replaces Cell 14)\n",
        "#      - Connects to CockroachDB source\n",
        "#      - Auto-deduplicates target for append_only mode\n",
        "#      - Compares column sums\n",
        "#      - Detects mismatches\n",
        "#   \n",
        "#   3. Detailed Diagnosis (automatic if issues found)\n",
        "#      - Column family sync analysis\n",
        "#      - CDC event distribution\n",
        "#      - Row-by-row comparison\n",
        "#      - Troubleshooting recommendations\n",
        "#\n",
        "# Smart behavior:\n",
        "#   ‚úÖ If everything matches ‚Üí Shows \"Perfect sync!\" and exits\n",
        "#   ‚ö†Ô∏è  If mismatches found ‚Üí Automatically runs detailed diagnosis\n",
        "#\n",
        "# No external dependencies - just run this!\n",
        "# ============================================================================\n",
        "\n",
        "import importlib,cockroachdb_ycsb,cockroachdb_debug, cockroachdb_conn\n",
        "importlib.reload(cockroachdb_conn)  # Reload first (cockroachdb_debug depends on it)\n",
        "importlib.reload(cockroachdb_ycsb)  # Reload first (cockroachdb_ycsb depends on it)\n",
        "importlib.reload(cockroachdb_debug)\n",
        "from cockroachdb_debug import run_full_diagnosis_from_config\n",
        "\n",
        "run_full_diagnosis_from_config(spark=spark, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a8f6b5",
      "metadata": {},
      "source": [
        "## Optional: Cleanup\n",
        "\n",
        "Run the cells below if you want to clean up the test resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7250ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è  SAFETY STOP: Cleanup Section\n",
        "# This cell prevents accidental cleanup when running \"Run All\"\n",
        "# \n",
        "# To cleanup resources, manually run each cell below INDIVIDUALLY:\n",
        "#   - Cell 16: Cancel changefeed\n",
        "#   - Cell 17: Drop CockroachDB source table  \n",
        "#   - Cell 18: Drop Databricks target table & checkpoint\n",
        "#   - Cell 19: Clear Azure changefeed data (optional - use for complete reset)\n",
        "\n",
        "raise RuntimeError(\n",
        "    \"\\n\"\n",
        "    \"‚ö†Ô∏è  CLEANUP SAFETY STOP\\n\"\n",
        "    \"\\n\"\n",
        "    \"The cells below will DELETE your resources.\\n\"\n",
        "    \"Do NOT run all cells - run each cleanup cell individually.\\n\"\n",
        "    \"\\n\"\n",
        "    \"üí° TIP: If Cell 13 shows sync issues due to old data,\\n\"\n",
        "    \"   run Cell 19 to clear Azure changefeed data completely.\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8e97da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 1: CANCEL CHANGEFEED(S)\n",
        "conn = get_cockroachdb_connection()\n",
        "try:\n",
        "    with conn.cursor() as cur:\n",
        "        # Find ALL changefeed jobs by matching the sink_uri\n",
        "        # (matches the same pattern used in Cell 9)\n",
        "        # Note: We cancel ALL matches to handle duplicate scenarios\n",
        "        sink_uri_pattern = f\"%{container_name}/{path}%\"\n",
        "        \n",
        "        cur.execute(\"\"\"\n",
        "            SELECT job_id, sink_uri\n",
        "            FROM [SHOW CHANGEFEED JOBS] \n",
        "            WHERE sink_uri LIKE %s\n",
        "            AND status IN ('running', 'paused')\n",
        "        \"\"\", (sink_uri_pattern,))\n",
        "        \n",
        "        changefeeds = cur.fetchall()\n",
        "        if changefeeds:\n",
        "            print(f\"üóëÔ∏è  Cancelling {len(changefeeds)} changefeed(s)...\")\n",
        "            for job_id, sink_uri in changefeeds:\n",
        "                cur.execute(f\"CANCEL JOB {job_id}\")\n",
        "                print(f\"   ‚úÖ Cancelled Job ID: {job_id}\")\n",
        "                print(f\"      Sink URI: {sink_uri[:80]}...\")\n",
        "            if len(changefeeds) > 1:\n",
        "                print(f\"\\n‚ö†Ô∏è  Cancelled {len(changefeeds)} changefeeds (duplicates detected!)\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  No active changefeeds found for this source ‚Üí target mapping\")\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35e74ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 2: DROP SOURCE TABLE (CockroachDB)\n",
        "conn = get_cockroachdb_connection()\n",
        "try:\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(f\"DROP TABLE IF EXISTS {source_table} CASCADE\")\n",
        "        conn.commit()\n",
        "    print(f\"‚úÖ Table '{source_table}' dropped from CockroachDB\")\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988ed2d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 19: CLEAR AZURE CHANGEFEED DATA (Optional)\n",
        "# ‚ö†Ô∏è  WARNING: This will DELETE all changefeed data in Azure for this table!\n",
        "#\n",
        "# Use this when:\n",
        "# - You want to start completely fresh\n",
        "# - Old data from previous runs is causing sync issues\n",
        "# - You changed the table schema (e.g., VARCHAR ‚Üí INT)\n",
        "#\n",
        "# Uses Azure SDK (same as Cell 11 for checking files)\n",
        "\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "# Use path from config (must match Cell 9 changefeed path)\n",
        "changefeed_path = f\"{path}/\"\n",
        "\n",
        "print(f\"üóëÔ∏è  Deleting Azure changefeed data...\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Container: {container_name}\")\n",
        "print(f\"Path: {changefeed_path}\")\n",
        "print()\n",
        "\n",
        "# Connect to Azure (same as Cell 9)\n",
        "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
        "blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
        "container_client = blob_service.get_container_client(container_name)\n",
        "\n",
        "# List all blobs with this prefix\n",
        "print(f\"üîç Scanning for files...\")\n",
        "blobs = list(container_client.list_blobs(name_starts_with=changefeed_path))\n",
        "\n",
        "if not blobs:\n",
        "    print(f\"‚ÑπÔ∏è  No files found at: {changefeed_path}\")\n",
        "    print(f\"   Files may have already been deleted, or path is incorrect\")\n",
        "    print()\n",
        "    print(f\"üí° To check what's in the container, run Cell 9\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found {len(blobs)} items to delete\")\n",
        "    \n",
        "    # Show sample items\n",
        "    data_files = [b for b in blobs if b.size > 0 and '.parquet' in b.name]\n",
        "    resolved_files = [b for b in blobs if '.RESOLVED' in b.name]\n",
        "    directories = [b for b in blobs if b.size == 0]\n",
        "    \n",
        "    print(f\"   üìÑ Data files: {len(data_files)}\")\n",
        "    print(f\"   üïê Resolved files: {len(resolved_files)}\")\n",
        "    print(f\"   üìÅ Directories: {len(directories)}\")\n",
        "    print()\n",
        "    \n",
        "    # Delete all blobs with this prefix\n",
        "    # Note: Azure SDK doesn't have recursive delete - we list all blobs and delete each one\n",
        "    print(f\"üîÑ Deleting {len(blobs)} items...\")\n",
        "    deleted = 0\n",
        "    failed = 0\n",
        "    \n",
        "    for blob in blobs:\n",
        "        try:\n",
        "            container_client.delete_blob(blob.name)\n",
        "            deleted += 1\n",
        "            if deleted % 50 == 0:\n",
        "                print(f\"   Deleted {deleted}/{len(blobs)} items...\", end='\\r')\n",
        "        except Exception as e:\n",
        "            # Some errors are expected (e.g., directories already removed)\n",
        "            error_str = str(e)\n",
        "            if \"DirectoryIsNotEmpty\" not in error_str and \"BlobNotFound\" not in error_str:\n",
        "                failed += 1\n",
        "                print(f\"\\n   ‚ö†Ô∏è  Failed: {blob.name[:60]}... - {e}\")\n",
        "    \n",
        "    print(f\"‚úÖ Deleted {deleted} items from Azure                    \")\n",
        "    if failed > 0:\n",
        "        print(f\"   ‚ö†Ô∏è  Failed to delete {failed} items\")\n",
        "    \n",
        "    print()\n",
        "    print(f\"=\" * 80)\n",
        "    print(f\"‚úÖ Cleanup complete!\")\n",
        "    print()\n",
        "    print(f\"üí° Next steps:\")\n",
        "    print(f\"   1. Drop the Databricks target table (Cell 17)\")\n",
        "    print(f\"   2. Re-run from Cell 6 (Snapshot) to start fresh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033d5054",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 3: DROP TARGET TABLE & CHECKPOINT (Databricks)\n",
        "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
        "checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}\"  # Must match Cell 10\n",
        "\n",
        "# Drop Delta table\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
        "print(f\"‚úÖ Delta table '{target_table_fqn}' dropped\")\n",
        "\n",
        "# Remove checkpoint\n",
        "try:\n",
        "    dbutils.fs.rm(checkpoint_path, True)\n",
        "    print(f\"‚úÖ Checkpoint '{checkpoint_path}' removed\")\n",
        "except:\n",
        "    print(f\"‚ÑπÔ∏è  Checkpoint not found (may have been already removed)\")\n",
        "\n",
        "print(\"\\n‚úÖ Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53c9621",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANUP CELL 4: Complete cleanup for fresh start\n",
        "\n",
        "# 1. Drop staging table\n",
        "staging_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}_staging_cf\"\n",
        "print(f\"üóëÔ∏è  Dropping staging table: {staging_table_fqn}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {staging_table_fqn}\")\n",
        "\n",
        "# 2. Drop target table (if not already done)\n",
        "target_table_fqn = f\"{target_catalog}.{target_schema}.{target_table}\"\n",
        "print(f\"üóëÔ∏è  Dropping target table: {target_table_fqn}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table_fqn}\")\n",
        "\n",
        "# 3. Clear checkpoint location\n",
        "checkpoint_path = f\"/checkpoints/{target_schema}_{target_table}_merge_cf\"\n",
        "print(f\"üóëÔ∏è  Clearing checkpoint: {checkpoint_path}\")\n",
        "try:\n",
        "    dbutils.fs.rm(checkpoint_path, recurse=True)\n",
        "    print(f\"   ‚úÖ Checkpoint cleared\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ÑπÔ∏è  Checkpoint may not exist: {e}\")\n",
        "\n",
        "# 4. Verify cleanup\n",
        "print(f\"\\n‚úÖ Cleanup complete! Ready for fresh start.\")\n",
        "print(f\"   Next: Re-run Cell 12 (ingestion)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a28f2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the schema\n",
        "print(f\"üìÅ Creating schema: {target_catalog}.{target_schema}\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n",
        "print(f\"‚úÖ Schema created\")\n",
        "\n",
        "# Verify schema exists\n",
        "schemas = spark.sql(f\"SHOW SCHEMAS IN {target_catalog}\").collect()\n",
        "schema_names = [row['databaseName'] for row in schemas]\n",
        "if target_schema in schema_names:\n",
        "    print(f\"‚úÖ Verified: Schema {target_schema} exists\")\n",
        "else:\n",
        "    print(f\"‚ùå Schema {target_schema} not found. Available schemas: {schema_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83bdda3d",
      "metadata": {},
      "source": [
        "# Debug Codes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_311_dogfood (3.11.13)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
