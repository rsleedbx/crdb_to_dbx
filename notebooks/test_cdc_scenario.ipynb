{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated CDC Scenario Testing\n",
    "\n",
    "This notebook uses the automated `load_and_merge_cdc_to_delta()` function to test CDC scenarios.\n",
    "\n",
    "## Features\n",
    "- ‚úÖ Auto-detects primary keys from CockroachDB\n",
    "- ‚úÖ Auto-detects column families\n",
    "- ‚úÖ Loads, transforms, merges, and writes in one call\n",
    "- ‚úÖ Verifies results automatically\n",
    "- ‚úÖ Compares with source files\n",
    "\n",
    "## Prerequisites\n",
    "- Unity Catalog Volume with synced test data\n",
    "- Configuration files: `.env/cockroachdb_credentials.json` and `.env/cockroachdb_pipelines.json`\n",
    "- Data synced via `test_cdc_matrix.sh` (auto-syncs to volume subdirectories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Setup Notes\n",
    "\n",
    "**If you see \"No timestamped directories found\" error:**\n",
    "\n",
    "The code automatically handles the `dbutils.fs.ls()` quirk where `item.name` can be empty. \n",
    "If you still see this error, check:\n",
    "- Run `test_cdc_matrix.sh` to generate test data if not already present\n",
    "- Verify the `TEST_FORMAT` and `TEST_NAME` variables match your test data****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"dbutils\" not in vars():\n",
    "    raise RuntimeError(\"This notebook must be run in Databricks Connect or workspace with dbutils available\")\n",
    "if \"spark\" not in vars():\n",
    "    raise RuntimeError(\"This notebook must be run in Databricks Connect or workspace with Spark available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ConnectorMode enum imported\n",
      "   Available modes: ['volume', 'azure_parquet', 'azure_json', 'azure_dual', 'direct']\n"
     ]
    }
   ],
   "source": [
    "# Import ConnectorMode enum for type-safe mode selection\n",
    "from cockroachdb import ConnectorMode\n",
    "\n",
    "# Available modes:\n",
    "# - ConnectorMode.VOLUME: Read JSON/Parquet from Unity Catalog Volumes\n",
    "# - ConnectorMode.AZURE_PARQUET: Read Parquet from Azure Blob Storage  \n",
    "# - ConnectorMode.AZURE_JSON: Read JSON from Azure Blob Storage\n",
    "# - ConnectorMode.DIRECT: Instream changefeed (live CDC)\n",
    "\n",
    "print(f\"‚úÖ ConnectorMode enum imported\")\n",
    "print(f\"   Available modes: {[mode.value for mode in ConnectorMode]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURATION SETUP\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Configuration loaded!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.abspath(\"../..\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "import cockroachdb\n",
    "importlib.reload(cockroachdb)\n",
    "from cockroachdb import load_crdb_config, load_and_merge_cdc_to_delta, cleanup_test_checkpoint\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION SETUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load configuration files\n",
    "git_root = os.path.abspath(\"../../..\")\n",
    "cockroach_dir = f\"{git_root}/sources/cockroachdb\"\n",
    "crdb_json_path = f\"{cockroach_dir}/.env/cockroachdb_credentials.json\"\n",
    "pipeline_json_path = f\"{cockroach_dir}/.env/cockroachdb_pipelines.json\"\n",
    "\n",
    "crdb_config = load_crdb_config(crdb_json_path)\n",
    "\n",
    "with open(pipeline_json_path, 'r') as f:\n",
    "    pipeline_config = json.load(f)\n",
    "\n",
    "print(\"\\n‚úÖ Configuration loaded!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Test Scenario\n",
    "\n",
    "Update these variables to test different scenarios from `test_cdc_matrix.sh`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üîß CHANGE THIS TO TEST DIFFERENT SCENARIOS\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# Available test scenarios (JSON first, then Parquet):\n",
    "#   - \"test-json_usertable_with_split\"        ‚≠ê Tests merge with column families\n",
    "#   - \"test-json_usertable_no_split\"\n",
    "#   - \"test-json_simple_test_with_split\"\n",
    "#   - \"test-json_simple_test_no_split\"\n",
    "#   - \"test-parquet_usertable_with_split\"\n",
    "#   - \"test-parquet_usertable_no_split\"\n",
    "#   - \"test-parquet_simple_test_with_split\"\n",
    "#   - \"test-parquet_simple_test_no_split\"\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# Test scenario (subdirectory name from test_cdc_matrix.sh)\n",
    "TEST_FORMAT=\"parquet\"   # json | parquet\n",
    "TEST_NAME=\"usertable_with_split\"\n",
    "\n",
    "# Test version (which test run to analyze)\n",
    "TEST_VERSION = 0  # 0=latest, 1=second newest, -1=oldest\n",
    "\n",
    "TEST_SCENARIO = f\"test-{TEST_FORMAT}_{TEST_NAME}\"  # ‚≠ê Change this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parsed scenario: test-parquet_usertable_with_split\n",
      "   Format: parquet\n",
      "   Table: usertable\n",
      "   Split: with_split\n",
      "   Using catalog: defaultdb\n",
      "   Using schema: public\n",
      "\n",
      "================================================================================\n",
      "TEST CONFIGURATION\n",
      "================================================================================\n",
      "Test scenario: test-parquet_usertable_with_split\n",
      "Test version: 0 (0=latest, -1=oldest)\n",
      "Source table: usertable\n",
      "Volume path prefix: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split\n",
      "  (Timestamp will be resolved automatically)\n",
      "Target table: main.robert_lee_cockroachdb.usertable_test_parquet_usertable_with_split_delta\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Parse test scenario to extract components\n",
    "# Uses centralized parse_test_scenario() from cockroachdb.py\n",
    "# Pattern: test-{format}_{table_name}_{split_info}\n",
    "# Examples:\n",
    "#   \"test-parquet_simple_test_no_split\" ‚Üí format='parquet', table_name='simple_test', has_split=False\n",
    "#   \"test-json_usertable_with_split\" ‚Üí format='json', table_name='usertable', has_split=True\n",
    "from cockroachdb import parse_test_scenario\n",
    "\n",
    "scenario = parse_test_scenario(TEST_SCENARIO)\n",
    "\n",
    "# Extract table name from scenario\n",
    "SOURCE_TABLE = scenario.table_name\n",
    "\n",
    "# CockroachDB connection defaults (used for schema auto-detection)\n",
    "# These are NOT part of the scenario name - they're configuration\n",
    "CRDB_CATALOG = \"defaultdb\"  # CockroachDB database/catalog\n",
    "CRDB_SCHEMA = \"public\"      # CockroachDB schema\n",
    "\n",
    "print(f\"‚úÖ Parsed scenario: {scenario.scenario_name}\")\n",
    "print(f\"   Format: {scenario.format}\")\n",
    "print(f\"   Table: {SOURCE_TABLE}\")\n",
    "print(f\"   Split: {scenario.split_info}\")\n",
    "print(f\"   Using catalog: {CRDB_CATALOG}\")\n",
    "print(f\"   Using schema: {CRDB_SCHEMA}\")\n",
    "print()\n",
    "\n",
    "# Derived configuration\n",
    "CATALOG = pipeline_config[\"catalog\"]\n",
    "SCHEMA = pipeline_config[\"schema\"]\n",
    "VOLUME_NAME = pipeline_config[\"volume_name\"]\n",
    "\n",
    "# Volume path prefix (timestamp will be resolved based on TEST_VERSION)\n",
    "# This allows testing different test runs without changing paths\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/{TEST_FORMAT}/{CRDB_CATALOG}/{CRDB_SCHEMA}/{TEST_SCENARIO}\"\n",
    "\n",
    "# Target Delta table\n",
    "TARGET_TABLE = f\"{SOURCE_TABLE}_{TEST_SCENARIO.replace('-', '_')}_delta\"\n",
    "TARGET_TABLE_PATH = f\"{CATALOG}.{SCHEMA}.{TARGET_TABLE}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test scenario: {TEST_SCENARIO}\")\n",
    "print(f\"Test version: {TEST_VERSION} (0=latest, -1=oldest)\")\n",
    "print(f\"Source table: {SOURCE_TABLE}\")\n",
    "print(f\"Volume path prefix: {VOLUME_PATH}\")\n",
    "print(f\"  (Timestamp will be resolved automatically)\")\n",
    "print(f\"Target table: {TARGET_TABLE_PATH}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Automated Test\n",
    "\n",
    "This single function call:\n",
    "- Auto-detects primary keys and column families\n",
    "- Loads data with Autoloader\n",
    "- Applies CDC transformations\n",
    "- Merges column family fragments\n",
    "- Writes to Delta table\n",
    "- Verifies results\n",
    "- Compares with source files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Parsed volume path:\n",
      "   Volume base: /Volumes/main/robert_lee_cockroachdb/parquet_files\n",
      "   Path prefix: parquet/defaultdb/public/test-parquet_usertable_with_split\n",
      "   Timestamp in path: None\n",
      "   Version parameter: 0\n",
      "üîç Auto-resolving timestamped path (version=0)...\n",
      "   ‚úÖ Resolved to timestamp: 1769034791\n",
      "   Full path: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791\n",
      "üîç Detected test table name from path: test_parquet_usertable_with_split\n",
      "================================================================================\n",
      "AUTOMATED CDC TESTING\n",
      "================================================================================\n",
      "üìå Code Version: cockroachdb@f603af7\n",
      "Source table: usertable\n",
      "Volume path: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791\n",
      "Target table: main.robert_lee_cockroachdb.usertable_test_parquet_usertable_with_split_delta\n",
      "CockroachDB catalog: defaultdb\n",
      "CockroachDB schema: public\n",
      "\n",
      "üîç Auto-detected format: parquet\n",
      "üîç Validating volume path...\n",
      "   File format: parquet\n",
      "   File extension: .parquet\n",
      "   ‚úÖ Found 4 parquet file(s)\n",
      "\n",
      "üîç Loading schema from volume...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert.lee/github/lakeflow-community-connectors/sources/cockroachdb/cockroachdb.py:179: UserWarning: catalog/schema not specified - using defaults. Specify 'catalog' and 'schema' for production use.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Schema file found\n",
      "   Primary keys: ['ycsb_key']\n",
      "   Has column families: True\n",
      "\n",
      "================================================================================\n",
      "FAST CHECKPOINT CLEARING\n",
      "================================================================================\n",
      "================================================================================\n",
      "PARALLEL CHECKPOINT DELETION\n",
      "================================================================================\n",
      "Path: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/_checkpoints\n",
      "Workers: 20\n",
      "\n",
      "‚ÑπÔ∏è  Checkpoint doesn't exist: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/_checkpoints\n",
      "   (This is OK if it's the first run)\n",
      "\n",
      "‚úÖ Dropped table: main.robert_lee_cockroachdb.usertable_test_parquet_usertable_with_split_delta\n",
      "\n",
      "================================================================================\n",
      "CLEAR COMPLETE\n",
      "================================================================================\n",
      "‚è±Ô∏è  Total time: 1.7s\n",
      "üìä Items deleted: 0\n",
      "================================================================================\n",
      "\n",
      "üì• Loading data with Autoloader (parquet format)...\n",
      "   ‚úÖ Autoloader configured\n",
      "\n",
      "üîß Applying CDC transformations...\n",
      "   ‚úÖ CDC metadata added\n",
      "\n",
      "üíæ Writing CDC events to temp table...\n",
      "   (Column family merge will happen in batch mode)\n",
      "   üöÄ Step 1: Writing raw CDC events to temp table...\n",
      "   ‚úÖ Raw CDC events written to temp table\n",
      "   üöÄ Step 2: Merging column family fragments in batch mode...\n",
      "   üîç Columns in temp table: 18 total\n",
      "      ['ycsb_key', 'field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9', '__crdb__event_type', '__crdb__updated', '_rescued_data', '_cdc_operation']\n",
      "      ... and 3 more\n",
      "   üîç CDC operations in temp table (BEFORE merge):\n",
      "      UPSERT: 20500\n",
      "      DELETE: 200\n",
      "   üîç DELETE key extraction analysis:\n",
      "      Total DELETE rows: 200\n",
      "      Unique (key + _cdc_timestamp + operation): 100\n",
      "      Sample ycsb_key values: ['user0000009901', 'user0000009902', 'user0000009903', 'user0000009904', 'user0000009905']\n",
      "      ‚ö†Ô∏è  WARNING: 100 DELETEs have duplicate (key+timestamp+operation)!\n",
      "\n",
      "üîç Column Family Merge (Batch Mode)\n",
      "   Primary key columns: ['ycsb_key']\n",
      "   Metadata columns: 17 columns\n",
      "   Data columns: 10 columns\n",
      "     ['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9']\n",
      "\n",
      "üìä Fragmentation Detection:\n",
      "   Total rows: 20,700\n",
      "   Unique events (PK + timestamp + operation): 10,550\n",
      "   Duplication ratio: 2.0x\n",
      "\n",
      "üîß Column family fragmentation detected!\n",
      "   Merging 20,700 fragments into 10,550 distinct CDC events...\n",
      "\n",
      "‚úÖ Merge transformation applied!\n",
      "   Batch DataFrame merged\n",
      "\n",
      "üíæ Writing CDC data to Delta table...\n",
      "   üöÄ Step 3: Applying CDC merge with DELETE support...\n",
      "   üîç Events after column family merge:\n",
      "      UPSERT: 10450\n",
      "      DELETE: 100\n",
      "   üîç Keys to delete: 100\n",
      "   üîç Non-DELETE rows: 10450\n",
      "   üîç After excluding DELETEd keys: 10350\n",
      "   üîç After deduplication (latest per key): 9950\n",
      "   üîç Rows removed by DELETE: 100\n",
      "   üîç Duplicate events removed: 400\n",
      "   üìù Creating initial table: 9,950 rows\n",
      "   ‚úÖ CDC merge complete!\n",
      "\n",
      "‚úÖ Verifying results...\n",
      "   üìä Delta table: 9,950 rows\n",
      "      UPSERT: 9,950\n",
      "\n",
      "üìä Comparing with source files...\n",
      "   üìÇ Found 4 files in /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791\n",
      "   üìÑ 202601212242005443437500000000000-b409b8db75c1cc45-1-30-00000000-test_parquet_usertable_with_split+data-1.parquet: 10000 events (Parquet)\n",
      "   üìÑ 202601212242005443437500000000000-b409b8db75c1cc45-1-30-00000001-test_parquet_usertable_with_split+pk-1.parquet: 10000 events (Parquet)\n",
      "   üìÑ 202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet: 550 events (Parquet)\n",
      "   üìÑ 202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000003-test_parquet_usertable_with_split+pk-1.parquet: 150 events (Parquet)\n",
      "   üìä Total events read: 20,700\n",
      "   üìä Format breakdown: 4 Parquet, 0 JSON\n",
      "   üìä After deduplication: 10,050 events\n",
      "   üìä Operation counts: SNAPSHOT=0, INSERT=0, UPDATE=9950, DELETE=100\n",
      "   üìä Active keys (unique): 9,950\n",
      "   üìä Source files: 4\n",
      "   üìä Unique keys (deduplicated): 9,950\n",
      "   üìä Total events (raw): 10,050 (includes updates to same keys)\n",
      "   üìä UPSERT events: 9,950\n",
      "   üìä DELETE events: 100\n",
      "\n",
      "üìä Comparison:\n",
      "   Delta: 9,950\n",
      "   Source: 9,950\n",
      "\n",
      "   ‚úÖ‚úÖ‚úÖ PERFECT MATCH! ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run automated test\n",
    "result = load_and_merge_cdc_to_delta(\n",
    "    source_table=SOURCE_TABLE,\n",
    "    volume_path=VOLUME_PATH,\n",
    "    target_table_path=TARGET_TABLE_PATH,\n",
    "    crdb_config=crdb_config,\n",
    "    catalog=CRDB_CATALOG,\n",
    "    schema=CRDB_SCHEMA,\n",
    "    spark=spark, \n",
    "    dbutils=dbutils,\n",
    "    clear_checkpoint=True,   # Set to False if appending data\n",
    "    verify=True,             # Verify Delta table\n",
    "    compare_source=True,     # Compare with source files\n",
    "    debug=True,              # Show detailed progress\n",
    "    version=TEST_VERSION     # Which test run to use (0=latest, -1=oldest)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLEANUP TEST CHECKPOINT\n",
      "================================================================================\n",
      "Volume path: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791\n",
      "Checkpoint: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/_checkpoints\n",
      "================================================================================\n",
      "\n",
      "üßπ Deleting checkpoint files...\n",
      "================================================================================\n",
      "PARALLEL CHECKPOINT DELETION\n",
      "================================================================================\n",
      "Path: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/_checkpoints\n",
      "Workers: 20\n",
      "\n",
      "üìä Found:\n",
      "   Subdirectories: 2\n",
      "   Files: 0\n",
      "\n",
      "üîç Recursively scanning directories (max 5 levels deep for hierarchical structures)...\n",
      "   delta/: 11 leaf directories\n",
      "   schema/: 2 leaf directories\n",
      "\n",
      "   Total: 13 directories to delete\n",
      "\n",
      "üî• Deleting 13 directories in parallel...\n",
      "   ‚úÖ delta/__tmp_path_dir\n",
      "   ‚úÖ 0/metadata\n",
      "   ‚úÖ offsets/0\n",
      "   ‚úÖ _schemas/0\n",
      "   ‚úÖ delta/metadata\n",
      "   ‚úÖ commits/0\n",
      "   ‚úÖ _schemas/__tmp_path_dir\n",
      "   ‚úÖ 0/__tmp_path_dir\n",
      "   ‚úÖ rocksdb/__tmp_path_dir\n",
      "   ‚úÖ commits/__tmp_path_dir\n",
      "   ‚úÖ offsets/__tmp_path_dir\n",
      "   ‚úÖ rocksdb/logs\n",
      "   ‚úÖ rocksdb/SSTs\n",
      "\n",
      "   Deleted 13 directories\n",
      "\n",
      "üßπ Cleaning up parent directory...\n",
      "   ‚úÖ Deleted parent: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/_checkpoints\n",
      "\n",
      "================================================================================\n",
      "DELETION SUMMARY\n",
      "================================================================================\n",
      "‚úÖ Successfully deleted: 13 items\n",
      "‚è±Ô∏è  Time: 37.9s\n",
      "================================================================================\n",
      "   ‚úÖ Deleted 13 checkpoint items\n",
      "\n",
      "================================================================================\n",
      "CLEANUP COMPLETE\n",
      "================================================================================\n",
      "‚è±Ô∏è  Time: 37.9s\n",
      "üìä Items deleted: 13\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚úÖ Ready for next test iteration!\n"
     ]
    }
   ],
   "source": [
    "# Quick cleanup after inspecting results\n",
    "# This is faster than clear_checkpoint=True because:\n",
    "# - You can inspect the results first\n",
    "# - Only deletes what's needed\n",
    "# - Can keep table for further inspection if desired\n",
    "\n",
    "# Option 1: Clean checkpoint only (keep table for inspection)\n",
    "cleanup_test_checkpoint(\n",
    "    volume_path=VOLUME_PATH,\n",
    "    dbutils=dbutils,\n",
    "    version=TEST_VERSION,\n",
    "    drop_table=False  # Keep table\n",
    ")\n",
    "\n",
    "# Option 2: Full cleanup (checkpoint + table)\n",
    "# cleanup_test_checkpoint(\n",
    "#     volume_path=VOLUME_PATH,\n",
    "#     target_table_path=TARGET_TABLE_PATH,\n",
    "#     dbutils=dbutils,\n",
    "#     spark=spark,\n",
    "#     version=TEST_VERSION,\n",
    "#     drop_table=True  # Drop table too\n",
    "# )\n",
    "\n",
    "print(\"\\n‚úÖ Ready for next test iteration!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Review Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST RESULTS\n",
      "================================================================================\n",
      "Success: True\n",
      "Primary keys: ['ycsb_key']\n",
      "Has column families: True\n",
      "Delta table rows: 9,950\n",
      "Source file rows: 9,950\n",
      "Match: True ‚úÖ\n",
      "================================================================================\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ TEST PASSED! ‚úÖ‚úÖ‚úÖ\n",
      "Column family merge worked correctly!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Primary keys: {result['primary_keys']}\")\n",
    "print(f\"Has column families: {result['has_column_families']}\")\n",
    "print(f\"Delta table rows: {result['delta_count']:,}\")\n",
    "print(f\"Source file rows: {result['source_count']:,}\")\n",
    "print(f\"Match: {result['match']} {'‚úÖ' if result['match'] else '‚ö†Ô∏è'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if result['match']:\n",
    "    print(\"\\n‚úÖ‚úÖ‚úÖ TEST PASSED! ‚úÖ‚úÖ‚úÖ\")\n",
    "    print(\"Column family merge worked correctly!\")\n",
    "else:\n",
    "    diff = result['delta_count'] - result['source_count']\n",
    "    print(f\"\\n‚ö†Ô∏è  TEST FAILED: {diff:+,} row difference\")\n",
    "    print(\"Review the logs above for details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Query Delta Table (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ycsb_key</th>\n",
       "      <th>_cdc_timestamp</th>\n",
       "      <th>_cdc_operation</th>\n",
       "      <th>field0</th>\n",
       "      <th>field1</th>\n",
       "      <th>field2</th>\n",
       "      <th>field3</th>\n",
       "      <th>field4</th>\n",
       "      <th>field5</th>\n",
       "      <th>field6</th>\n",
       "      <th>field7</th>\n",
       "      <th>field8</th>\n",
       "      <th>field9</th>\n",
       "      <th>__crdb__event_type</th>\n",
       "      <th>__crdb__updated</th>\n",
       "      <th>_rescued_data</th>\n",
       "      <th>_source_file</th>\n",
       "      <th>_processing_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>newuser0000000001</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_1</td>\n",
       "      <td>field1_new_1</td>\n",
       "      <td>field2_new_c4ca4238a0b923820dcc509a6f75849b</td>\n",
       "      <td>field3_new_yy</td>\n",
       "      <td>field4_new_1</td>\n",
       "      <td>field5_new_1</td>\n",
       "      <td>field6_new_1</td>\n",
       "      <td>field7_new_1</td>\n",
       "      <td>field8_new_1</td>\n",
       "      <td>field9_new_1</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>newuser0000000002</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_2</td>\n",
       "      <td>field1_new_2</td>\n",
       "      <td>field2_new_c81e728d9d4c2f636f067f89cc14862c</td>\n",
       "      <td>field3_new_yyy</td>\n",
       "      <td>field4_new_2</td>\n",
       "      <td>field5_new_2</td>\n",
       "      <td>field6_new_2</td>\n",
       "      <td>field7_new_2</td>\n",
       "      <td>field8_new_2</td>\n",
       "      <td>field9_new_2</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>newuser0000000003</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_3</td>\n",
       "      <td>field1_new_3</td>\n",
       "      <td>field2_new_eccbc87e4b5ce2fe28308fd9f2a7baf3</td>\n",
       "      <td>field3_new_yyyy</td>\n",
       "      <td>field4_new_3</td>\n",
       "      <td>field5_new_3</td>\n",
       "      <td>field6_new_3</td>\n",
       "      <td>field7_new_3</td>\n",
       "      <td>field8_new_3</td>\n",
       "      <td>field9_new_3</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>newuser0000000004</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_4</td>\n",
       "      <td>field1_new_4</td>\n",
       "      <td>field2_new_a87ff679a2f3e71d9181a67b7542122c</td>\n",
       "      <td>field3_new_yyyyy</td>\n",
       "      <td>field4_new_4</td>\n",
       "      <td>field5_new_4</td>\n",
       "      <td>field6_new_4</td>\n",
       "      <td>field7_new_4</td>\n",
       "      <td>field8_new_4</td>\n",
       "      <td>field9_new_4</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>newuser0000000005</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_5</td>\n",
       "      <td>field1_new_5</td>\n",
       "      <td>field2_new_e4da3b7fbbce2345d7772b0674a318d5</td>\n",
       "      <td>field3_new_yyyyyy</td>\n",
       "      <td>field4_new_5</td>\n",
       "      <td>field5_new_5</td>\n",
       "      <td>field6_new_5</td>\n",
       "      <td>field7_new_5</td>\n",
       "      <td>field8_new_5</td>\n",
       "      <td>field9_new_5</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>newuser0000000006</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_6</td>\n",
       "      <td>field1_new_6</td>\n",
       "      <td>field2_new_1679091c5a880faf6fb5e6087eb1b2dc</td>\n",
       "      <td>field3_new_yyyyyyy</td>\n",
       "      <td>field4_new_6</td>\n",
       "      <td>field5_new_6</td>\n",
       "      <td>field6_new_6</td>\n",
       "      <td>field7_new_6</td>\n",
       "      <td>field8_new_6</td>\n",
       "      <td>field9_new_6</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>newuser0000000007</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_7</td>\n",
       "      <td>field1_new_7</td>\n",
       "      <td>field2_new_8f14e45fceea167a5a36dedd4bea2543</td>\n",
       "      <td>field3_new_yyyyyyyy</td>\n",
       "      <td>field4_new_7</td>\n",
       "      <td>field5_new_7</td>\n",
       "      <td>field6_new_7</td>\n",
       "      <td>field7_new_7</td>\n",
       "      <td>field8_new_7</td>\n",
       "      <td>field9_new_7</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>newuser0000000008</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_8</td>\n",
       "      <td>field1_new_8</td>\n",
       "      <td>field2_new_c9f0f895fb98ab9159f51fd0297e236d</td>\n",
       "      <td>field3_new_yyyyyyyyy</td>\n",
       "      <td>field4_new_8</td>\n",
       "      <td>field5_new_8</td>\n",
       "      <td>field6_new_8</td>\n",
       "      <td>field7_new_8</td>\n",
       "      <td>field8_new_8</td>\n",
       "      <td>field9_new_8</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>newuser0000000009</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_9</td>\n",
       "      <td>field1_new_9</td>\n",
       "      <td>field2_new_45c48cce2e2d7fbdea1afc51c7c6ad26</td>\n",
       "      <td>field3_new_yyyyyyyyyy</td>\n",
       "      <td>field4_new_9</td>\n",
       "      <td>field5_new_9</td>\n",
       "      <td>field6_new_9</td>\n",
       "      <td>field7_new_9</td>\n",
       "      <td>field8_new_9</td>\n",
       "      <td>field9_new_9</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>newuser0000000010</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>UPSERT</td>\n",
       "      <td>field0_new_10</td>\n",
       "      <td>field1_new_10</td>\n",
       "      <td>field2_new_d3d9446802a44259755d38e6d163e820</td>\n",
       "      <td>field3_new_yyyyyyyyyyy</td>\n",
       "      <td>field4_new_10</td>\n",
       "      <td>field5_new_10</td>\n",
       "      <td>field6_new_10</td>\n",
       "      <td>field7_new_10</td>\n",
       "      <td>field8_new_10</td>\n",
       "      <td>field9_new_10</td>\n",
       "      <td>c</td>\n",
       "      <td>1769035360730318091.0000000000</td>\n",
       "      <td>None</td>\n",
       "      <td>/Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791/2026-01-21/202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000002-test_parquet_usertable_with_split+data-1.parquet</td>\n",
       "      <td>2026-01-22 00:17:51.542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[ycsb_key: string, _cdc_timestamp: string, _cdc_operation: string, field0: string, field1: string, field2: string, field3: string, field4: string, field5: string, field6: string, field7: string, field8: string, field9: string, __crdb__event_type: string, __crdb__updated: string, _rescued_data: string, _source_file: string, _processing_time: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display sample data\n",
    "display(spark.table(TARGET_TABLE_PATH).limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_cdc_operation</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UPSERT</td>\n",
       "      <td>9950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[_cdc_operation: string, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display operation breakdown\n",
    "display(spark.table(TARGET_TABLE_PATH).groupBy(\"_cdc_operation\").count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What Just Happened\n",
    "\n",
    "1. **Auto-detection**: Primary keys and column families detected from CockroachDB\n",
    "2. **Loading**: Parquet files loaded from Unity Catalog Volume with Autoloader\n",
    "3. **Transformation**: CDC metadata added (operation type, timestamp, source file)\n",
    "4. **Merging**: Column family fragments merged into complete rows\n",
    "5. **Writing**: Data written to Delta table with streaming aggregation\n",
    "6. **Verification**: Row counts compared between Delta table and source files\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Test more scenarios by changing `TEST_SCENARIO` in Step 2\n",
    "- Compare results across different format/split combinations\n",
    "- Validate that `test-parquet_usertable_with_split` produces correct count (not 11x inflation)\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "- **Automation**: One function call replaces 8 manual steps\n",
    "- **Auto-detection**: No need to manually specify primary keys or check for column families\n",
    "- **Verification**: Built-in validation ensures data integrity\n",
    "- **Flexibility**: All steps can be controlled with optional parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Test Iterator Pattern (Community Connector)\n",
    "\n",
    "This section demonstrates using the **Iterator Pattern** (Pattern 1) instead of Autoloader.\n",
    "The iterator pattern is useful for:\n",
    "- Testing and prototyping\n",
    "- Low-volume workloads\n",
    "- Batch processing with manual control\n",
    "\n",
    "### Supported Modes:\n",
    "1. **`volume`** - Read JSON/Parquet files from Unity Catalog Volumes (file-based)\n",
    "2. **`azure_parquet`** - Read Parquet files from Azure Blob Storage (file-based)\n",
    "3. **`direct`** - Instream changefeed connection (live CDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Test Different Modes\n",
    "\n",
    "To test different data sources, simply change the `ITERATOR_MODE` variable in the configuration cell:\n",
    "\n",
    "#### Mode 1: Volume (File-based - JSON or Parquet) ‚≠ê RECOMMENDED\n",
    "```python\n",
    "ITERATOR_MODE = ConnectorMode.VOLUME\n",
    "```\n",
    "- **Use case:** Testing with files already synced to Unity Catalog\n",
    "- **Data source:** JSON or Parquet files from `test_cdc_matrix.sh`\n",
    "- **Format:** Auto-detected from `TEST_FORMAT` variable (set in Step 2)\n",
    "- **Pros:** Fast, reliable, reproducible, supports both JSON and Parquet\n",
    "- **Cons:** Requires files to be synced first\n",
    "\n",
    "**To switch between JSON and Parquet in Volume mode:**\n",
    "- Change `TEST_FORMAT = \"json\"` or `TEST_FORMAT = \"parquet\"` in **Step 2**\n",
    "- The volume path and format will be auto-configured\n",
    "\n",
    "#### Mode 2: Azure Parquet (File-based - Parquet only)\n",
    "```python\n",
    "ITERATOR_MODE = ConnectorMode.AZURE_PARQUET\n",
    "```\n",
    "- **Use case:** Reading directly from Azure Blob Storage\n",
    "- **Data source:** Parquet files written by CockroachDB changefeeds\n",
    "- **Format:** Parquet only\n",
    "- **Pros:** No volume sync needed, direct access to Azure\n",
    "- **Cons:** Requires Azure credentials, Parquet only (no JSON support)\n",
    "\n",
    "#### Mode 3: Direct Instream (Live CDC) ‚ö†Ô∏è USE WITH CAUTION\n",
    "```python\n",
    "ITERATOR_MODE = ConnectorMode.DIRECT\n",
    "```\n",
    "- **Use case:** Live CDC testing, development, real-time data\n",
    "- **Data source:** Direct changefeed connection to CockroachDB\n",
    "- **Format:** JSON (sinkless changefeed)\n",
    "- **Pros:** Real-time data, no file storage needed, tests live connection\n",
    "- **Cons:** Requires active CockroachDB connection, may run indefinitely\n",
    "- **‚ö†Ô∏è Note:** This creates a live changefeed connection - use safety limits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Reference: Testing Combinations\n",
    "\n",
    "| Scenario | Step 2: TEST_FORMAT | Configuration Cell: ITERATOR_MODE | Result |\n",
    "|----------|---------------------|-----------------------------------|---------|\n",
    "| **JSON files from Volume** | `\"json\"` | `ConnectorMode.VOLUME` | Reads JSON files from Unity Catalog |\n",
    "| **Parquet files from Volume** | `\"parquet\"` | `ConnectorMode.VOLUME` | Reads Parquet files from Unity Catalog |\n",
    "| **Parquet from Azure Blob** | `\"parquet\"` | `ConnectorMode.AZURE_PARQUET` | Reads Parquet from Azure Storage |\n",
    "| **Live CDC (Instream)** | (any) | `ConnectorMode.DIRECT` | Creates live changefeed connection |\n",
    "\n",
    "**Example Workflow:**\n",
    "\n",
    "1. **Test JSON files:**\n",
    "   - Step 2: Set `TEST_FORMAT = \"json\"`\n",
    "   - Configuration cell: Set `ITERATOR_MODE = ConnectorMode.VOLUME`\n",
    "   - Run iterator test ‚Üí Reads JSON files\n",
    "\n",
    "2. **Test Parquet files:**\n",
    "   - Step 2: Set `TEST_FORMAT = \"parquet\"`\n",
    "   - Configuration cell: Keep `ITERATOR_MODE = ConnectorMode.VOLUME`\n",
    "   - Run iterator test ‚Üí Reads Parquet files\n",
    "\n",
    "3. **Test live CDC:**\n",
    "   - Configuration cell: Set `ITERATOR_MODE = ConnectorMode.DIRECT`\n",
    "   - Run iterator test ‚Üí Connects to CockroachDB changefeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import cockroachdb\n",
    "importlib.reload(cockroachdb)\n",
    "from cockroachdb import LakeflowConnect, ConnectorMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ITERATOR PATTERN CONFIGURATION\n",
      "================================================================================\n",
      "Mode: volume (enum: VOLUME)\n",
      "üìÅ Data source: Unity Catalog Volume\n",
      "   Path prefix: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split\n",
      "   Resolved path: /Volumes/main/robert_lee_cockroachdb/parquet_files/parquet/defaultdb/public/test-parquet_usertable_with_split/1769034791\n",
      "   Format: Auto-detected from path (parquet)\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üîß CONFIGURE ITERATOR PATTERN MODE (Using ConnectorMode Enum)\n",
    "# ============================================================================\n",
    "\n",
    "# Select which mode to test using the ConnectorMode enum:\n",
    "#   - ConnectorMode.VOLUME        : Read JSON/Parquet from Unity Catalog Volumes (recommended)\n",
    "#   - ConnectorMode.AZURE_PARQUET : Read Parquet from Azure Blob Storage\n",
    "#   - ConnectorMode.AZURE_JSON    : Read JSON from Azure Blob Storage\n",
    "#   - ConnectorMode.DIRECT        : Instream changefeed (live CDC connection)\n",
    "\n",
    "ITERATOR_MODE = ConnectorMode.VOLUME  # ‚≠ê Change this to test different modes\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ITERATOR PATTERN CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mode: {ITERATOR_MODE.value} (enum: {ITERATOR_MODE.name})\")\n",
    "\n",
    "# Build connector options based on selected mode\n",
    "connector_options = {}\n",
    "\n",
    "if ITERATOR_MODE == ConnectorMode.VOLUME:\n",
    "    # Mode 1: Read from Unity Catalog Volume (JSON or Parquet files)\n",
    "    print(f\"üìÅ Data source: Unity Catalog Volume\")\n",
    "    print(f\"   Path: {VOLUME_PATH}\")\n",
    "    print(f\"   Format: Auto-detected from path ({TEST_FORMAT})\")\n",
    "    \n",
    "    connector_options = {\n",
    "        'mode': ConnectorMode.VOLUME.value,  # Convert enum to string\n",
    "        'volume_path': VOLUME_PATH,\n",
    "        'spark': spark,\n",
    "        'dbutils': dbutils\n",
    "    }\n",
    "\n",
    "elif ITERATOR_MODE == ConnectorMode.AZURE_PARQUET:\n",
    "    # Mode 2: Read from Azure Blob Storage (Parquet files)\n",
    "    print(f\"‚òÅÔ∏è  Data source: Azure Blob Storage (Parquet)\")\n",
    "    print(f\"   Account: {pipeline_config.get('azure_account', 'N/A')}\")\n",
    "    print(f\"   Container: {pipeline_config.get('container_name', 'N/A')}\")\n",
    "    \n",
    "    # Construct Azure path prefix\n",
    "    azure_path_prefix = f\"{TEST_FORMAT}/{CRDB_CATALOG}/{CRDB_SCHEMA}/{TEST_SCENARIO}\"\n",
    "    print(f\"   Path prefix: {azure_path_prefix}\")\n",
    "    \n",
    "    connector_options = {\n",
    "        'mode': ConnectorMode.AZURE_PARQUET.value,\n",
    "        'azure_account': pipeline_config.get('azure_account'),\n",
    "        'azure_key': pipeline_config.get('azure_key'),\n",
    "        'container_name': pipeline_config.get('container_name'),\n",
    "        'azure_path_prefix': azure_path_prefix,\n",
    "        'format': 'parquet'  # Azure mode only supports Parquet\n",
    "    }\n",
    "\n",
    "elif ITERATOR_MODE == ConnectorMode.DIRECT:\n",
    "    # Mode 3: Direct instream changefeed connection\n",
    "    print(f\"üî¥ Data source: Instream changefeed (Live CDC)\")\n",
    "    print(f\"   Host: {crdb_config.get('host', 'N/A')}\")\n",
    "    print(f\"   Database: {crdb_config.get('database', 'N/A')}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Note: This will create a live changefeed connection\")\n",
    "    \n",
    "    connector_options = {\n",
    "        'mode': ConnectorMode.DIRECT.value,\n",
    "        'connection_url': crdb_config.get('connection_url'),\n",
    "        'catalog': CRDB_CATALOG,\n",
    "        'schema': CRDB_SCHEMA,\n",
    "        'format': 'json'  # Direct mode uses JSON format\n",
    "    }\n",
    "\n",
    "else:\n",
    "    valid_modes = [mode.name for mode in ConnectorMode]\n",
    "    raise ValueError(f\"Unknown mode: {ITERATOR_MODE}. Valid modes: {valid_modes}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Initializing connector in 'volume' mode...\n",
      "‚úÖ Connector initialized!\n",
      "\n",
      "üìñ Reading 'usertable' using iterator pattern...\n",
      "   Mode: volume\n",
      "   Primary keys: ['ycsb_key']\n",
      "\n",
      "Reading table: usertable (mode=volume, cursor=)\n",
      "   Batch 1: 9,950 records (cursor: 20260121224231000000...)\n",
      "Reading table: usertable (mode=volume, cursor=202601212242310000000000000000001-b409b8db75c1cc45-1-30-00000003-test_parquet_usertable_with_split+pk-1.parquet)\n",
      "‚úÖ No more records. Finished reading.\n",
      "\n",
      "üìä Summary:\n",
      "   Total records: 9,950\n",
      "   Total batches: 1\n",
      "   CDC Operations:\n",
      "      SNAPSHOT: 9,950\n",
      "\n",
      "üìã Sample record (first):\n",
      "   ycsb_key: newuser0000000002\n",
      "   _cdc_updated: 1769035360730318091.0000000000\n",
      "   _cdc_operation: SNAPSHOT\n",
      "   field0: field0_new_2\n",
      "   field1: field1_new_2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Test Community Connector Iterator Pattern\n",
    "# ============================================================================\n",
    "\n",
    "from cockroachdb import LakeflowConnect\n",
    "\n",
    "# Initialize connector with selected mode (configured in previous cell)\n",
    "print(f\"üîå Initializing connector in '{ITERATOR_MODE}' mode...\")\n",
    "connector = LakeflowConnect(connector_options)\n",
    "print(f\"‚úÖ Connector initialized!\")\n",
    "\n",
    "# Get table schema\n",
    "print(f\"\\nüìã Fetching table schema for '{SOURCE_TABLE}'...\")\n",
    "schema = connector.get_table_schema(SOURCE_TABLE, {})\n",
    "print(f\"‚úÖ Table schema: {len(schema.fields)} fields\")\n",
    "\n",
    "# Get table metadata (primary keys, etc.)\n",
    "print(f\"\\nüìã Fetching table metadata...\")\n",
    "metadata = connector.read_table_metadata(SOURCE_TABLE, {})\n",
    "print(f\"‚úÖ Primary keys: {metadata['primary_keys']}\")\n",
    "print(f\"‚úÖ Ingestion type: {metadata['ingestion_type']}\")\n",
    "\n",
    "# Initialize offset for reading\n",
    "start_offset = {\"cursor\": \"\"}\n",
    "\n",
    "# Read data using iterator pattern\n",
    "print(f\"\\nüìñ Reading data from {SOURCE_TABLE} using iterator pattern...\")\n",
    "print(f\"   Mode: {ITERATOR_MODE}\")\n",
    "all_records = []\n",
    "batch_count = 0\n",
    "\n",
    "while True:\n",
    "    # Read one batch\n",
    "    record_iterator, end_offset = connector.read_table(\n",
    "        table_name=SOURCE_TABLE,\n",
    "        start_offset=start_offset,\n",
    "        table_options={}\n",
    "    )\n",
    "    \n",
    "    # Collect records from iterator\n",
    "    batch_records = list(record_iterator)\n",
    "    \n",
    "    if not batch_records:\n",
    "        print(f\"‚úÖ No more records. Finished reading.\")\n",
    "        break\n",
    "    \n",
    "    batch_count += 1\n",
    "    all_records.extend(batch_records)\n",
    "    print(f\"   Batch {batch_count}: {len(batch_records):,} records (cursor: {end_offset.get('cursor', 'N/A')[:20]}...)\")\n",
    "    \n",
    "    # Check if we're done (offset didn't change)\n",
    "    if end_offset.get('cursor') == start_offset.get('cursor'):\n",
    "        print(f\"‚úÖ Cursor unchanged. Finished reading.\")\n",
    "        break\n",
    "    \n",
    "    # Update offset for next iteration\n",
    "    start_offset = end_offset\n",
    "    \n",
    "    # Safety limit for instream mode (prevent infinite loop)\n",
    "    if ITERATOR_MODE == ConnectorMode.DIRECT and batch_count >= 100:\n",
    "        print(f\"‚ö†Ô∏è  Safety limit reached (100 batches). Stopping.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüìä Total records read: {len(all_records):,}\")\n",
    "print(f\"üìä Total batches: {batch_count}\")\n",
    "\n",
    "# Show sample records\n",
    "if all_records:\n",
    "    print(f\"\\nüìã Sample record (first):\")\n",
    "    sample = all_records[0]\n",
    "    for key, value in list(sample.items())[:5]:\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Check CDC operations\n",
    "    operations = {}\n",
    "    for record in all_records:\n",
    "        op = record.get('_cdc_operation', 'UNKNOWN')\n",
    "        operations[op] = operations.get(op, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä CDC Operations:\")\n",
    "    for op, count in sorted(operations.items()):\n",
    "        print(f\"   {op}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Write Iterator Data to Delta Table\n",
    "\n",
    "Now that we have the records from the iterator, we can write them to a Delta table manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Iterator Pattern Results:\n",
      "   Total records: 9,950\n",
      "   (Deduplication already applied by connector)\n",
      "   Pandas DataFrame: 9950 rows, 16 columns\n",
      "   Columns: ['ycsb_key', '_cdc_updated', '_cdc_operation', 'field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6']...\n",
      "\n",
      "üìä Final row count: 9,950\n",
      "\n",
      "üíæ Writing iterator data to Delta table...\n",
      "   Target: main.robert_lee_cockroachdb.usertable_test_parquet_usertable_with_split_delta_iterator\n",
      "‚úÖ Successfully wrote 9,950 rows to main.robert_lee_cockroachdb.usertable_test_parquet_usertable_with_split_delta_iterator\n"
     ]
    }
   ],
   "source": [
    "# Convert iterator records to DataFrame and write to Delta\n",
    "# NOTE: Deduplication is now handled by the connector's _read_table_from_volume() method!\n",
    "# The connector already applied:\n",
    "#   ‚úÖ Column family fragment merging (if needed)\n",
    "#   ‚úÖ Deduplication by primary key (keep latest by timestamp)\n",
    "#   ‚úÖ DELETE operation filtering\n",
    "# So we just need to write the final results to Delta!\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create target table name for iterator pattern\n",
    "iterator_target_table = f\"{TARGET_TABLE}_iterator\"\n",
    "iterator_table_path = f\"{CATALOG}.{SCHEMA}.{iterator_target_table}\"\n",
    "\n",
    "# Convert records to DataFrame\n",
    "if all_records:\n",
    "    print(f\"\\nüìä Iterator Pattern Results:\")\n",
    "    print(f\"   Total records: {len(all_records):,}\")\n",
    "    print(f\"   (Deduplication already applied by connector)\")\n",
    "    \n",
    "    # Convert to pandas first (handles variable columns better)\n",
    "    import pandas as pd\n",
    "    pdf = pd.DataFrame(all_records)\n",
    "    \n",
    "    print(f\"   Pandas DataFrame: {len(pdf)} rows, {len(pdf.columns)} columns\")\n",
    "    print(f\"   Columns: {list(pdf.columns)[:10]}...\")  # Show first 10\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_iterator = spark.createDataFrame(pdf)\n",
    "    \n",
    "    print(f\"\\nüìä Final row count: {df_iterator.count():,}\")\n",
    "    \n",
    "    # Write to Delta table\n",
    "    print(f\"\\nüíæ Writing iterator data to Delta table...\")\n",
    "    print(f\"   Target: {iterator_table_path}\")\n",
    "    \n",
    "    df_iterator.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(iterator_table_path)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully wrote {df_iterator.count():,} rows to {iterator_table_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No records to write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Autoloader vs Iterator Pattern\n",
    "\n",
    "Compare results from both patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PATTERN COMPARISON\n",
      "================================================================================\n",
      "‚úÖ Autoloader Pattern (Pattern 2): 9,950 rows\n",
      "   Table: main.robert_lee_cockroachdb.usertable_test_parquet_usertable_with_split_delta\n",
      "   Features: Streaming, checkpointing, file tracking\n",
      "\n",
      "‚úÖ Iterator Pattern (Pattern 1): 9,950 rows\n",
      "   Table: main.robert_lee_cockroachdb.usertable_test_parquet_usertable_with_split_delta_iterator\n",
      "   Features: Manual batching, cursor tracking, memory efficient\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ MATCH! Both patterns produced 9,950 rows\n",
      "Both patterns are working correctly!\n",
      "\n",
      "================================================================================\n",
      "PATTERN CHARACTERISTICS\n",
      "================================================================================\n",
      "\n",
      "Pattern 1 - Iterator (Community Connector):\n",
      "  ‚úÖ Simple, explicit control\n",
      "  ‚úÖ Works with batch processing\n",
      "  ‚úÖ Good for testing/prototyping\n",
      "  ‚ö†Ô∏è Manual cursor management\n",
      "  ‚ö†Ô∏è No built-in streaming\n",
      "\n",
      "Pattern 2 - Autoloader (Standalone):\n",
      "  ‚úÖ Automated file tracking\n",
      "  ‚úÖ Built-in checkpointing\n",
      "  ‚úÖ Streaming capable\n",
      "  ‚úÖ Production-ready\n",
      "  ‚ö†Ô∏è More complex setup\n",
      "\n",
      "Recommendation:\n",
      "  - Use Pattern 1 for: Testing, development, low-volume workloads\n",
      "  - Use Pattern 2 for: Production, high-volume, continuous CDC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare results from both patterns\n",
    "print(\"=\"*80)\n",
    "print(\"PATTERN COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Autoloader Pattern (Pattern 2)\n",
    "try:\n",
    "    autoloader_count = spark.table(TARGET_TABLE_PATH).count()\n",
    "    print(f\"‚úÖ Autoloader Pattern (Pattern 2): {autoloader_count:,} rows\")\n",
    "    print(f\"   Table: {TARGET_TABLE_PATH}\")\n",
    "    print(f\"   Features: Streaming, checkpointing, file tracking\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Autoloader Pattern: Not run yet\")\n",
    "    autoloader_count = None\n",
    "\n",
    "# Iterator Pattern (Pattern 1)\n",
    "try:\n",
    "    iterator_count = spark.table(iterator_table_path).count()\n",
    "    print(f\"\\n‚úÖ Iterator Pattern (Pattern 1): {iterator_count:,} rows\")\n",
    "    print(f\"   Table: {iterator_table_path}\")\n",
    "    print(f\"   Features: Manual batching, cursor tracking, memory efficient\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Iterator Pattern: Not run yet\")\n",
    "    iterator_count = None\n",
    "\n",
    "# Compare\n",
    "if autoloader_count is not None and iterator_count is not None:\n",
    "    if autoloader_count == iterator_count:\n",
    "        print(f\"\\n‚úÖ‚úÖ‚úÖ MATCH! Both patterns produced {autoloader_count:,} rows\")\n",
    "        print(\"Both patterns are working correctly!\")\n",
    "    else:\n",
    "        diff = abs(autoloader_count - iterator_count)\n",
    "        print(f\"\\n‚ö†Ô∏è MISMATCH: {diff:,} row difference\")\n",
    "        print(\"Review the data transformations in each pattern\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PATTERN CHARACTERISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Pattern 1 - Iterator (Community Connector):\n",
    "  ‚úÖ Simple, explicit control\n",
    "  ‚úÖ Works with batch processing\n",
    "  ‚úÖ Good for testing/prototyping\n",
    "  ‚ö†Ô∏è Manual cursor management\n",
    "  ‚ö†Ô∏è No built-in streaming\n",
    "  \n",
    "Pattern 2 - Autoloader (Standalone):\n",
    "  ‚úÖ Automated file tracking\n",
    "  ‚úÖ Built-in checkpointing\n",
    "  ‚úÖ Streaming capable\n",
    "  ‚úÖ Production-ready\n",
    "  ‚ö†Ô∏è More complex setup\n",
    "\n",
    "Recommendation:\n",
    "  - Use Pattern 1 for: Testing, development, low-volume workloads\n",
    "  - Use Pattern 2 for: Production, high-volume, continuous CDC\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_311_dogfood (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
